---
title: "A Comparison of Regression models for Cylindrical data in Psychology."
author: |
  | Jolien Cremers^[Corresponding author: j.cremers@uu.nl] $^1$, Helena J.M. Pennings$^{2,3}$,  Christophe Ley $^{4}$
  | $^1$Department of Methodology and Statistics, Utrecht University
  | $^2$TNO
  | $^3$Department of Education, Utrecht University
  | $^4$Department of Applied Mathematics, Computer Science and Statistics, Ghent University

bibliography: CircularData.bib
csl: apa.csl
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
  html_document: default
  word_document: default
fontsize: 11pt
geometry: margin=1in
header-includes:
  - \usepackage{multirow}
  - \usepackage{appendix}
  - \usepackage{color}
  - \usepackage{hyperref}
  - \usepackage{subcaption}

keywords: bla vla
documentclass: article
pandoc_args: --natbib
abstract: |
  Cylindrical dats is multivariate data which consists of a directional, in this paper circular, component   and a linear component. Examples of cylindrical data in psychology include human navigation (direction     and distance of movement), eye-tracking research (direction and length of saccades) and data from an       interpersonal circumplex (type and strength of interpersonal behavior). In this paper we adapt four        models for cylindrical data to include a regression of the circular and linear component onto a set of     covariates. Subsequently, we illustrate how to fit these models and interpret their results on a dataset   on the interpersonal behavior of teachers.
---
```{r inlude = FALSE, echo = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, echo = FALSE}
library(circular)
library(bpnreg)
library(haven)
library(tikzDevice)
library(plotrix)
library(tidyr)
library(dplyr) #sample_n()
library(MASS) #mvrnorm()
library(kableExtra) #latex tables
library(plotrix)
```


\section{Introduction}\label{Introduction}

Cylindrical data are data that consist of a linear variable and a directional 
variable. In this paper, the directional variable is circular meaning that it 
consists of a single angle instead of a set of angles. A circular variable is 
different from a linear variable in the sense that it is measured on a different
scale. Figure \ref{circline} shows the difference between a circular scale 
(right) and a linear scale (left). The most important difference is that on a 
circular scale the datapoints 0$^\circ$ and 360$^\circ$ are connected and in 
fact represent the same number while on a linear scale the two ends, $-\infty$ 
and $\infty$ are not connected. This difference requires us to use different 
statistical methods for circular variables (see e.g. @fisher1995statistical for 
an introduction to circular data and @mardia2000directional,
@jammalamadaka2001topics and @ley2017modern for a more elaborate overview).

Cylindrical data occur in several fields of research, such as for instance in 
meteorology [@garcia2013exploring], ecology [@garcia2014test] or marine research
[@lagona2015hidden]. Several types of data in psychology are also of a 
cylindrical nature. For example, in research on human navigation in the field of
cognitive psychology both distance, a linear variable, and direction, a circular
variable, of movement are of interest [@chrastil2017rotational]. In eye-tracking
research, which can be used for investigating various cognitive processes (e.g. 
those involved in reading a text), we can also speak of cylindrical data. When 
measuring eye-movements we speak of the eye-movements themselves, the saccades, 
and fixations, the periods of time between movements when the eyes are looking 
at one point. Of the saccades both the direction, a circular variable, and the 
duration, a linear variable, are of interest (for a review of eye-tracking 
research see @rayner200935th). Data from circumplex measurement instruments,
e.g. the interpersonal circumplex as used in personality psychology, are also of
a cylindrical nature (see Section \ref{Example} for a more detailed
explanation).

In this paper we will discuss how a correct statistical treatment of such 
cylindrical data can lead to new insights. In particular we will show how 
cylindrical models pave the way for circular-linear and linear-circular 
regression. We will do this for a motivating example, the teacher data,  from
the field of educational psychology . In this example, apart from modelling the
relation between the linear and circular component of a cylindrical variable we
would also like to predict the two components from a set of covariates in a
regression model. The teacher data will be further introduced in Sections
\ref{Example} and \ref{DataAnalysis}.

\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{Plots/circline.pdf}
\caption{The difference between a linear scale (left) and a circular scale (right).}
\label{circline}
\end{figure}

As is the case for circular data, the analysis of cylindrical data requires 
special methods. Several methods have been put forward to model the relation 
between the linear and circular component of a cylindrical variable. Some of 
these are based on regressing the linear component onto the circular component 
using the following type of relation: $$y = \beta_0 + \beta_1*\cos(\theta) + 
\beta_2*\sin(\theta)+ \epsilon,$$ where $y$ is the linear component and $\theta$
the circular component [@mardia1978model;
@johnson1978some; @mastrantonio2015bayesian]. Others model the relation in a 
different way, e.g. by specifying a multivariate model for several linear and 
circular variables and modelling their covariance matrix 
[@mastrantonio2018joint] or by proposing a joint cylindrical distribution. For 
example, @abe2017tractable introduce a cylindrical distribution based on a 
Weibull distribution for the linear component and a sine-skewed von Mises 
distribution for the circular component and link these through their shape and 
concentration parameters respectively. However, none of the methods that have 
been proposed thus far include additional covariates onto which both the
circular and linear component are regressed.

Our aim in this paper is to fill this gap in the literature by adapting four
existing cylindrical methods in such a way that they include a regression of
both the linear and circular component of a cylindrical variable onto a set of
covariates. First however, we will introduce the teacher data in Section
\ref{Example}. In Section \ref{Models} we introduce the four modified models for
cylindrical data that we use to analyze the data from the motivating example. We
also choose a model selection criterion to compare the models. The teacher data
will be analysed in Section \ref{DataAnalysis}. The paper will be concluded with
a discussion in Section \ref{Discussion}. The Appendix contains technical
details.





\section{Teacher data}\label{Example}

The motivating example for this article comes from the field of educational 
psychology and was collected for the studies on classroom climate of
@pennings2017interpersonal, @Claessens2016side and @vanderWant2015role. An 
indicator of classroom climate is the students' perceptions of their teachers'
interpersonal behavior. These interpersonal perceptions, both in educational
psychology as well as in other areas of psychology, can be measured using
circumplex measurement instruments (see
@horowitz2010handbook for an overview of many such instruments).

The Questionnaire on Teacher Interaction (QTI) [@wubbels2006interpersonal] is
one such circumplex measurement instrument. It is used to study student
perceptions of their teachers' interpersonal behavior and contains
items that load on two dimensions: Agency and Communion. Agency refers to the
degree of power or control a teacher exerts in interaction with his/her
students. Communion refers to the degree of friendliness or affiliation a
teacher conveys in interaction with his/her students. The loadings on the two 
dimensions of the QTI can be placed in a two-dimensional space formed by Agency
(vertical) and Communion (horizontal), see Figure \ref{QTI}. Different parts of
this space are characterized by different teacher behavior, e.g. 'helpful' or
'uncertain'. We call the two-dimensional space the interpersonal circle (IPC).
The idea is that the IPC is ``a continuous order with no beginning or end''
[@gurtman2009exploring, p. 2]. We call such ordering a circumplex ordering and
the IPC is therefore often called the interpersonal circumplex. The ordering
also implies that scores on the IPC could be viewed as a circular variable.

\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{Plots/IPC-T.png}
\caption{The interpersonal circle for teachers (IPC-T). The words presented in
the circumference of the circle are anchor words to describe the type of
behavior located in each part of the IPC.}
\label{QTI}
\end{figure}

@Cremers2018Assessing show how data from the IPC can be considered circular data
and analyzed as such using a regression model. The two-dimension scores Agency
and Communion can be converted to a circular score using the two-argument
arctangent function in \eqref{PredVal}, where $A$ represents a score on
the Agency dimension and $C$ represents a score on the Communion dimension. The
resulting circular variable $\theta$ can then be modeled. However, when
two-dimensional data are converted to the circle we lose some information, the
length of the two-dimensional vector $(A, C)$, \emph{i.e.}, its euclidean norm
$\mid\mid (A, C) \mid\mid$. This length represents the strength of the type of
interpersonal behavior a teacher shows towards his/her students. In a
cylindrical model we are able to incorporate this information, and model a
circular variable $\theta$ together with a linear variable corresponding to
$\mid\mid (A, C) \mid\mid$. This leads to an improved analysis of data from the
IPC. In the next section we introduce several models that can be used for more
accurate and informative regression analysis on the teacher data. Descriptives
for the teacher data are given in Section \ref{DataAnalysis}.

\begin{equation}\label{PredVal}
\theta          = \text{atan2}\left(A, \: C\right)  =
\left|{\begin{array}{lcl}
                                                                       \arctan\left(\frac{A}{C}\right) & \text{if}  \quad&C > 0 \\
\arctan\left(\frac{A}{C}\right) + \pi & \text{if}  \quad& C  <  0  \:\: \&\:\: A \geq 0\\
 \arctan\left(\frac{A}{C}\right) - \pi & \text{if}  \quad&C  <  0 \:\:  \&\:\:A  < 0\\
 +\frac{\pi}{2} & \text{if}  \quad& C  =  0  \:\: \&\:\:A > 0\\
 -\frac{\pi}{2} & \text{if}  \quad& C =  0  \:\: \&\:\:A < 0\\
 \text{undefined} & \text{if} \quad& C =  0   \:\: \&\:\:A = 0.
 \end{array}}
\right.
\end{equation}

```{r loaddata, return = FALSE, echo = FALSE}
Dat <- read_spss("Data Heleen/mergeddata.sav")

#only select the first measurement occasion
Dat <- subset(Dat, Time == 0)

#sort on class size
Dat <- Dat[order(-Dat$nklas), ]

#Now remove duplicates 
#(because dataframe is sorted on class size the smallest classes are removed)

duplicates <- duplicated(Dat[,c("docnr", "Time")])
Dat <- Dat[!duplicates,]

#center and remove missings for self-efficacy
Dat <- Dat %>% drop_na(Efficacy_CM)
Dat$SEc <- Dat$Efficacy_CM- mean(Dat$Efficacy_CM)

set.seed(50)

#Create holdout and test sample
Dat.hold <- sample_n(Dat, 15)
Dat.test <- Dat[-as.numeric(rownames(Dat.hold)),]

#Compute circular and linear outcome
theta.test <- atan2(Dat.test$lAgency, Dat.test$lcommunion)
y.test     <- sqrt(Dat.test$lAgency^2 + Dat.test$lcommunion^2)

theta.hold <- atan2(Dat.hold$lAgency, Dat.hold$lcommunion)
y.hold     <- sqrt(Dat.hold$lAgency^2 + Dat.hold$lcommunion^2)

```



\section{Four cylindrical regression models}\label{Models}

In this section we introduce four cylindrical regression models to contain 
predictors for the linear and circular outcomes, $\Theta$ and $Y$. We extend the models from @mastrantonio2018joint and
@abe2017tractable. Additionally, we introduce two models where the relation
between $\Theta$ and $Y$ is modelled as follows (following
@mastrantonio2015bayesian):

\begin{equation}\label{circlinlink}
y = \gamma_0 + \gamma_{cos}*\cos(\theta)*r + \gamma_{sin}*\sin(\theta)*r + \gamma_1*x_1 + \dots + \gamma_q*x_q +  \epsilon,
\end{equation}

where $r$ will be introduced in Section \ref{CL-(G)PN}, the error term $\epsilon
\sim N(0, \sigma)$, $\gamma_0, \gamma_{cos}, \gamma_{sin}, \gamma_1, \dots, 
\gamma_q$ are the intercept and regression coefficients and   $x_1, \dots, x_q$
are the $q$ covariate values. In this model $Y$ follows a normal distribution
and $\Theta$ a projected normal (PN) or general projected normal (GPN)
distribution on the circle.


\subsection{The modified CL-PN and modified CL-GPN  models}\label{CL-(G)PN}

In both of these models the relation between $\Theta \in [0, 2\pi)$ and $Y\in 
(-\infty, + \infty)$ is specified as in \eqref{circlinlink} with the following
conditional distribution:

\begin{equation}\label{ycondtheta}
f(y \mid \theta) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[\frac{c^2 + (y - (\gamma_0 + \gamma_1x_1 + \dots + \gamma_qx_q))^{2} - 2c(y - (\gamma_0 + \gamma_1x_1 + \dots + \gamma_qx_q))}{2\sigma^2}\right],
\end{equation}

where $c = \begin{bmatrix} r \cos \theta \\  r\sin \theta \end{bmatrix}^t 
\begin{bmatrix} \gamma_{cos} \\ \gamma_{sin} \end{bmatrix}$, $r \geq 0$, 
$\gamma_0, \gamma_{cos}, \gamma_{sin}, \gamma_1, \dots, \gamma_q$ are the
intercept and regression coefficients and $\sigma^2 \geq 0$ is the error
variance. The linear outcome thus has a normal distribution, conditional on
$\Theta$. 

For the circular outcome we assume either a projected normal (PN) or a
general projected normal (GPN) distribution. These distributions arise from the
radial projection of a distribution defined on the plane onto the circle. The
relation between a bivariate variate $\boldsymbol{S}$ in the plane and the
circular outcome $\Theta$ is defined as follows:

\begin{equation}\label{projection}
\boldsymbol{S} = \begin{bmatrix} S^{I} \\ S^{II} \end{bmatrix} = R\boldsymbol{u} = \begin{bmatrix} R \cos \Theta \\  R\sin \Theta \end{bmatrix},
\end{equation}

where $R = \mid\mid S \mid\mid$, the euclidean norm of the bivariate vector 
$\boldsymbol{S}$. In the PN distribution we assume $\boldsymbol{S} \sim 
N_2(\boldsymbol{\mu}, \boldsymbol{I})$ and in the GPN we assume $\boldsymbol{S} 
\sim N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ where $\boldsymbol{\Sigma} = 
\begin{bmatrix} \tau^2 + \rho^2 & \rho\\ \rho & 1 \end{bmatrix}$, $\rho \in 
(-\infty, +\infty)$ and $\tau^2 \geq 0$ (as in @hernandez2016general).

Following @nunez2011bayesian, the joint density of $\Theta$ and $R$ for the PN
distribution in a regression set-up equals:

\begin{equation}\label{pnreg}
f(\theta,r \mid \boldsymbol{\mu}, \boldsymbol{I}) = [2\pi]^{-1} \exp\left[ \frac{-r^2 - \boldsymbol{\mu}^2\boldsymbol{\mu} + 2r\boldsymbol{u}^t\boldsymbol{\mu}}{2}\right],
\end{equation}

In a regression setup the outcome $\theta_i$ for each individual $i = 1, \dots,
n$, where $n$ is the sample size, is generated independently from \eqref{pnreg}.
The mean vector $\boldsymbol{\mu}_i \in \mathbb{R}^2$ is defined as
$\boldsymbol{\mu}_i = \boldsymbol{z}_i\boldsymbol{B}$. The vector 
$\boldsymbol{z}_i$ is a a vector of $p$ covariate values and $\boldsymbol{B} = 
(\boldsymbol{\beta}^{I}, \boldsymbol{\beta}^{II})$ contain the regression 
coefficients. Note however that the dimensions of $\boldsymbol{\beta}^{I}$ and 
$\boldsymbol{\beta }^{I}$ need not necessarily be the same and we are thus 
allowed to have a different set of predictor variables and vectors 
$\boldsymbol{z}_i^I$ and $\boldsymbol{z}_i^{II}$ for the two components of 
$\boldsymbol{\mu}_i$.

Following @wang2012directional and @hernandez2016general the joint density of
$r$ and $\theta$ for the GPN distribution equals:

\begin{equation}\label{gpnreg}
f(\theta, r \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = r(2\pi\tau)^{-1} \exp\left[ -0.5 \sigma^2(r\boldsymbol{u}-\boldsymbol{\mu})^{t}\boldsymbol{\Sigma}^{-1}(r\boldsymbol{u}-\boldsymbol{\mu})\right],
\end{equation}

where $\boldsymbol{\Sigma} = \begin{bmatrix} \tau^2 + \rho^2  & \rho\\ \rho & 1 
\end{bmatrix}$, $\boldsymbol{u}= \begin{bmatrix} \cos \theta \\ \sin \theta 
\end{bmatrix}$. In a regression setup the outcome $\theta_i$ for each individual
is generated independently from \eqref{gpnreg}. The mean vector 
$\boldsymbol{\mu}_i \in \mathbb{R}^2$ is defined as $\boldsymbol{\mu}_i = 
\boldsymbol{z}_i(\boldsymbol{\beta}^{I}, \boldsymbol{\beta}^{II})$. The vector 
$\boldsymbol{z}_i$ is a a vector of $p$ covariate values and each 
$\boldsymbol{\beta}^{k}$ is a vector with two regression coefficients, one for 
each of the two components of $\boldsymbol{\mu}_i$. Note that for the CL-GPN 
model we do need to have the same predictors for both components of 
$\boldsymbol{\mu}_i$.

Both cylindrical models introduced here are estimated using MCMC methods based
on @nunez2011bayesian, @wang2012directional and @hernandez2016general for the 
regression of the circular outcome. A detailed description of the Bayesian
estimation and MCMC samplers can be found in Appendices \ref{A1} and \ref{A2}.


\subsection{The modified Abe-Ley model}\label{WeiSSVM}

This model is an extension of the cylindrical model introduced by
@abe2017tractable to the regression context. The joint density of $\Theta$ and
$Y$, in this model defined on the positive real half-line $[0, + \infty)$, is:

\begin{equation}\label{WeiSSVMdensity}
f(\theta, y) = \frac{\alpha(\beta)^\alpha}{2\pi\cosh(\kappa)}
                 (1 +\lambda\sin(\theta - \mu))
                 y^{\alpha-1}
                 \exp[-((\beta y)^{\alpha}(1-\tanh(\kappa)\cos(\theta - \mu)))],
\end{equation}

In a regression setup the outcome vector $(\theta_i, y_i)^t$ for each individual
is generated independently from \eqref{WeiSSVMdensity}. The parameters $\alpha >
0$ and $\beta_i = \exp(\boldsymbol{x}_i^t\boldsymbol{\nu}) > 0$ are linear shape
and scale parameters, $\mu_i = \eta_0 + 
2\tan^{-1}(\boldsymbol{z}_i^t\boldsymbol{\eta}) \in [0, 2\pi)$, $\kappa > 0$ and
$\lambda \in [-1, 1]$ are circular location, concentration and skewness 
parameters. The parameter $\boldsymbol{\nu}$ is a vector of $q$ regression 
coefficients $\nu_j \in (-\infty, +\infty)$ for the prediction of $y$ where $j =
0, \dots, q$ and $\nu_0$ is the intercept. The parameter $\eta_0 \in [0, 2\pi)$
is the intercept and $\boldsymbol{\eta}$ is a vector of $p$ regression
coefficients $\eta_j \in (-\infty, +\infty)$ for the prediction of $\theta$
where $j = 1, \dots, p$. The vector $\boldsymbol{x}_i$ is a vector of predictor
values for the prediction of $y$ and $\boldsymbol{z}_i$ is a vector of predictor
values for the prediction of $\theta$.

As in @abe2017tractable, the conditional distribution of $y$ given $\theta$ is a
Weibull distribution with scale $\alpha$ and shape
$\beta(1-\tanh(\kappa)\cos(\theta - \mu))^{1-\alpha}$ and the conditional 
distribution of $\theta$ given $y$ is a sine skewed von Mises distribution with 
location parameter $\mu$ and concentration parameter $(\beta 
y)^\alpha\tanh(\kappa)$.

The log-likelihood for this model equals:

\begin{align}\label{WeiSSVMLikelihood}
l(\alpha, \boldsymbol{\nu}, \lambda, \kappa, \boldsymbol{\eta}) 
   &= n[\ln(\alpha) - \ln(2\pi\cosh(\kappa))] + \alpha \sum^{n}_{i = 1} \ln(\exp(\boldsymbol{x}_i^t\boldsymbol{\nu})) \nonumber\\
   &\:\:\:\:+\sum^{n}_{i = 1} \ln(1 +\lambda\sin(\theta_i - \eta_0 + 2\tan^{-1}(\boldsymbol{z}_i^t\boldsymbol{\eta}))) 
   +(\alpha-1)\sum^{n}_{i = 1} \ln(y_i) \nonumber\\
   &\:\:\:\:-\sum^{n}_{i = 1}( \exp(\boldsymbol{x}_i^t\boldsymbol{\nu})y_i)^{\alpha}(1-\tanh(\kappa)\cos(\theta_i - \eta_0 + 2\tan^{-1}(\boldsymbol{z}_i^t\boldsymbol{\eta})))
\end{align}

We can use numerical optimization (Nelder-Mead) to find solutions for the
maximum likelihood (ML) estimates for the parameters of the model.


\subsection{Modified joint projected and skew normal (GPN-SSN)}\label{CL-GPN_multivariate}

This model is an extension of the cylindrical model introduced by
@mastrantonio2018joint to the regression context. It contains $p$ circular
outcomes and $q$ linear outcomes. The circular outcomes $\boldsymbol{\Theta} = 
(\boldsymbol{\Theta}_1, \dots, \boldsymbol{\Theta}_p)$ are modeled together by a
multivariate GPN distribution and the linear outcomes $\boldsymbol{Y} = 
(\boldsymbol{Y}_1, \dots, \boldsymbol{Y}_q)$  are modeled together by a 
multivariate skew normal distribution [@sahu2003new]. Because the GPN 
distribution is modelled using a so-called augmented representation (as in 
\eqref{projection} and \eqref{gpnreg}) it is convenient to use a similar tactic
for modelling the multivariate skew normal distribution. Following
@mastrantonio2018joint the linear outcomes are represented as:

$$\boldsymbol{Y} = \boldsymbol{\mu}_y + \boldsymbol{\Lambda}\boldsymbol{D} + \boldsymbol{H},$$
where where $\boldsymbol{\mu}_y$ is a mean vector for the linear outcome 
$\boldsymbol{Y}$, $\boldsymbol{\Lambda} = \text{diag}(\boldsymbol{\lambda})$ is a $q 
\times q$ diagonal matrix with diagonal elements $\boldsymbol{\lambda} = 
\lambda_1, \dots, \lambda_q$ (skewness parameters), $\boldsymbol{D} \sim HN_q(\boldsymbol{0}_q, 
\boldsymbol{I}_q)$, a q-dimensional half normal distribution
[@olmos2012extension], and $\boldsymbol{H} \sim N_q(\boldsymbol{0}_q,
\boldsymbol{\Sigma}_y)$. This means that conditional on the auxiliary data
$\boldsymbol{D}$, $\boldsymbol{Y}$ is normally distributed with mean
$\boldsymbol{\mu}_y + \boldsymbol{\Lambda}\boldsymbol{D}$ and covariance matrix
$\boldsymbol{\Sigma}_y$. The joint density for $(\boldsymbol{Y},
\boldsymbol{D})^t$ is defined as follows:

\begin{equation}\label{YDjoint}
f(\boldsymbol{y}, \boldsymbol{d}) = 2^q\phi_q(\boldsymbol{y} \mid \boldsymbol{\mu}_y + \boldsymbol{\Lambda}\boldsymbol{d}, \boldsymbol{\Sigma}_y) \phi_1(\boldsymbol{d} \mid \boldsymbol{0}_q, \boldsymbol{I}_q).
\end{equation}



As in @mastrantonio2018joint dependence between the linear and circular outcome
is created by modelling the augmented representations of $\boldsymbol{\Theta}$,
and $\boldsymbol{Y}$ together in a $2 \times p + q$ dimensional normal
distribution. The joint density of the model is then represented by:

\begin{equation}\label{YDThetarjoint}
f(\boldsymbol{\theta}, \boldsymbol{r}, \boldsymbol{y}, \boldsymbol{d}) = 2^q\phi_{2p+q}((\boldsymbol{s}, \boldsymbol{y})^t \mid \boldsymbol{\mu} + (\boldsymbol{0}_{2p}, diag(\boldsymbol{\lambda})\boldsymbol{d})^t, \boldsymbol{\Sigma}) \phi_q(\boldsymbol{d} \mid \boldsymbol{0}_q, \boldsymbol{I}_q) \prod_{j = 1}^{p}r_j,
\end{equation}

where the mean vector $\boldsymbol{\mu} = (\boldsymbol{\mu}_s,
\boldsymbol{\mu}_y)^t$ and $\boldsymbol{\Sigma} = \left ( \begin{matrix}
\boldsymbol{\Sigma}_s & \boldsymbol{\Sigma}_{sy} \\ \boldsymbol{\Sigma}_{sy^t} &
\boldsymbol{\Sigma}_y \\ \end{matrix} \right )$. The matrix
$\boldsymbol{\Sigma}_s$ is the covariance matrix for the variances of and
covariances between the augmented representations of the circular outcome and
the matrix $\boldsymbol{\Sigma}_{sy}$ contains covariances between the augmented
representations of the circular outcome and the linear outcome.

In our regression extension we have $i = 1, \dots, n$ observations of $p$ 
circular outcomes, $q$ linear outcomes and $k$ covariates. The mean in the 
density in \eqref{YDThetarjoint} then becomes $\boldsymbol{\mu}_i = 
\boldsymbol{x}_i\boldsymbol{B}$ where $\boldsymbol{B}$ is a $k \times (2 
\times p + q)$ matrix with regression coeffients. We estimate the model using 
MCMC methods. A detailed description of these methods is given in Appendix
\ref{A3}.


\subsection{Model fit criterion}\label{Modelfit}

For the four cylindrical models we focus on their out-of-sample predictive 
performance to determine the fit of the model. To do so we split our data in a
training and holdout set (10 $\%$ of the sample). A proper criterion to compare
out-of-sample predictive performance is the the Predictive Log Scoring Loss
(PLSL) [@gneiting2007strictly]. The lower the value of this criterion, the
better the predictive performance of the model. Using ML estimates this
criterion can be computed as follows:

\begin{equation}\label{PLSLML}
PLSL = -2 \sum_{i = 1}^{M}\log l(x_i \mid \hat{\boldsymbol{\vartheta}}),
\end{equation}

where $l$ is the model likelihood, $M$ is the sample size of the holdout set, 
$x_i$ is the $i^{th}$ datapoint from the holdout set and 
$\hat{\boldsymbol{\vartheta}}$ are the ML estimates of the model parameters. 
Using posterior samples the criterion is similar to the log pointwise predictive
density (lppd) as outlined in @BDA and can be computed as:

\begin{equation}\label{PLSLBayes}
PLSL = -2 \frac{1}{B} \sum_{j = 1}^{B}\sum_{i = 1}^{M} \log l(x_i \mid \boldsymbol{\vartheta}^{(j)}),
\end{equation}

where $B$ is the amount of posterior samples and $\boldsymbol{\vartheta}^{(j)}$ 
are the posterior estimates of the model parameters for the $j^{th}$ iteration. 
Note that although we fit the CL-PN, CL-GPN and joint GPN-SSN models using 
Bayesian statistics, we do not take prior information into account when 
assessing model fit. According to @BDA \textcolor{red}{(p. ???)} this is not 
necessary since we are assessing the fit of a model to data, the holdout set,
only. They argue that the prior in such case is only of interest for estimating
the parameters of the model but not for determining the predictive accuracy. 
For each of the cylindrical models we can then compute a PLSL for the circular
and linear outcome by using the conditional log-likelihoods of the respective
outcome.





\section{Data Analysis}\label{DataAnalysis}

In this section we analyze the teacher data with the help of the four
cylindrical models from Section \ref{Models}. First however, we give a more
detailed description of the dataset.

```{r FitCLPN, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling CL-PN.R")

its <- 20000
set.seed(101)

ZI.PN.test  <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))
ZII.PN.test <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))
X.PN.test   <- as.matrix(cbind(rep(1, length(theta.test)),
                               cos(theta.test),
                               sin(theta.test),
                               Dat.test$SEc))

ZI.PN.hold  <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))
ZII.PN.hold <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))
X.PN.hold   <- as.matrix(cbind(rep(1, length(theta.hold)),
                               cos(theta.hold),
                               sin(theta.hold),
                               Dat.hold$SEc))

res_CLPN <- CLPN(theta.test, y.test, X.PN.test, ZI.PN.test, ZII.PN.test, its,
                 theta.hold, y.hold, X.PN.hold, ZI.PN.hold, ZII.PN.hold)
```


```{r FitCLGPN, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling CL-GPN.R")

its <- 20000
set.seed(101)

Z.GPN.test <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))
X.GPN.test <- as.matrix(cbind(rep(1, length(theta.test)),
                              cos(theta.test),
                              sin(theta.test),
                              Dat.test$SEc))

Z.GPN.hold <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))
X.GPN.hold <- as.matrix(cbind(rep(1, length(theta.hold)),
                              cos(theta.hold),
                              sin(theta.hold),
                              Dat.hold$SEc))

res_CLGPN <- CLGPN(theta.test, y.test, X.GPN.test, Z.GPN.test, its, p = 2, 
                   theta.hold, y.hold, X.GPN.hold, Z.GPN.hold)

```

```{r FitCLGPNM, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling Joint GPN-SSN.R")

its <- 20000
set.seed(101)

X.MGPN.test <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))

X.MGPN.hold <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))

res_CLGPNM <- JGPNSSN(theta.test, y.test, X.MGPN.test, its, p = 1, q = 1, 
                      theta.hold, y.hold, X.MGPN.hold)

```

```{r FitAbeLey, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Abe-Ley optimization.R")

set.seed(101)

Z.test     <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))
X.test    <- as.matrix(cbind(rep(1, length(theta.test)), Dat.test$SEc))

Z.hold     <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))
X.hold     <- as.matrix(cbind(rep(1, length(theta.hold)), Dat.hold$SEc))

colnames(X.test) <- c("ax", "bx")
colnames(Z.test) <- c("az", "bz")
colnames(X.hold) <- c("ax", "bx")
colnames(Z.hold) <- c("az", "bz")


dat.t <- as.data.frame(cbind(theta.test, y.test, X.test, Z.test))
dat.h <- as.data.frame(cbind(theta.hold, y.hold, X.hold, Z.hold))

ui.reg = rbind(c(1, 0, 0, 0, 0, 0, 0),
               c(-1, 0, 0, 0, 0, 0, 0),
               c(0, 0, 0, 0, 1, 0, 0),
               c(0, 0, 0, 0, 0, 1, 0),
               c(0, 0, 0, 0, 0, 0, 1),
               c(0, 0, 0, 0, 0, 0, -1))

ci.reg = c(-pi, -pi, 0, 0, -1, -1)

param <- c(1,1,1,1,1,1,0)*0.9

ui.reg %*% param - ci.reg

resAL <- constrOptim(param, func.regII, ui = ui.reg, ci = ci.reg, method = "Nelder-Mead",
                     control = list(maxit = 1000000, fnscale = -1), data = dat.t)

```


\subsection{Data description}\label{DataDescriptives}

The teacher data was collected between 2010 and 2015 and contains several 
repeated measures on the IPC of 161 teachers. Measurements were obtained using 
the QTI and taken in different years and classes. For this paper we only 
consider one measurement, the first occasion (2010) and largest class if data 
for multiple classes were available. In addition to the score on the IPC, the 
circular outcome, and the strength of the score on the IPC, the linear outcome, 
a teachers' self-efficacy (\verb|SE|) concerning classroom management ia used as
covariate in the analysis. After the removal of missings we have a sample of 148
teachers of which 10\% (n = 15) is used as a holdout set for the computation of
the PLSL. Table \ref{Tableteacherdescriptives} shows descriptives
for the dataset. Figure \ref{dataplot} is a scatterplot showing the relation
between the linear and circular outcome of the teacher data.

```{r dataplot, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

tikz("Plots/dataplot.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

plot(y.test, theta.test*(180/pi), 
     ylab = "IPC location", xlab = "IPC strength",
     bty = "n", ylim = c(-180, 180))

dev.off()
tools::texi2dvi("Plots/dataplot.tex", pdf=TRUE)

```

```{r descripitves, echo = FALSE, results = FALSE}
#sample size
nrow(Dat)

#summary statistics
summary(Dat$Efficacy_CM)
summary(sqrt(Dat$lAgency^2 + Dat$lcommunion^2))
summary(as.circular(atan2(Dat$lAgency, Dat$lcommunion)%%(2*pi)))

sd(Dat$Efficacy_CM)
sd(sqrt(Dat$lAgency^2 + Dat$lcommunion^2))

```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/dataplot.pdf}
\caption{Plot showing the relation between the linear and circular component of the teacher data.}
\label{dataplot}
\end{figure}

\begin{table}
\centering
\caption{Descriptives for the teacher dataset} 
\begin{tabular}{lrrrl}
  \noalign{\smallskip}\hline\noalign{\smallskip}
Variable & mean/$\bar{\theta}$ & sd/$\hat{\rho}$ & Range & Type \\ \hline\noalign{\smallskip}
IPC &33.22$^\circ$& 0.76 & - & Circular\\
strength IPC & 0.43 & 0.15 & 0.08 - 0.80 & Linear\\
SE & 5.04 & 1.00 & 1.5 - 7.0 & Linear\\
   \hline
\end{tabular}
\label{Tableteacherdescriptives}
\caption*{Note that $\hat{\rho}$ is an sample estimate for the circular concentration where a value of 0 means that the data is not concentrated at all, i.e. spread over the entire circle, and a value of 1 means that all data is concentrated at a single point on the circle. }
\end{table}


\subsection{Models}\label{DataModels}

The regression equations for the linear and the circular outcome of the four
cylindrical models fit to the teacher dataset are as follows:

\begin{itemize}
\item For the modified CL-PN and CL-GPN models:

$\hat{\boldsymbol{\mu}}_{i} = \begin{pmatrix}
  \mu_{i}^{I}  \\
\mu_{i}^{II}
 \end{pmatrix}=\begin{pmatrix}
  \beta_0^{I} + \beta_1^{I}\text{SE}_i  \\
  \beta_0^{II} + \beta_1^{II}\text{SE}_i
 \end{pmatrix},$

$\hat{y}_i = \gamma_0 + \gamma_{cos}\cos\theta_ir_i + \gamma_{sin}\sin\theta_ir_i + \gamma_1\text{SE}_i.$

\item For the modified Abe-Ley model:

$\hat{\mu}_{i} = \eta_0 + 2 * \tan^{-1}(\eta_1\text{SE}_i),$

$\hat{\beta}_{i} = \exp(\nu_0 + \nu_1\text{SE}_i).$

\item For the modified joint projected and skew normal model:

$\hat{\boldsymbol{\mu}}_{i} = \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\text{SE}_i$, where $\boldsymbol{\mu}_i = (\boldsymbol{\mu}_{s_i}, \boldsymbol{\mu}_{y_i})^t$, $\boldsymbol{\beta}_0 = (\beta_{0_s^{I}}, \beta_{0_s^{II}},\beta_{0_y})$ and $\boldsymbol{\beta}_1 = (\beta_{1_s^{I}}, \beta_{1_s^{II}},\beta_{1_y})$.
\end{itemize}

We use the loglikelihoods of the following conditional densities for the
computation of the PLSL:

\begin{itemize}
\item for the modified CL-PN model:

$y_i \mid \mu_i, \sigma^2 \sim N(\mu_i, \sigma^2)$, where $\mu_i = \hat{y}_i$ and for $\theta_i$ we use \eqref{pnreg}.

\item for the modified CL-GPN model:

$y_i \mid \mu_i, \sigma^2 \sim N(\mu_i, \sigma^2)$, where $\mu_i = \hat{y}_i$ and for $\theta_i$ we use \eqref{gpnreg}.

\item for the modified Abe-Ley model:

$y_i \mid \boldsymbol{\theta}_i, \beta_i, \mu_i, \kappa, \alpha \sim W\left(\beta_i(1-\tanh(\kappa)\cos(\theta_i - \mu_i))^{1/\alpha}, \alpha\right)$, a Weibull distribution.

$\theta_i \mid y_i, \beta_i, \mu_i, \kappa, \alpha \lambda \sim SSVM\left(\mu_i, (\beta_iy_i)^{\alpha}(\tanh{\kappa})\right)$, a sine-skewed von Mises distribution.

\item for the modified joint projected and skew normal model:

$y_i \mid \boldsymbol{\mu}_i, \boldsymbol{\Sigma}, \boldsymbol{\theta}_i, r_i \sim SSN(\mu_{i_y} + \lambda d_i + \boldsymbol{\Sigma}_{sy}^t\boldsymbol{\Sigma}_w^{-1}(\boldsymbol{s}_i - \boldsymbol{\mu}_{i_s}), \boldsymbol{\Sigma}_y + \boldsymbol{\Sigma}_{sy}^t\boldsymbol{\Sigma}_s\boldsymbol{\Sigma}_{sy}),$

$\theta_i \mid \boldsymbol{\mu}_i, \boldsymbol{\Sigma}, y_i, d_i \sim GPN(\boldsymbol{\mu}_{i_s} + \boldsymbol{\Sigma}_{sy}\boldsymbol{\Sigma}_y^{-1} (y_i - \mu_{i_y} - \lambda d_i), \boldsymbol{\Sigma}_s + \boldsymbol{\Sigma}_{sy}\boldsymbol{\Sigma}_y^{-1}\boldsymbol{\Sigma}_{sy}^t)$

where $SSN$ is the skew normal distribution.

\end{itemize}


\subsection{Results \& Analysis}\label{DataResults}

Before analysis there were a couple of settings that we had to specify for the 
cylindrical models. Starting values for the Abe-Ley model were the following 
$\eta_0 = 0.9, \eta_1 = 0.9, \nu_0 = 0.9, \nu_1 = 0.9, \kappa = 0.9, \alpha = 
0.9, \lambda = 0$. The initial amount of iterations for the three MCMC samplers 
was set to 2000. After we checked convergence via traceplots we concluded that 
some of the parameters of the joint GPN-SSN model did not converge. Therefore
we set the amount of iterations of the MCMC models to 20,000 and subtracted a 
burn-in of 5000. With this specification the MCMC chains for all parameters 
converge. Note that we choose the same amount of iterations for all three 
Bayesian cylindrical models to make their comparison via the PLSL as fair as 
possible. Lastly, the predictor \verb|SE| was centered before inclusion in the
analysis.

```{r estCLPN, echo = FALSE}

CLPN.Gamma.m   <- apply(res_CLPN$Gamma[5000:20000,], 2, mode_est)
CLPN.Gamma.hpd <- apply(res_CLPN$Gamma[5000:20000,], 2, hpd_est)

CLPN.BI.m   <- apply(res_CLPN$BI[5000:20000,], 2, mode_est)
CLPN.BI.hpd <- apply(res_CLPN$BI[5000:20000,], 2, hpd_est)

CLPN.BII.m <- apply(res_CLPN$BII[5000:20000,], 2, mode_est)
CLPN.BII.hpd <- apply(res_CLPN$BII[5000:20000,], 2, hpd_est)

CLPN.Sigma.m   <- mode_est(res_CLPN$Sigma[5000:20000])
CLPN.Sigma.hpd <- hpd_est(res_CLPN$Sigma[5000:20000])

modes.PN <- c(CLPN.BI.m, CLPN.BII.m, CLPN.Gamma.m, CLPN.Sigma.m, NA, NA, NA)
hpd.LB.PN <- c(CLPN.BI.hpd[1,], CLPN.BII.hpd[1,], CLPN.Gamma.hpd[1,], CLPN.Sigma.hpd[1], NA, NA, NA)
hpd.UB.PN <- c(CLPN.BI.hpd[2,], CLPN.BII.hpd[2,], CLPN.Gamma.hpd[2,], CLPN.Sigma.hpd[2], NA, NA, NA)

CLPNres.tab <- cbind(modes.PN, hpd.LB.PN, hpd.UB.PN)

```


```{r estCLGPN, echo = FALSE}

CLGPN.Sig.m   <- mode_est(res_CLGPN$Sig)
CLGPN.Sig.hpd <- hpd_est(res_CLGPN$Sig)

CLGPN.Gamma.m   <- apply(res_CLGPN$Gamma[5000:20000,], 2, mode_est)
CLGPN.Gamma.hpd <- apply(res_CLGPN$Gamma[5000:20000,], 2, hpd_est)

CLGPN.BI.m   <- apply(res_CLGPN$BI[5000:20000,], 2, mode_est)
CLGPN.BI.hpd <- apply(res_CLGPN$BI[5000:20000,], 2, hpd_est)

CLGPN.BII.m   <- apply(res_CLGPN$BII[5000:20000,], 2, mode_est)
CLGPN.BII.hpd <- apply(res_CLGPN$BII[5000:20000,], 2, hpd_est)

CLGPN.Sigma11.m   <- mode_est(res_CLGPN$Sigma[1,1,5000:20000])
CLGPN.Sigma11.hpd <- hpd_est(res_CLGPN$Sigma[1,1,5000:20000])
CLGPN.Sigma22.m   <- mode_est(res_CLGPN$Sigma[2,2,5000:20000])
CLGPN.Sigma22.hpd <- hpd_est(res_CLGPN$Sigma[2,2,5000:20000])
CLGPN.Sigma12.m   <- mode_est(res_CLGPN$Sigma[1,2,5000:20000])
CLGPN.Sigma12.hpd <- hpd_est(res_CLGPN$Sigma[1,2,5000:20000])

modes.GPN  <- c(CLGPN.BI.m, CLGPN.BII.m,
                CLGPN.Gamma.m, CLGPN.Sig.m,
                CLGPN.Sigma11.m, CLGPN.Sigma12.m, CLGPN.Sigma22.m)
hpd.LB.GPN <- c(CLGPN.BI.hpd[1,], CLGPN.BII.hpd[1,],
               CLGPN.Gamma.hpd[1,], CLGPN.Sig.hpd[1],
               CLGPN.Sigma11.hpd[1], CLGPN.Sigma12.hpd[1], CLGPN.Sigma22.hpd[1])
hpd.UB.GPN <- c(CLGPN.BI.hpd[2,], CLGPN.BII.hpd[2,], 
               CLGPN.Gamma.hpd[2,], CLGPN.Sig.hpd[2],
               CLGPN.Sigma11.hpd[2], CLGPN.Sigma12.hpd[2], CLGPN.Sigma22.hpd[2])

CLGPNres.tab <- cbind(modes.GPN, hpd.LB.GPN, hpd.UB.GPN)
CLPNGPNres.tab <- round(cbind(CLPNres.tab, CLGPNres.tab), 2)

rownames(CLPNGPNres.tab) <- c("$\\beta_0^{I}$", "$\\beta_1^{I}$",
                              "$\\beta_0^{II}$", "$\\beta_1^{II}$",
                              "$\\gamma_0$", "$\\gamma_{cos}$",
                              "$\\gamma_{sin}$", "$\\gamma_1$",
                              "$\\sigma$", "$\\sum_{1,1}$",
                              "$\\sum_{1,2}$", "$\\sum_{2,2}$")
colnames(CLPNGPNres.tab) <- c("Mode", "HPD LB", "HPD UB", "Mode", "HPD LB", "HPD UB")

kable(CLPNGPNres.tab, "latex", booktabs = TRUE, escape = FALSE,
      caption = "Results for the modified CL-PN and CL-GPN model")%>%
add_header_above(c("Parameter" = 1, "CL-PN" = 3, "CL-GPN" = 3))

```


```{r estSL, echo = FALSE}

ALres.tab <- matrix(round(c(resAL$par[1:4], resAL$par[6], resAL$par[5], resAL$par[7]), 2),7,1)
rownames(ALres.tab) <- c("$\\eta_0$", "$\\eta_1$",
                         "$\\nu_0$", "$\\nu_1$",
                         "$\\alpha$", "$\\kappa$", "$\\lambda$")

kable(ALres.tab, "latex", booktabs = T, escape = FALSE,
      caption = "Results for the modified Abe-Ley model")%>%
add_header_above(c("Parameter" = 1, "ML-estimate" = 1))

```


```{r estCLMGPN, echo = FALSE}

CLGPNM.lambda.m   <- mode_est(res_CLGPNM$lambda[5000:20000, ])
CLGPNM.lambda.hpd <- hpd_est(res_CLGPNM$lambda[5000:20000, ])

CLGPNM.Beta11.m <- mode_est(res_CLGPNM$Beta[1,1,5000:20000])
CLGPNM.Beta21.m <- mode_est(res_CLGPNM$Beta[2,1,5000:20000])
CLGPNM.Beta12.m <- mode_est(res_CLGPNM$Beta[1,2,5000:20000])
CLGPNM.Beta22.m <- mode_est(res_CLGPNM$Beta[2,2,5000:20000])
CLGPNM.Beta13.m <- mode_est(res_CLGPNM$Beta[1,3,5000:20000])
CLGPNM.Beta23.m <- mode_est(res_CLGPNM$Beta[2,3,5000:20000])

CLGPNM.Beta11.hpd <- hpd_est(res_CLGPNM$Beta[1,1,5000:20000])
CLGPNM.Beta21.hpd <- hpd_est(res_CLGPNM$Beta[2,1,5000:20000])
CLGPNM.Beta12.hpd <- hpd_est(res_CLGPNM$Beta[1,2,5000:20000])
CLGPNM.Beta22.hpd <- hpd_est(res_CLGPNM$Beta[2,2,5000:20000])
CLGPNM.Beta13.hpd <- hpd_est(res_CLGPNM$Beta[1,3,5000:20000])
CLGPNM.Beta23.hpd <- hpd_est(res_CLGPNM$Beta[2,3,5000:20000])

CLGPNM.Betacon11.m <- mode_est(res_CLGPNM$Betacon[1,1,5000:20000])
CLGPNM.Betacon21.m <- mode_est(res_CLGPNM$Betacon[2,1,5000:20000])
CLGPNM.Betacon12.m <- mode_est(res_CLGPNM$Betacon[1,2,5000:20000])
CLGPNM.Betacon22.m <- mode_est(res_CLGPNM$Betacon[2,2,5000:20000])
CLGPNM.Betacon13.m <- mode_est(res_CLGPNM$Betacon[1,3,5000:20000])
CLGPNM.Betacon23.m <- mode_est(res_CLGPNM$Betacon[2,3,5000:20000])

CLGPNM.Betacon11.hpd <- hpd_est(res_CLGPNM$Betacon[1,1,5000:20000])
CLGPNM.Betacon21.hpd <- hpd_est(res_CLGPNM$Betacon[2,1,5000:20000])
CLGPNM.Betacon12.hpd <- hpd_est(res_CLGPNM$Betacon[1,2,5000:20000])
CLGPNM.Betacon22.hpd <- hpd_est(res_CLGPNM$Betacon[2,2,5000:20000])
CLGPNM.Betacon13.hpd <- hpd_est(res_CLGPNM$Betacon[1,3,5000:20000])
CLGPNM.Betacon23.hpd <- hpd_est(res_CLGPNM$Betacon[2,3,5000:20000])

CLGPNM.Sigma11.m <- mode_est(res_CLGPNM$Sigma[1,1,5000:20000])
CLGPNM.Sigma12.m <- mode_est(res_CLGPNM$Sigma[1,2,5000:20000])
CLGPNM.Sigma13.m <- mode_est(res_CLGPNM$Sigma[1,3,5000:20000])
CLGPNM.Sigma22.m <- mode_est(res_CLGPNM$Sigma[2,2,5000:20000])
CLGPNM.Sigma23.m <- mode_est(res_CLGPNM$Sigma[2,3,5000:20000])
CLGPNM.Sigma33.m <- mode_est(res_CLGPNM$Sigma[3,3,5000:20000])

CLGPNM.Sigma11.hpd <- hpd_est(res_CLGPNM$Sigma[1,1,5000:20000])
CLGPNM.Sigma12.hpd <- hpd_est(res_CLGPNM$Sigma[1,2,5000:20000])
CLGPNM.Sigma13.hpd <- hpd_est(res_CLGPNM$Sigma[1,3,5000:20000])
CLGPNM.Sigma22.hpd <- hpd_est(res_CLGPNM$Sigma[2,2,5000:20000])
CLGPNM.Sigma23.hpd <- hpd_est(res_CLGPNM$Sigma[2,3,5000:20000])
CLGPNM.Sigma33.hpd <- hpd_est(res_CLGPNM$Sigma[3,3,5000:20000])

CLGPNM.Sigmacon11.m <- mode_est(res_CLGPNM$Sigmacon[1,1,5000:20000])
CLGPNM.Sigmacon12.m <- mode_est(res_CLGPNM$Sigmacon[1,2,5000:20000])
CLGPNM.Sigmacon13.m <- mode_est(res_CLGPNM$Sigmacon[1,3,5000:20000])
CLGPNM.Sigmacon22.m <- mode_est(res_CLGPNM$Sigmacon[2,2,5000:20000])
CLGPNM.Sigmacon23.m <- mode_est(res_CLGPNM$Sigmacon[2,3,5000:20000])
CLGPNM.Sigmacon33.m <- mode_est(res_CLGPNM$Sigmacon[3,3,5000:20000])

CLGPNM.Sigmacon11.hpd <- hpd_est(res_CLGPNM$Sigmacon[1,1,5000:20000])
CLGPNM.Sigmacon12.hpd <- hpd_est(res_CLGPNM$Sigmacon[1,2,5000:20000])
CLGPNM.Sigmacon13.hpd <- hpd_est(res_CLGPNM$Sigmacon[1,3,5000:20000])
CLGPNM.Sigmacon22.hpd <- hpd_est(res_CLGPNM$Sigmacon[2,2,5000:20000])
CLGPNM.Sigmacon23.hpd <- hpd_est(res_CLGPNM$Sigmacon[2,3,5000:20000])
CLGPNM.Sigmacon33.hpd <- hpd_est(res_CLGPNM$Sigmacon[3,3,5000:20000])

modes.GPN  <- c(CLGPNM.Beta11.m, CLGPNM.Beta12.m, CLGPNM.Beta13.m,
                CLGPNM.Beta21.m, CLGPNM.Beta22.m, CLGPNM.Beta23.m,
                CLGPNM.Sigma11.m, CLGPNM.Sigma22.m, CLGPNM.Sigma33.m,
                CLGPNM.Sigma12.m, CLGPNM.Sigma13.m, CLGPNM.Sigma23.m,
                CLGPNM.lambda.m)
hpd.LB.GPN <- c(CLGPNM.Beta11.hpd[1], CLGPNM.Beta12.hpd[1], CLGPNM.Beta13.hpd[1],
                CLGPNM.Beta21.hpd[1], CLGPNM.Beta22.hpd[1], CLGPNM.Beta23.hpd[1],
                CLGPNM.Sigma11.hpd[1], CLGPNM.Sigma22.hpd[1], CLGPNM.Sigma33.hpd[1],
                CLGPNM.Sigma12.hpd[1], CLGPNM.Sigma13.hpd[1], CLGPNM.Sigma23.hpd[1],
                CLGPNM.lambda.hpd[1])
hpd.UB.GPN <- c(CLGPNM.Beta11.hpd[2], CLGPNM.Beta12.hpd[2], CLGPNM.Beta13.hpd[2],
                CLGPNM.Beta21.hpd[2], CLGPNM.Beta22.hpd[2], CLGPNM.Beta23.hpd[2],
                CLGPNM.Sigma11.hpd[2], CLGPNM.Sigma22.hpd[2], CLGPNM.Sigma33.hpd[2],
                CLGPNM.Sigma12.hpd[2], CLGPNM.Sigma13.hpd[2], CLGPNM.Sigma23.hpd[2],
                CLGPNM.lambda.hpd[2])

modescon.GPN  <- c(CLGPNM.Betacon11.m, CLGPNM.Betacon12.m, CLGPNM.Betacon13.m,
                   CLGPNM.Betacon21.m, CLGPNM.Betacon22.m, CLGPNM.Betacon23.m,
                   CLGPNM.Sigmacon11.m, CLGPNM.Sigmacon22.m, CLGPNM.Sigmacon33.m,
                   CLGPNM.Sigmacon12.m, CLGPNM.Sigmacon13.m, CLGPNM.Sigmacon23.m,
                   CLGPNM.lambda.m)
hpdcon.LB.GPN <- c(CLGPNM.Betacon11.hpd[1], CLGPNM.Betacon12.hpd[1], CLGPNM.Betacon13.hpd[1],
                   CLGPNM.Betacon21.hpd[1], CLGPNM.Betacon22.hpd[1], CLGPNM.Betacon23.hpd[1],
                   CLGPNM.Sigmacon11.hpd[1], CLGPNM.Sigmacon22.hpd[1], CLGPNM.Sigmacon33.hpd[1],
                   CLGPNM.Sigmacon12.hpd[1], CLGPNM.Sigmacon13.hpd[1], CLGPNM.Sigmacon23.hpd[1],
                   CLGPNM.lambda.hpd[1])
hpdcon.UB.GPN <- c(CLGPNM.Betacon11.hpd[2], CLGPNM.Betacon12.hpd[2], CLGPNM.Betacon13.hpd[2],
                   CLGPNM.Betacon21.hpd[2], CLGPNM.Betacon22.hpd[2], CLGPNM.Betacon23.hpd[2],
                   CLGPNM.Sigmacon11.hpd[2], CLGPNM.Sigmacon22.hpd[2], CLGPNM.Sigmacon33.hpd[2],
                   CLGPNM.Sigmacon12.hpd[2], CLGPNM.Sigmacon13.hpd[2], CLGPNM.Sigmacon23.hpd[2],
                   CLGPNM.lambda.hpd[2])

CLGPNMres.tab <- round(cbind(modes.GPN, hpd.LB.GPN, hpd.UB.GPN, modescon.GPN, hpdcon.LB.GPN, hpdcon.UB.GPN),2)

rownames(CLGPNMres.tab) <- c("$\\beta_{0_s^{I}}$", "$\\beta_{0_s^{II}}$",
                             "$\\beta_{0_y}$", "$\\beta_{1_s^{I}}$",
                             "$\\beta_{1_s^{II}}$", "$\\beta_{1_y}$",
                             "$\\sum_{s_{1,1}}$", "$\\sum_{s_{2,2}}$",
                             "$\\sum_{y_{3,3}}$", "$\\sum_{s_{1,2}}$",
                             "$\\sum_{sy_{1,3}}$", "$\\sum_{sy_{2,3}}$",
                             "$\\lambda$")
colnames(CLGPNMres.tab) <- c("Mode", "HPD LB", "HPD UB", "Mode", "HPD LB", "HPD UB")


kable(CLGPNMres.tab, "latex", booktabs = TRUE, escape = FALSE, caption = "Results for the modified joint projected and skew normal model")%>%
add_header_above(c("Parameter" = 1, "Unconstrained" = 3, "Constrained" = 3))

```

Tables 2, 3 and 4 show results from the four models that were fit to the teacher
dataset. For the models in which Bayesian estimation was used we show both the 
estimated posterior mode and the 95\% highest posterior density (HPD) interval 
for each parameter. Before we turn to an evaluation of the fit of the models, we
discuss results of and the interpretation of parameters from the four models. We
will cover parameters concerning the circular outcome, linear outcome and the
association between them separately.

\subsubsection{Circular component}

Firstly, we can compare the estimated mean of the circular outcome between the
four models. In Table 5 we see the means of the posterior predictive
distributions for the circular outcome of the CL-PN, CL-GPN and joint GPN-SSN 
model. For the CL-PN model we can actually also compute this mean from the
estimates in Table 2 as follows: $\mu_{circ} = atan2(\beta_0^{II}, \beta_0^{I})
= 32.62^\circ$. The estimates for the CL-PN, CL-GPN and joint GPN-SSN  model are
about equal and correspond quite well to the actual data mean of 33.22$^\circ$
in Table \ref{Tableteacherdescriptives}. The estimate from the Abe-Ley model
which is 0.36 radians or 20.62$^\circ$ is different. This difference could be
caused by the fact that the densities for the circular ouctome, projected normal
or sine-skewed von Mises, differ between the models.


```{r means, echo = FALSE}
means <- round(rbind(mean_circ(res_CLPN$theta_pred[,5000:20000]),
                     mean_circ(res_CLGPN$theta_pred[,5000:20000]),
                     mean_circ(res_CLGPNM$theta_pred[,5000:20000]))*(180/pi), 2)

rownames(means) <- c("Cl-PN", "CL-GPN", "GPN-SSN")
colnames(means) <- "Mean (degrees)"
kable(means, "latex", booktabs = T, escape = FALSE, caption = "Posterior estimates for the circular mean in the CL-PN, CL-GPN and joint GPN-SSN models")%>%
   kable_styling() %>%
   add_footnote("Note that these means are based on the posterior predictive distribution for the intercepts following (Wang & Gelfand, 2013).", notation="alphabet")

```

We can also compare the effect of self-efficacy (\verb|SE|) on the circular
outcome. For the CL-PN model and Abe-Ley models we can get estimates of a
circular regression coefficient of this effect. For the Abe-Ley model this
estimate is the parameter $\eta_1 = -0.02$. This means that for each unit
increase in self-efficacy, at the inflection point of the circular regression
line, the score of the teacher on the IPC decreases with $0.02*(180/\pi) =
1.15^\circ$. \textcolor{red}{CHRISTOPHE: KLOPT DIT, OF MOET IK $\lambda$ HIER NOG MEENEMEN; NEGEER IK NU HET SINE-SKEWED GEDEELTE} The inflection point is the point at which the regression line
starts flattening off, i.e. the steepness of the slope decreases. For the CL-PN
model we can use methods from @CremersMulderKlugkist2017. The estimated
posterior mode of $b_c$, a parameter that is comparable to $\eta_1$ in the
Abe-Ley model, equals $1.53$ and its 95\% HPD interval is $(-17.37; 21.26)$.
This means that for each unit increase in self-efficacy, at the inflection point
of the circular regression line, the score of the teacher on the IPC increases
with $1.53*(180/\pi) = 87.66^\circ$. The values of $b_c$ and $\eta_1$ are thus
quite different even though they should represent the same effect. However, the
slope at the inflection point that $b_c$ and $\eta_1$ describe is not
necessarily representative of the effect of \verb|SE| in the data range. In
Figure \ref{regline} we see that the inflection point (square) of the predicted
circular regression line for the CL-PN model lies just ouside of the data range
and that for the Abe-Ley model even further (outside the x-axis range). The
slope of the regression lines in the data range is much more alike even though
one is positive and the other negative. The slope at the mean self-efficicay,
which has a value of 0 because it was centered, $SAM$ is estimated at $0.07
(-0.03; 0.17)$. The $SAM$ can be interpreted as the amount of change on the
circle (in radians) for a unit increase at the mean of the predictor variable.
However, for the CL-PN model the 95\% HPD interval of $SAM$ for the effect of
\verb|SE| includes 0 meaning that the value is not different from 0. THis
explains in part how the slopes of the two regression lines are predicted to be
different. For the Abe-Ley model standard errors of the parameters are not known
so we cannot formally test whether $\eta_1$ differs from 0. Neither is there a
known way of computing the effect at predictor values other than the one of the
inflection point.

For the CL-GPN and GPN-SSN we cannot compute circular regression coefficients. 
Instead, we will compute posterior predictive distributions for the predicted 
circular outcome of individuals scoring the minimum, maximum and average 
self-efficacy. The modes and 95\% HPD intervals of these posterior predictive 
distributions in the CL-GPN model are $\hat{\theta}_{SE_{min}} = 211.31^\circ 
(144.41^\circ \: 43.43^\circ)$, $\hat{\theta}_{SE_{mean}} = 25.82^\circ 
(335.55^\circ \: 145.11^\circ)$ and $\hat{\theta}_{SE_{max}} =30.96^\circ 
(6.72^\circ \: 74.35^\circ)$ respectively. For the GPN-SSN model the modes and
95\% HPD intervals of the posterior predictive distributions are
$\hat{\theta}_{SE_{min}} = 352.06^\circ (121.82^\circ \: 77.75^\circ)$,
$\hat{\theta}_{SE_{mean}} = 22.74^\circ (336.71^\circ \: 132.25^\circ)$ and
$\hat{\theta}_{SE_{max}} = 30.87^\circ (358.80^\circ \: 82.13^\circ)$
respectively. For both models we can thus conclude that as the self-efficacy
increases, the score of the teacher on the IPC moves counterlockwise. We can
however not reach any conclusions about the size of this effect or whether it is
different from zero.

```{r ppreg, echo = FALSE, results = FALSE}
mode_est_circ(res_CLGPN$theta_pred.min)
mode_est_circ(res_CLGPN$theta_pred.mean)
mode_est_circ(res_CLGPN$theta_pred.max)

hpd_est_circ(res_CLGPN$theta_pred.min)
hpd_est_circ(res_CLGPN$theta_pred.mean)
hpd_est_circ(res_CLGPN$theta_pred.max)

mode_est_circ(res_CLGPNM$theta_pred.min)
mode_est_circ(res_CLGPNM$theta_pred.mean)
mode_est_circ(res_CLGPNM$theta_pred.max)

hpd_est_circ(res_CLGPNM$theta_pred.min)
hpd_est_circ(res_CLGPNM$theta_pred.mean)
hpd_est_circ(res_CLGPNM$theta_pred.max)

```

```{r circreg, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

tikz("Plots/reglinediffSE.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

a1 <- mode_est(res_CLPN$BI[5000:20000, 1])
b1 <- mode_est(res_CLPN$BI[5000:20000, 2])
a2 <- mode_est(res_CLPN$BII[5000:20000, 1])
b2 <- mode_est(res_CLPN$BII[5000:20000, 2])
ax <- -(a1*b1 + a2*b2)/(b1^2 + b2^2)
ac <- atan2(a2 + b2*ax, a1 + b1*ax)

x <- seq(-5, 5, by = 0.1)

pred.AL <- 0.37 + 2*atan(-0.01*x)
pred.PN <- atan2(a2 + b2*x, a1 + b1*x)

plot(Dat.test$SEc, theta.test*(180/pi), bty = "n", yaxt = "n",
     xlim = c(-5, 2), ylab = "$\\theta_{test}$", xlab = "self-efficacy")
axis(side = 2,  at = c(-160, -80, 0, 80, 160))
points(x, pred.AL*(180/pi), type = "l", lty = 1)
points(x, pred.PN*(180/pi), type = "l", lty = 2)
points(ax, ac*(180/pi), pch = 15)

dev.off()
tools::texi2dvi("Plots/reglinediffSE.tex", pdf=TRUE)

```

```{r reg.coef, cache = TRUE, echo = FALSE, results = FALSE}

a1 <- res_CLPN$BI[5000:20000, 1]
b1 <- res_CLPN$BI[5000:20000, 2]
a2 <- res_CLPN$BII[5000:20000, 1]
b2 <- res_CLPN$BII[5000:20000, 2]

ax <- -(a1*b1 + a2*b2)/(b1^2 + b2^2)
ac <- atan2(a2 + b2*ax, a1 + b1*ax)
bc <- -tan(atan2(a2, a1)-ac)/ax

mode_est(ax)

mode_est(bc)
hpd_est(bc)

mode_est(bc/(1 + (bc*(-ax))^2))
hpd_est(bc/(1 + (bc*(-ax))^2))

```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/reglinediffSE.pdf}
\caption{Plot showing circular regresion lines for the effect of self-efficacy as predicted by the Abe-Ley (solid line) and CL-PN model (dashed line). The black square indicated the inflection point of the circular regression line for the CL-PN model.}
\label{regline}
\end{figure}


\subsubsection{Linear component}

For the linear outcome, we compare the regression coefficients of \verb|SE| for
the linear outcome, the strength of the interpersonal behavior of a teacher. For
the CL-PN, CL-GPN and joint GPN-SSN model this are the parameters $\gamma_1$ and
$\beta_{1_y}$ respectively. Note that in the CL-PN and CL-GPN model the effect 
of \verb|SE| is the effect controlled for $\cos\theta$ and $\sin\theta$ similar 
to a 'normal' multiple regression model. For the GPN-SSN model the effect of
\verb|SE| is also controlled for the circular outcome. However in this model the
associaton is modelled in a way that is similar to modelling the association
between several outcome variables in a multivariate regression or ANOVA model.
Additionally, the distribution we assume for the linear outcome is different.
For the CL-PN and CL-GPN models we assume a normal distribution while for the
GPN-SSN model we assume a skewed normal distribution. These differences will
lead to differences in the estimated effect of \verb|SE| on the linear outcome.
The interpretation of the regression coefficients is however similar for all
three models and is like the interpretation of a regression coefficient in any
other standard linear regression model.

For all three models there is an effect of \verb|SE| on the strength of the 
interpersonal behavior of a teacher, the HPD interval does not include 0:
$\gamma_1 = 0.03 \: (0.00; 0.07)$ (CL-PN), $\gamma_1 = 0.03 \: (0.00; 0.06)$
(CL-GPN) and $\beta_{1_y} = 0.08 \: (0.05; 0.12)$ (GPN-SSN). This means that
teachers with a higher self-efficacy for classroom management are stronger in
their interpersonal behavior than teachers with a lower self-efficacy. In
particular, for each unit increase in self-efficacy the strength of their
interpersonal behavior is 0.03 or 0.08 (depending on the model) higher. This is
an increase of 4.17 or 11.11 \% considering the range of the strength on the IPC
in the data.

For the Abe-Ley model we make use of the fact that the conditional distribution
for the linear outcome is Weibull. This means that we can use methods from
survival analsis in medical statistics to interpret the effect of self-efficacy.
In survival analysis they make use of a 'survival' function in which the time is
plotted against the probability of survival of subjects suffering from a
specific medical condition. In our data however we plot the strength on the IPC
(instead of time) against the probability of a teacher having such a strength.
This probability can be computed using the so-called 'survival-function' which
in our case is equal to $\exp(-\alpha y_i^{\beta})$ where $\beta = \exp(\nu_0 +
\nu_1SE_i)$. In Figure \ref{reglineweib} we plot the survival function for the
minimum, mean and maximum value of self-efficacy. From this Figure we conclude
that the stronger interpersonal behaviors are less probable. We also see that
the self-efficacy positively influences the strength on the IPC, the probability
of having a stronger interpersonal behavior increases with increasing
self-efficacy. This result is equivalent to the effect of self-efficacy found
using the CL-PN, CL-GPN and GPN-SSN models.

```{r survivalplot, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}
summary(Dat.test$SEc)

x <- c(-3.5, 0, 2)
y <- seq(0, 1, by = 0.05)
h <- matrix(NA, length(y), length(x))
s <- matrix(NA, length(y), length(x))
 
for(j in 1:length(x)){
  
  for(i in 1:length(y)){
    beta <- exp(resAL$par[3] + resAL$par[4]*x[j])
    h[i,j] <- beta*resAL$par[6]*y[i]^(resAL$par[6]-1)
    s[i,j] <- exp(-resAL$par[6]*y[i]^beta)
  }
  
}

tikz("Plots/survivaldiffSE.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

plot(y, s[,1], type = "l", ylab = "P(strength IPC)", xlab = "strength IPC", bty = "n")
points(y, s[,2], type = "l", lty = 2)
points(y, s[,3], type = "l", lty = 3)
legend(0.7, 1, legend = c("min(SE)", "mean(SE)", "max(SE)"), lty = c(1,2,3), bty = "n")

dev.off()
tools::texi2dvi("Plots/survivaldiffSE.tex", pdf=TRUE)


```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/survivaldiffSE.pdf}
\caption{Plot showing the probability of having a particular strength of interersonal behavior (survival plot) for the minimum, mean and maximum self-efficacy in the data.}
\label{reglineweib}
\end{figure}


\subsubsection{Association between the linear and circular component}

For all four models we can also investigate the association between the linear 
and circular component. In the CL-PN and CL-GPN model we do this using the 
parameters $\gamma_{\cos}$ and $\gamma_{sin}$, the coefficients of the
regression of the linear outcome onto the sine and cosine of the circular
outcome. In the joint SSN-GPN model we look at the covariances between the
linear outcome and the sine and cosine of the circular outcome $\sum_{sy_{2,3}}$
and $\sum_{sy_{1,3}}$. For the Abe-Ley model we can use the model parameters in
Table 3 to compute a circular-linear correlation [@abe2017tractable].

In both the CL-PN and CL-GPN model there is an effect of the cosine of the 
circular outcome onto the linear outcome (HPD interval does not include 0). In 
the teacher data the sine and cosine component have a substantive meaning. In 
this case the Communion component of the IPC positively effects the strength of 
a teachers' type of interpersonal behavior. Thus the teachers showing 
interpersonal behavior types with higher communion scores (e.g. 'helpful' and 
'understanding' in Figure \ref{QTI}) are stronger in their behavior. In the 
joint SSN-GPN model we reach a sligtly different conclusion. In this model both 
covariances, $\sum_{sy_{2,3}} = 0.09$ and $\sum_{sy_{1,3}} = 0.23$, are 
different from 0, but the one between the cosine of the circular outcome and the
linear outcome is larger. This means that both the Communion and Agency 
component of the IPC positively effect the strength of a teachers' type of 
interpersonal behavior but the effect of the Communon component is larger. Thus,
teachers scoring high on Agency but slightly higher on Communion (the 'helpful' 
category) are strongest in their behavior. Following @abe2017tractable the
estimated correlation between the circular and linear outcome is equal to
\textcolor{red}{CHRISTOPHE: KAN JIJ DEZE BEREKENEN, IN R KAN IK GEEN MANIER
VINDEN OM LEGENDRE FUNCTIES MET NIET INTEGER DEGREE TE EVALUEREN EN ALS IK
http://functions.wolfram.com/webMathematica/FunctionEvaluation.jsp?name=LegendreP2General
GEBRUIKT KRIJG IK IMAGINAIRE GETALLEN (voor m = 1 en m = 2), IK WEET NIET OF DAT
DE BEDOELING IS..}. This correlation is not affected by $\mu$ or $\beta$ meaning
that the covariate, self-efficacy, can not influence the correlation between the
type and strength of interpersonal behavior.

```{r correlation, echo = FALSE, eval = FALSE, results = FALSE}

require(pracma)

alpha <- resAL$par[6]
kappa <- resAL$par[5]
lambda <- resAL$par[7]

#R cannot compute legendre functions of non-integer degree
#leg.0 <- legendre(1/alpha, cosh(kappa))[1,] 
#leg.1 <- legendre(1/alpha, cosh(kappa))[2,]
#leg.2 <- legendre(1/alpha, cosh(kappa))[3,]

#so we use http://functions.wolfram.com/webMathematica/FunctionEvaluation.jsp?name=LegendreP2General

1/3.82
cosh(1.58)




```

\subsubsection{Model fit}

Instead of interpreting specific model parameters we can also look at the 
overall fit of the models to the data. In Section \ref{Modelfit} we introduced 
the criterion, PLSL, that we use to assess model fit in this paper. This
criterion is focused on the out-of-sample predictive performance. Table 6 shows
the values of this criterion for the linear and circular outcome of the four
different models fit to the teacher data. 

The CL-PN and CL-GPN model have the best out-of-sample predictive performance 
for the linear outcome. Note that they have roughly the same performance since 
they model the linear outcome in the same way apart from the value of $r$ in 
\eqref{circlinlink} which is a parameter computed in the estimation of the 
parameters for the circular variable. This similarity in fit is also shown in 
Figure \ref{preddist} which shows histograms of the test and holdout data
together with the (posterior) predicitive distributions for all four cylindrical
models. The posterior predictive distributions for the CL-PN and CL-GPN model
are almost the same for the linear outcome.

The CL-GPN model has the best out-of-sample predicitve performance for the 
circular outcome. The Abe-Ley model has the second best performance. This means 
that for the circular variable a slightly skewed distribution (see Figure 
\ref{preddist}) fits best. The GPN-SSN model fits worst even though it assumes
the same distribution for the circular outcome as the CL-GPN model. This could
be due to the fact that the relation between the circular and linear outcome is 
modeled differently in these two models. In the CL-GPN model the linear outcome 
depends on the circular one following the relation in \eqref{circlinlink} but 
the circular outcome is not dependent on the linear one. In the GPN-SSN model 
both outcomes are modelled with a joint variance-covariance matrix governing 
their dependence. In Figure \ref{preddist} we see however that the actual 
predictive densities do not differ that much between the four models. It is 
possible that the large difference in PLSL are caused by the fact that they are 
computed for a relatively small holdout dataset (n = 15) 
\textcolor{red}{CHRISTOPHE: IS DIT LOGISCH?}.

```{r ModelFit, echo = FALSE}

CLGPNM.ll.circ <- -2*mean(res_CLGPNM$ll.circ[5000:20000])
CLGPNM.ll.lin  <- -2*mean(res_CLGPNM$ll.lin[5000:20000])

CLGPN.ll.circ <- -2*mean(res_CLGPN$ll.circ[5000:20000])
CLGPN.ll.lin  <- -2*mean(res_CLGPN$ll.lin[5000:20000])

CLPN.ll.circ <- -2*mean(res_CLPN$ll.circ[5000:20000])
CLPN.ll.lin  <- -2*mean(res_CLPN$ll.lin[5000:20000])
 
AL.ll.circ <- -2*func.reg.cond.circ(resAL$par, dat.h)
AL.ll.lin  <- -2*func.reg.cond.lin(resAL$par, dat.h)

PLSL <- round(rbind(c(CLPN.ll.circ, CLGPN.ll.circ, AL.ll.circ, CLGPNM.ll.circ),
                    c(CLPN.ll.lin, CLGPN.ll.lin, AL.ll.lin, CLGPNM.ll.lin)), 2)
rownames(PLSL) <- c("circular", "linear")
colnames(PLSL) <- c("CL-PN", "CL-GPN", "Abe-Ley", "Joint GPN-SSN")

kable(PLSL, "latex", booktabs = T, escape = FALSE, caption = "PLSL criteria for the circular and linear outcome in the four cylindrical models")

```

```{r preddist, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

source("R-code/Sample Abe-Ley.R")

ALdy  <- matrix(NA, length(y.test), 1000)
ALdth <- matrix(NA, length(theta.test), 1000)

ALdy.h  <- matrix(NA, length(y.hold), 1000)
ALdth.h <- matrix(NA, length(theta.hold), 1000)

tikz("Plots/preddistdiffSE.tex", standAlone =TRUE, height = 6, width = 6, pointsize = 12, engine = "pdftex")

layout(matrix(c(1,2,3, 4,5,5), 3, 2, byrow = TRUE),heights=c(2,2,1))

par(mar = c(5,5,1,1))

x.h <- Dat.hold$SEc
x   <- Dat.test$SEc

for(i in 1:length(theta.test)){
  beta <- exp(resAL$par[3] + resAL$par[4]*x[i])
  mu   <- resAL$par[1] + 2*atan(resAL$par[2]*x[i])

  dat <- rweiSSVM(1000, beta = beta, alpha = resAL$par[6], mu = mu,
                  kappa = resAL$par[5], lambda = resAL$par[7])
  
  ALdy[i,] <- dat$x
  ALdth[i,] <- dat$theta
}

for(i in 1:length(theta.hold)){
  beta <- exp(resAL$par[3] + resAL$par[4]*x.h[i])
  mu   <- resAL$par[1] + 2*atan(resAL$par[2]*x.h[i])

  dat <- rweiSSVM(1000, beta = beta, alpha = resAL$par[6], mu = mu,
                  kappa = resAL$par[5], lambda = resAL$par[7])
  
  ALdy.h[i,]  <- dat$x
  ALdth.h[i,] <- dat$theta
  
}

hist(theta.test, prob = TRUE, ylim = c(0, 1.25),
     main = "", ylab = "Circular", xlab = "$\\theta_{test}$")
lines(density(res_CLPN$theta_pred[,19000:20000]), lty = 1)
lines(density(res_CLGPN$theta_pred[,19000:20000]), lty = 2)
lines(density(ALdth), lty = 3)
lines(density(res_CLGPNM$theta_pred[,19000:20000]), lty = 4)

hist(theta.hold, prob = TRUE, ylim = c(0, 1.25),
     main = "", ylab = "Circular", xlab = "$\\theta_{holdout}$")
lines(density(res_CLPN$theta_pred.hold[,19000:20000]), lty = 1)
lines(density(res_CLGPN$theta_pred.hold[,19000:20000]), lty = 2)
lines(density(ALdth.h), lty = 3)
lines(density(res_CLGPNM$theta_pred.hold[,19000:20000]), lty = 4)

hist(y.test, prob = TRUE, ylim = c(0, 3),
     main = "", ylab = "Linear", xlab = "$y_{test}$")
lines(density(res_CLPN$y_pred[,5000:20000]), lty = 1)
lines(density(res_CLGPN$y_pred[,5000:20000]), lty = 2)
lines(density(ALdy), lty = 3)
lines(density(res_CLGPNM$y_pred[,5000:20000]), lty = 4)

hist(y.hold, prob = TRUE, ylim = c(0, 4),
     main = "", ylab = "Linear", xlab = "$y_{holdout}$")
lines(density(res_CLPN$y_pred.hold[,5000:20000]), lty = 1)
lines(density(res_CLGPN$y_pred.hold[,5000:20000]), lty = 2)
lines(density(ALdy.h), lty = 3)
lines(density(res_CLGPNM$y_pred.hold[,5000:20000]), lty = 4)

par(mar = c(2,2,1,1))

plot(1, type = "n", axes=FALSE, xlab="", ylab="")
legend(x = "top", inset = 0,
       legend = c("CL-PN", "CL-GPN", "Abe-Ley", "GPN-SSN"),
       lty = c(1,2,3,4), horiz = TRUE)

dev.off()

tools::texi2dvi("Plots/preddistdiffSE.tex",pdf=TRUE)

```

\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{Plots/preddistdiffSE.pdf}
\caption{Histograms of linear and circular values of the test and holdout set of the teacher data plotted together with (posterior) predictive density estimates for the modified CL-PN, CL-GPN, Abe-Ley and joint GPN-SSN models based on $n \times 1000$ samples from the (posterior) predictive distribution.}
\label{preddist}
\end{figure}




\section{Discussion}\label{Discussion}

In this paper we have modified four models for cylindrical data such that they 
include a regression of both the linear and circular components onto a set of 
covariates. Subsequently we have used these four methods to analyze a dataset on
the interpersonal behavior of teachers. Here we will comment on the differences
between these models, the results from the analysis of the teacher data and how
cylindrical models improve the analysis of this type of data; data from an
interpersonal circumplex.

In terms of interpretability, the CL-PN and Abe-Ley model score best. As shown 
in the previous section it is relatively easy to interpret the parameters of the
linear and the circular component for these models. In the CL-GPN and joint 
GPN-SSN model the interpretation of the parameters of the circular component it 
not easy, if at all possible. This is caused by the fact that in addition to the
mean vector the covariance matrix of the GPN distribution affects the location 
of the circular data, making it difficult to compute regression coefficients on 
the circle. @wang2012directional state that we can use Monte Carlo integration 
to compute a circular mean and variance for the GPN distribution. This might 
also be a solution to computing coefficients on the circle and could be applied 
to the methods of @CremersMulderKlugkist2017 such that they can be used for 
circular regression coefficients in GPN models as well. \textcolor{red}{HIER OOK
NOG EEN OPMERKING OVER HOE DE ASSOCIATION TUSSEN CIRCULAR AND LINEAR COMPONENT
WORDT GEMODELLEERD}.

In terms of flexibility the joint GPN-SSN scores best. In this model multiple 
linear and circular outcomes can be included and we can thus apply it to 
multivariate cylindrical data. In addition for both the linear anc circular 
components this model and the CL-GPN and CL-PN models are extendable to a 
mixed-effects model and can thus also be fit to longitudinal data. Extension to 
mixed-effects, longitudinal (see @nunez2014bayesian and @hernandez2016general 
for hierarchical/mixed-effects models for the PN and GPN distributions 
respectively). For the Abe-Ley model this may also be possible and has been done
for the conditional distribution of its linear component, the Weibull 
distribution, but has not been done in previous literature for the conditional 
distribution of its circular component (sine-skewed von 
Mises)\textcolor{red}{CHRISTOPHE: KLOPT DIT?}. In addition, the joint GPN-SSN
model allows for non-symmetrical shapes of the distributions of both the linear
(sine-skewed normal) and the circular (general projected normal) components. The
Abe-Ley model also allows for this and the CL-GPN model allows for a
non-symmetrical shape of the circular component.

To investigate model fit for the teacher data we asessed the fit to the linear
and circular component separately using the PLSL, a criterion that is designed
to assess the out-of-sample preditive performance. The CL-PN and CL-GPN model
have the best fit for the linear component and the CL-GPN model has the best fit
for the circular component. Differences in fit, in addition to being a result of
the different distributions that were used to model the linear and circular
component of the data, may also be caused by the way in which the relation
between the linear and circular component is modeled. Whereas in both the
Abe-Ley and joint GPN-SSN model the distribution of the linear component is
conditional on the circular component and vice versa, the distribution of the
circular component in the CL-PN and CL-GPN model is independent of the linear
component. In these models the linear component are related through a regression
structure where the circular component serves as a predictor and the linear
component as the outcome. We also investigated the (posterior) predictive
distributions of each model and its fit to the training data, the part of the
teacher data the model was fit on, and the holdout data, the part of the teacher
data that we used to assess the out-of-sample predictive performance. From these
plots we saw that whereas the PLSL criteria seem to indicate a substantive
difference in fit, the (posterior) predicitve distributions seem to be quite
similar (especially for the circular component). This discrepancy might be
caused by the fact that the holdout dataset, for which the PLSL criteria were
computed, is quite small and thus that small deviations in fit could lead to
large deviations in the criterion value.

The four cylindrical models that were modified to the regression context in this
paper are not the only cylindrical distributions available from the literature.
Several other cylindrical distributions, amongst which are @kato2008dependent,
@sugasawa2015flexible and @fernandez2007models have been introduced (for more
references we refer to chapter 2 of @ley2017modern). In the present research we
have decided not to include these models for reasons of space, complexity of the
models and ease of implementing a regression structure. In further research
however it would be interesting to investigate other types of cylindrical
distributions as well.

Even though some of the models have downsides regarding interpretation, e.g. the
Abe-Ley model does not provide standard errors and parameters from the models 
with a GPN distribution for the circular component are hard to interpret, 
cylindrical models in general offer new insights into data of a cylindrical 
nature in psychology. Concerning the particular example in this paper, the 
teacher data that were measured using an interpersonal circumplex, we were able 
to analyze all information in the data simultaneously. Before, the two 
components from the interpersonal circumplex were analyzed separately. With such
an approach we get information about the strength of a teachers' score on the 
two axes of interpersonal behavior, in our case Agency and Communion. This 
however does not allow us to distinguish between different types of behavior 
that are specific combinations of Agency and Communion. A solution to that would
be to treat circumplex data as circular [@Cremers2018Assessing]. This solution 
however, does not retain the information about the strength of the specific type
of interpersonal behavior a teacher displays. This is possible using the 
cylindrical models in the present paper, we can simultaneosly model the 
information about the type (a circular variable) and strength (a linear 
variable) of interpersonal behavior that a teacher displays and the association
between them.









\newpage
\section*{References}
<div id="refs"></div>


\newpage
\begin{appendices}
\section{Appendix}\label{Appendix}

In this appendix we outline the MCMC procedures to fit the cylindrical regression models from Section \ref{Models}. R-code for the MCMC sampler and the analysis of the teacher data can be found here: \url{https://github.com/joliencremers/CylindricalComparisonCircumplex}.

\subsection{Bayesian Model and MCMC procedure for the modified CL-PN model}\label{A1}

We use the following algorithm to obtain posterior estimates from the model:

\begin{enumerate}

\item Split the data, with the circular outcome $\boldsymbol{\theta} = \theta_i, \dots, \theta_n$, the linear outcome $\boldsymbol{y} = y_i, \dots, y_n$ where $n$ is the sample size, and the design matrices $\boldsymbol{Z}^k$ and $\boldsymbol{X}$ for the two components of the circular and the linear outcome respectively in a training (90\%) and holdout (10\%) set.

\item Define the prior parameters for the training set. In this paper we use:

\begin{itemize}
\item Prior for $\boldsymbol{\gamma}$: $N_q(\boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_{0})$, with  $\boldsymbol{\mu}_{0} = (0,0,0,0)^t$ and  $\boldsymbol{\Lambda}_{0} = 10^{-4}\boldsymbol{I}_4$.
\item Prior for $\sigma^2$: $IG(\alpha_{0}, \beta_{0})$, an inverse-gamma prior with $\alpha_{0} = 0.001$ and  $\beta_{0} = 0.001$.
\item Prior for $\boldsymbol{\beta^{k}}$: $N_2(\boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_{0})$, with $\boldsymbol{\mu}_{0} = (0,0)^t$ and  $\boldsymbol{\Lambda}_{0} = 10^{-4}\boldsymbol{I}_2$ for $k \in I,II$.
\end{itemize}

\item Set starting values $\boldsymbol{\gamma} = (0,0,0,0)^t$, $\sigma^2 = 1$ and $\boldsymbol{\beta^{k}} = (0,0)$ for $k \in I,II$. Also set starting values $r_i = 1$ in the training and holdout set. 

\item Compute the latent bivariate outcome $\boldsymbol{s}_i = (s_i^{I}, s_i^{II})^t$ underlying the circular outcome for the holdout and training dataset as follows:

$$\begin{bmatrix} s^{I}_{i} \\ s^{II}_{i} \end{bmatrix} = \begin{bmatrix} r_i \cos \theta_i \\  r_i\sin \theta_i\end{bmatrix}$$

\item Sample $\boldsymbol{\gamma}$, $\sigma^2$ and $\boldsymbol{\beta^{k}}$ for $k \in I,II$ for the training dataset from their conditional posteriors:

\begin{itemize}
\item Posterior for $\boldsymbol{\gamma}$: $N_q(\boldsymbol{\mu}_n, \sigma^2\boldsymbol{\Lambda}^{-1}_n)$, with $\boldsymbol{\mu}_n = (\boldsymbol{X}^t\boldsymbol{X} + \boldsymbol{\Lambda}_0)^{-1}(\boldsymbol{\Lambda}_0\boldsymbol{\mu}_0 + \boldsymbol{X}^t\boldsymbol{y})$ and $\boldsymbol{\Lambda}_n = (\boldsymbol{X}^t\boldsymbol{X} + \boldsymbol{\Lambda}_0)$.
\item Posterior for $\sigma^2$: $IG(\alpha_{n}, \beta_{n})$, an inverse-gamma posterior with $\alpha_{n} = \alpha_0 + n/2$ and $\beta_{n} = \beta_0 + \frac{1}{2}(\boldsymbol{y}^t\boldsymbol{y} + \boldsymbol{\mu}_{0}^t\boldsymbol{\Lambda}_0\boldsymbol{\mu}_{0} + \boldsymbol{\mu}_{n}^t\boldsymbol{\Lambda}_n\boldsymbol{\mu}_{n})$.
\item Posterior for $\boldsymbol{\beta^{k}}$: $N_2(\boldsymbol{\mu}^k_n, \boldsymbol{\Lambda}^{k}_n)$, with $\boldsymbol{\mu}^k_n = ((\boldsymbol{Z}^k)^t\boldsymbol{Z}^k + \boldsymbol{\Lambda}^k_0)^{-1}(\boldsymbol{\Lambda}^k_0\boldsymbol{\mu}^k_0 + (\boldsymbol{Z}^k)^t\boldsymbol{s}^k)$ and $\boldsymbol{\Lambda}^{k}_n = ((\boldsymbol{Z}^k)^t\boldsymbol{Z}^k + \boldsymbol{\Lambda}^k_0)$.
\end{itemize}

\item Sample new $r_i$ for the training and holdout dataset from the following posterior:

$$f(r_i \mid \theta_i, \boldsymbol{\mu}_i) \propto r_i \exp{(-\frac{1}{2}(r_i)^2 + b_ir_i)}$$ 
where $b_i = \begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}^t\boldsymbol{\mu}_i$, $\boldsymbol{\mu}_i = \boldsymbol{z_i}\boldsymbol{B}$ and $\boldsymbol{B} = (\boldsymbol{\beta}^{I}, \boldsymbol{\beta}^{II})$. 

We can sample from this posterior using a slice sampling technique (Cremers et al., 2018): 

\begin{itemize}
\item In a slice sampler the joint density for an auxiliary variable $v_{i}$ with $r_{i}$ is:


$$p(r_{i}, v_{i}\mid \theta_{i}, \boldsymbol{\mu}_{i}=\boldsymbol{z}_{i}\boldsymbol{B}) \propto r_{i} \textbf{I}\left(0 < v_i < \exp\left\{ -\frac{1}{2}(r_{i} - b_{i})^2\right\}\right)\textbf{I}(r_i > 0).$$

\noindent The full conditional for $v_{i}$, $p(v_{i} \mid r_{i},\boldsymbol{\mu}_{i}, \theta_{i})$, is:

$$U\left(0, \exp\left\{-\frac{1}{2}(r_{i} -  b _{i})^2\right\}\right)$$

and the full conditional for $r_i$, $p(r_{i} \mid v_{i},\boldsymbol{\mu}_{i}, \theta_{i})$, is proportional to:
$$r_{i} \textbf{I}\left(b_{i} + \max\left\{-b_{i}, -\sqrt{-2\ln v_{i}}\right\} < r_{i} < b_{i} + \sqrt{-2\ln v_{i}}\right).$$

\noindent We thus sample $v_{i}$ from the uniform distribution specified above. Independently we sample a value $m$ from $U(0,1)$. We obtain a new value for $r_{i}$ by computing $ r_{i} = \sqrt{(r_{i_{2}}^{2}-r_{i_{1}}^{2})m + r_{i_{1}}^{2}}$ where $r_{i_{1}}=b_{i} +\max\left\{-b_{i}, -\sqrt{-2\ln v_{i}}\right\}$ and $ r_{i_{2}}= b_{i} + \sqrt{-2\ln v_{i}}$.
\end{itemize}

\item Compute the PLSL for the circular and linear outcome on the holdout set using the estimates of $\boldsymbol{\gamma}$, $\sigma^2$ and $\boldsymbol{\beta^{k}}$ for $k \in I,II$ for the training dataset.

\item Repeat steps 4 to 8 until the sampled parameter estimates have converged.

\end{enumerate}





\newpage
\subsection{Bayesian Model and MCMC procedure for the modified CL-GPN mode}\label{A2}

We use the following algorithm to obtain posterior estimates from the model:

\begin{enumerate}

\item Split the data, with the circular outcome $\boldsymbol{\theta} = \theta_i, \dots, \theta_n$, the linear outcome $\boldsymbol{y} = y_i, \dots, y_n$ where $n$ is the sample size, and the design matrices $\boldsymbol{Z}^k$ and $\boldsymbol{X}$ for the two components of the circular and the linear outcome respectively in a training (90\%) and holdout (10\%) set.

\item Define the prior parameters for the training set. In this paper we use:

\begin{itemize}
\item Prior for $\boldsymbol{\gamma}$: $N_q(\boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_{0})$, with $\boldsymbol{\mu}_{0} = (0,0,0,0)^t$ and $\boldsymbol{\Lambda}_{0} = 10^{-4}\boldsymbol{I}_4$.
\item Prior for $\sigma^2$: $IG(\alpha_{0}, \beta_{0})$, an inverse-gamma prior with $\alpha_{0} = 0.001$ and $\beta_{0} = 0.001$.
\item Prior for $\boldsymbol{\beta}_{j}$: $N_2(\boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_0)$, with $\boldsymbol{\mu}_{0} = (0,0)^t$ and  $\boldsymbol{\Sigma}_{0} = 10^{5}\boldsymbol{I}_2$ for $j \in 1, \dots, p$ where $p$ is the number of covariates in $\boldsymbol{Z}$.
\item Prior for $\rho$: $N(\mu_0, \sigma^2)$, with $\mu_0 = 0$ and $\sigma^2 = 10^{4}$.
\item Prior for $\tau$: $IG(\alpha_{0}, \beta_{0})$, an inverse gamma prior with $\alpha_{0} = 0.01$ and $\beta_{0} = 0.01$.
\end{itemize}


\item Set starting values $\boldsymbol{\gamma} = (0,0,0,0)^t$, $\sigma^2 = 1$, $\boldsymbol{\beta}_j = (0,0)^t$, $\rho = 0$, $\tau = 1$ and $\boldsymbol{\Sigma} = \begin{bmatrix} \tau^2 + \rho^2 & \rho\\ \rho & 1 \end{bmatrix}$. Also set starting values $r_i = 1$ in the training and holdout set. 

\item Compute the latent bivariate outcome $\boldsymbol{s}_i = (s_i^{I}, s_i^{II})^t$ underlying the circular outcome for the holdout and training dataset as follows:

$$\begin{bmatrix} s^{I}_{i} \\ s^{II}_{i} \end{bmatrix} = \begin{bmatrix} r_i \cos \theta_i \\  r_i\sin \theta_i\end{bmatrix}$$

\item Sample $\boldsymbol{\gamma}$, $\sigma^2$, $\boldsymbol{\beta}_j$, $\rho$ and $\tau$ for the training dataset from their conditional posteriors:

\begin{itemize}
\item Posterior for $\boldsymbol{\gamma}$: $N_q(\boldsymbol{\mu}_n, \sigma^2\boldsymbol{\Lambda}^{-1}_n)$, with $\boldsymbol{\mu}_n = (\boldsymbol{X}^t\boldsymbol{X} + \boldsymbol{\Lambda}_0)^{-1}(\boldsymbol{\Lambda}_0\boldsymbol{\mu}_0 + \boldsymbol{X}^t\boldsymbol{y})$ and $\boldsymbol{\Lambda}_n = (\boldsymbol{X}^t\boldsymbol{X} + \boldsymbol{\Lambda}_0)$.
\item Posterior for $\sigma^2$: $IG(\alpha_{n}, \beta_{n})$, an inverse-gamma posterior where $\alpha_{n} = \alpha_0 + n/2$ and $\beta_{n} = \beta_0 + 0.5(\boldsymbol{y}^t\boldsymbol{y} + \boldsymbol{\mu}_{0}^t\boldsymbol{\Lambda}_0\boldsymbol{\mu}_{0} + \boldsymbol{\mu}_{n}^t\boldsymbol{\Lambda}_n\boldsymbol{\mu}_{n})$.
\item Posterior for $\boldsymbol{\beta}_j$: $N_2(\boldsymbol{\mu}_{j_{n}}, \boldsymbol{\Sigma}_{j_{n}})$, with $\boldsymbol{\mu}_{j_{n}} = \boldsymbol{\Sigma}_{j_{n}}\boldsymbol{\Sigma}^{-1}\Bigg(-\sum_{i=1}^{n}z_{i,j-1}\sum_{l\neq j}z_{i,l-1}\boldsymbol{\beta}_l + \sum_{i=1}^{n}z_{i,j-1}r_i\begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}\Bigg)$ and  $\boldsymbol{\Sigma}_{j_{n}} = \Big(\sum_{i=1}^{n}z_{i,j-1}^2\boldsymbol{\Sigma}^{-1}+\boldsymbol{\Lambda}_0\Big)^{-1}$ for $j \in 1, \dots, p$ where p is the number of covariates in $\boldsymbol{Z}$.
\item Posterior for $\rho$: $N(\mu_n, \sigma^2_n)$, with $\mu_n = \frac{\tau^{-2} \sum_{i=1}^{n}(s^{I}_{i} - \mu_i^{I})(s^{II}_{i} - \mu_i^{II}) + \mu_0\sigma_0^{-2}}{\tau^{-2}\sum_{i=1}^{n}(s^{II}_{i} - \mu^{II})^2 + \sigma_0^{-2}}$ and $\sigma_n^2 = \frac{1}{\tau^{-2}\sum_{i=1}^{n}(s^{II}_{i} - \mu^{II})^2 + \sigma_0^{-2}}$ where $\mu_i^{I} = \boldsymbol{z}_i\boldsymbol{\beta}^{I}$ and $\mu_i^{II} = \boldsymbol{z}_i\boldsymbol{\beta}^{II}$.
\item Posterior for $\tau$: $IG(\alpha_n, \beta_n)$, an inverse-gamma posterior with $\alpha_n = \frac{n}{2} + \alpha_0$ and $\beta_n = \sum\limits_{i = 1}^{n}(s^{I}_{i} - \{\mu_i^{II} + \rho(s^{II}_{i} - \mu^{II})\})^2 + \beta_0$
\end{itemize}

\item Sample new $r_i$ for the training and holdout dataset from the following posterior:

$$f(r_i \mid \theta_i, \boldsymbol{\mu}_i) \propto r_i \exp{\left\{-0.5A_i\bigg(r_i-\frac{B_i}{A_i}\bigg)^2\right\}}$$ 
where $B_i = \begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}^t\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_i$, $\boldsymbol{\mu}_i = \boldsymbol{z_i}\boldsymbol{B}$, $\boldsymbol{B} = (\boldsymbol{\beta}^{I}, \boldsymbol{\beta}^{II})$ and $A_i = \begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}^t\boldsymbol{\Sigma}^{-1}\begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}$.

We can sample from this posterior using a slice sampling technique (Hernandez-Stumpfhauser et.al. 2018):

\begin{itemize}
\item In a slice sampler the joint density for an auxiliary variable $v_{i}$ with $r_{i}$ is:

$$p(r_{i}, v_{i}\mid \theta_{i}, \boldsymbol{\mu}_{i}=\boldsymbol{z}_{i}\boldsymbol{B}^{t}) \propto r_{i} \textbf{I}\bigg(0 < v_i < \exp\left\{ -.5 A_i(r_{i} - \frac{B_i}{A_i})^2\right\}\bigg)\textbf{I}(r_i > 0)$$

\item The full conditional for $v_{i}$, $p(v_{i} \mid r_{i},\boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}, \theta_{i})$, is:

$$U\Bigg(0, \exp\left\{-.5A_i\bigg(r_i -  \frac{B_{i}}{A_i}\bigg)^2\right\}\Bigg)$$
and the full conditional for $r_i$, $p(r_{i} \mid v_{i},\boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}, \theta_{i})$, is proportional to:
$$r_{i} \textbf{I}\left(\frac{B_i}{A_i} + \max\left\{-\frac{B_i}{A_i}, -\sqrt{\frac{-2\ln v_{i}}{A_i}}\right\} < r_{i} < \frac{B_i}{A_i} + \sqrt{\frac{-2\ln v_{i}}{A_i}}\right)$$
\item We thus sample $v_{i}$ from the uniform distribution specified above. Independently we sample a value $m$ from $U(0,1)$. We obtain a new value for $r_{i}$ by computing $r_{i} = \sqrt{(r_{i_{2}}^{2}-r_{i_{1}}^{2})m + r_{i_{1}}^{2}}$ where $r_{i_{1}}=\frac{B_i}{A_i} +\max\left\{-\frac{B_i}{A_i}, -\sqrt{\frac{-2\ln v_{i}}{A_i}}\right\}$ and $ r_{i_{2}}= \frac{B_i}{A_i} + \sqrt{\frac{-2\ln v_{i}}{A_i}}$.

\end{itemize}

\item Compute the PLSL for the circular and linear outcome on the holdoutset using the estimates of $\boldsymbol{\gamma}$, $\sigma^2$, $\boldsymbol{\beta^{k}}$, $\rho$ and $\tau$ for the training dataset. Use the density $f(\theta, r \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})$ for the circular outcome.

\item Repeat steps 4 to 8 until the sampled parameter estimates have converged.

\end{enumerate}





\newpage
\subsection{Bayesian Model and MCMC procedure multivariate GPN model}\label{A3}

\begin{enumerate}
\item Split the data, with the circular outcome $\boldsymbol{\theta} = \theta_i, \dots, \theta_n$, the linear outcome $\boldsymbol{y} = y_i, \dots, y_n$ where $n$ is the sample size, and the design matrix $\boldsymbol{X}$ in a training (90\%) and holdout (10\%) set. Note that in this paper we have only one circular outcome and one linear outcome and the MCMC procedure outlined here is specified for this situation. It can however be generalized to a situation with multiple circular and linear outcomes without too much effort. 

\item Define the prior parameters for the training set. Note that in this paper we have only one circular outcome and one linear outcome, so $p = 1$ and $q = 1$. In this paper we use the following priors:

\begin{itemize}
\item Prior for $\boldsymbol{\Sigma}$: $IW(\boldsymbol{\Psi}_0, \nu_0)$, an inverse-Wishart with $\boldsymbol{\Psi}_0 = 10^{-4}\boldsymbol{I}_{2p + q}$ and $\nu_0 = 1$.   
\item Prior for $\boldsymbol{\beta}$: $MN(\boldsymbol{\beta}_0, \boldsymbol{\Sigma}_0  \otimes \boldsymbol{\kappa}_0)$, where $\boldsymbol{\beta}$ is a vectorized $\boldsymbol{B}$, the matrix with regression coefficients, $\boldsymbol{\beta}_0 = \boldsymbol{0}_{k(2p + q)}$, $\boldsymbol{B}_0 = \boldsymbol{0}_{k \times (2p + q)}$ and $\boldsymbol{\kappa}_0 = 10^{-4}\boldsymbol{I}_k$.
\item Prior for $\lambda$: $N(\gamma_0, \omega_0)$, with $\gamma_0 = 0$ and $\omega_0 = 10000$.
\end{itemize}

\item Set starting values $\boldsymbol{\beta} = (0,0,0,0,0,0)^t$, $\boldsymbol{\Sigma} = \boldsymbol{I}_3$ and $\lambda = 0$. Also set starting values $r_i = 1$ and $d_i = 1$ in the training and holdout set. 

\item Compute the latent bivariate outcome $\boldsymbol{s}_i = (s_i^{I}, s_i^{II})^t$ underlying the circular outcome for the holdout and training dataset as follows:

$$\begin{bmatrix} s^{I}_{i} \\ s^{II}_{i} \end{bmatrix} = \begin{bmatrix} r_i \cos \theta_i \\  r_i\sin \theta_i\end{bmatrix}$$
\item Compute the latent outcomes $\tilde{y}_i$ underlying the linear outcome for the holdout and training dataset as follows:

$$\tilde{y}_i = \lambda d_i $$

\item Compute $\boldsymbol{\eta}_i$ defined as follows for each individual i:

$$\boldsymbol{\eta}_i = (\boldsymbol{s}_i,y_i)^t - (\boldsymbol{0}_{2p}, \lambda d_i)$$

\item Sample $\boldsymbol{B}$, $\boldsymbol{\Sigma}$ and $\lambda$ for the training dataset from their conditional posteriors:

\begin{itemize}
\item Posterior for $\boldsymbol{B}$: $MN(\boldsymbol{B}_n, \boldsymbol{\kappa}_n, \boldsymbol{\Sigma}_n)$, with $\boldsymbol{B}_n = \boldsymbol{\kappa}_n^{-1}\boldsymbol{X}^t\boldsymbol{\eta} + \boldsymbol{\kappa}_0\boldsymbol{B}_0$ and $\boldsymbol{\kappa}_n = \boldsymbol{X}^t\boldsymbol{X} + \boldsymbol{\kappa}_0$.
\item Posterior for $\Sigma$: $IW(\boldsymbol{\Psi}_n, \nu_n)$, an inverse-Wishart with $\boldsymbol{\Psi}_n = \boldsymbol{\Psi}_0 + (\boldsymbol{\eta} - \boldsymbol{X}^t\boldsymbol{B})^t(\boldsymbol{\eta} - \boldsymbol{X}^t\boldsymbol{B}) + (\boldsymbol{B} - \boldsymbol{B}_0)^t\boldsymbol{\kappa}_0(\boldsymbol{B} - \boldsymbol{B}_0)$ and $\nu_n = \nu_0 + n$
\item Posterior for $\lambda$: $N(\gamma_n, \omega_n)$, with $\omega_n = \big(\sum_{i = 1}^{n}d_i^2\boldsymbol{\Sigma}_{y|s}^{-1} + \omega_0^{-1}\big)^{-1}$ and $\gamma_n = \omega_n \big(\sum_{i = 1}^{n}d_i\boldsymbol{\Sigma}_{y|s}^{-1}(y_i - \boldsymbol{\mu}_{y_i|s_i}) + \omega_0^{-1}\gamma_0 \big)$
\end{itemize}

\item Sample new $d_i$ for the training and holdout dataset from the following posterior:

$$f(d_i) \propto \phi_q(y_i|\boldsymbol{\mu}_{y_i|s_i} + \lambda d_i, \boldsymbol{\Sigma}_{y|s})\phi_q(d_i|0, 1),$$

where $\boldsymbol{\mu}_{y_i|s_i} = \boldsymbol{x}_i\boldsymbol{B}_{y_i|s_i}$. We can see $d_i$ as a positive regressor with $\lambda$ as covariate and $\phi_q(d_i|0, 1)$ as prior (Mastrantonio, 2018). The full conditional is then a q-dimensional truncated normal with support $\mathbb{R}^{+}$ as follows:

$$N_q(\boldsymbol{M}_{d_i}, \boldsymbol{V}_q),$$ \\where $\boldsymbol{V}_q = \big(\lambda^2\boldsymbol{\Sigma}_{y|s}^{-1} + 1\big)$ and $\boldsymbol{M}_{d_i} = \boldsymbol{V}_d\lambda\boldsymbol{\Sigma}_{y|s}^{-1}\big(y_i - \boldsymbol{\mu}_{y_i, s_i}\big)$.

\item Sample new $r_i$ for the training and holdout dataset from the following posterior:

$$f(r_i \mid \theta_i, \boldsymbol{\mu}_i) \propto r_i \exp{\left\{-0.5A_i\bigg(r_i-\frac{B_i}{A_i}\bigg)^2\right\}}$$ 
where $B_i = \begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}^t\boldsymbol{\Sigma}_{s_i|y_i}^{-1}\boldsymbol{\mu}_{s_i|y_i}$, $\boldsymbol{\mu}_{s_i|y_i} = \boldsymbol{x}_i\boldsymbol{B}_{s_i|y_i}$ and $A_i = \begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}^t\boldsymbol{\Sigma}_{s_i| y_i}^{-1}\begin{bmatrix} \cos \theta_i \\ \sin \theta_i\end{bmatrix}$. The parameters $\boldsymbol{\mu}_{s_i|y_i}$ and $\boldsymbol{\Sigma}_{s_i| y_i}$ are the conditional mean and covariance matrix of $\boldsymbol{s}_i$ assuming that $(\boldsymbol{s}_i, y_i)^t \sim N_{2p+q}(\boldsymbol{\mu} + (\boldsymbol{0}_{2p}, \lambda d_i)^t, \boldsymbol{\Sigma})$.

Because in this paper $\boldsymbol{\theta}$ originates from a bivariate variable that is known we can simply define the $r_i$ as the euclidean norm of the bivariate datapoints. However, for didactic purposes continue with the explanation of the sampling procedure. We
can sample from posterior for $r_i$ using a slice sampling technique (Hernandez-Stumpfhauser et.al. 2018):

\begin{itemize}
\item In a slice sampler the joint density for an auxiliary variable $v_{i}$ with $r_{i}$ is:

$$p(r_{i}, v_{i}\mid \theta_{i}, \boldsymbol{\mu}_{i}=\boldsymbol{x}_{i}\boldsymbol{B}^{t}) \propto r_{i} \textbf{I}\bigg(0 < v_i < \exp\left\{ -.5 A_i(r_{i} - \frac{B_i}{A_i})^2\right\}\bigg)\textbf{I}(r_i > 0)$$

\item The full conditionals for $v_{i}$, $p(v_{i} \mid r_{i},\boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}, \theta_{i})$, is:

$$U\Bigg(0, \exp\left\{-.5A_i\bigg(r_i -  \frac{B_{i}}{A_i}\bigg)^2\right\}\Bigg)$$
and the full conditional for $r_i$, $p(r_{i} \mid v_{i},\boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}, \theta_{i})$ , is proportional to :
$$r_{i} \textbf{I}\left(\frac{B_i}{A_i} + \max\left\{-\frac{B_i}{A_i}, -\sqrt{\frac{-2\ln v_{i}}{A_i}}\right\} < r_{i} < \frac{B_i}{A_i} + \sqrt{\frac{-2\ln v_{i}}{A_i}}\right)$$
\item We thus sample $v_{i}$ from the uniform distribution specified above. Independently we sample a value $m$ from $U(0,1)$. We obtain a new value for $r_{i}$ by computing $r_{i} = \sqrt{(r_{i_{2}}^{2}-r_{i_{1}}^{2})m + r_{i_{1}}^{2}}$ where $r_{i_{1}}=\frac{B_i}{A_i} +\max\left\{-\frac{B_i}{A_i}, -\sqrt{\frac{-2\ln v_{i}}{A_i}}\right\}$ and $ r_{i_{2}}= \frac{B_i}{A_i} + \sqrt{\frac{-2\ln v_{i}}{A_i}}$.

\end{itemize}

\item Compute the PLSL for the circular and linear outcome on the holdoutset using the estimates of $\boldsymbol{B}$, $\boldsymbol{\Sigma}$ and $\lambda$ for the training dataset.

\item Repeat steps 4 to 10 until the sampled parameter estimates have converged.

\item In the MCMC sampler we have estimated an unconstrained $\boldsymbol{\Sigma}$. However, for identification of the model we need to apply the following constraint to both $\boldsymbol{\Sigma}$ and $\boldsymbol{\mu}$:

$$\boldsymbol{C} = \begin{bmatrix} \boldsymbol{C}_w & \boldsymbol{0}_{2p \times q} \\ \boldsymbol{0}_{2p \times q}^t & \boldsymbol{I}_q \end{bmatrix}$$
where $\boldsymbol{C}_w$ is a $2p \times 2p$ diagonal matrix with every $(2(j-1) + k)^{th}$ entry $> 0$ where $k \in 1, 2$ and $k = 1, \dots, p$ (Mastrantonio, 2018). The estimates $\boldsymbol{\Sigma}$ and $\boldsymbol{\mu}$ can then be related to their constrained versions, $\tilde{\boldsymbol{\Sigma}}$ and $\tilde{\boldsymbol{\mu}}$ as follows:

$$\boldsymbol{\mu} = \boldsymbol{C}\tilde{\boldsymbol{\mu}}$$
$$\boldsymbol{\Sigma} = \boldsymbol{C}\tilde{\boldsymbol{\Sigma}}\boldsymbol{C}.$$

\end{enumerate}

\end{appendices}
