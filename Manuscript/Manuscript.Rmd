---
title             : "Regression models for Cylindrical data in Psychology"
shorttitle        : "Cylindrical data in Psychology"

author: 
  - name          : "Jolien Cremers"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "jcre@sund.ku.dk"
  - name          : "Helena J.M. Pennings"
    affiliation   : "2,3"
  - name          : "Christophe Ley"
    affiliation   : "4"

affiliation:
  - id            : "1"
    institution   : "Department of Biostatistics, University of Copenhagen"
  - id            : "2"
    institution   : "TNO"
  - id            : "3"
    institution   : "Department of Education, Utrecht University"
  - id            : "4"
    institution   : "Department of Applied Mathematics, Computer Science and Statistics, Ghent University"



authornote: |
  

abstract: |
  Cylindrical data are multivariate data which consist of a directional,
  in this paper circular, and a linear component. Examples of cylindrical
  data in psychology include human navigation (direction and distance of 
  movement), eye-tracking research (direction and length of saccades) and
  data from an interpersonal circumplex (location and intensity on the IPC). 
  In this paper we adapt four models for
  cylindrical data to include a regression of the circular and linear
  component onto a set of covariates. Subsequently, we illustrate how to
  fit these models and interpret their results on a dataset on the
  interpersonal behavior of teachers.

keywords          : "cylindrical data, regression, interpersonal behavior"
#wordcount         : "X"

bibliography      : ["CircularData.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : yes
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf


header-includes:
  - \DeclareRobustCommand{\VANDER}[3]{#2} 
  - \DeclareRobustCommand{\VAN}[3]{#2}
  - \DeclareRobustCommand{\DEN}[3]{#2}
  - \usepackage{multirow}
  - \usepackage{rotating}
  - \usepackage{color}
  - \usepackage{subcaption}
  - \raggedbottom
   #% set up for citation (in .tex %  place \DeclareRobustCommand{\VANDER}[3]{#3} before bibliography)

---
```{r inlude = FALSE, echo = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, echo = FALSE}
library(papaja)
library(circular)
library(bpnreg)
library(haven)
library(tikzDevice)
library(plotrix)
library(tidyr)
library(dplyr) #sample_n()
library(MASS) #mvrnorm()
library(kableExtra) #latex tables
library(plotrix)
library(caret) #cross-validation
library(coda) #geweke diagnostic
```
In the social sciences the use of cylindrical data is very common. Cylindrical
data consist of a linear and a circular component. @gurtman2011reasoning refers
to cylindrical data as vectors, with a directional measure (i.e., the circular
component) and a measure indicating the magnitude (i.e., the linear component).
Many established models in psychology are often referred to as circular or
circumplex models, but those models are cylindrical. Examples of such
cylindrical models are the interpersonal circle/circumplex
[@leary1957;@wiggins1996history;@wubbels2006interpersonal], the circumplex of
affect [@russell1980circumplex], the circumplex of human emotion
[@plutchik1997general] or the model of human values [@schwartz1992values].
\newline
\indent Also, many of the more recent types of data that are studied in
psychology are cylindrical. For example, research on human navigation uses data
where distance (i.e., the linear component) and direction (i.e., the circular
component) are of interest [@chrastil2017rotational] or in eye-tracking, the
saccade data also consist of both the direction (i.e., the circular variable)
and the duration (i.e., the linear variable) (e.g., @rayner200935th). Apart from
the social sciences, data with a circular and linear component more commonly
occur in meteorology [@garcia2013exploring], ecology [@garcia2014test] or marine
research [@lagona2015hidden] \newline
\indent Up until now researchers studying cylindrical data had to rely on linear
statistical methods to analyze their research results. However, lately more and
more of these researchers acknowledge that linear methods are not sufficient and
call for new methods [@gurtman2011reasoning; @pennings2017complexity;
@wright2009integrating] that take into account both the circular and the linear
component of these data.
\newline
\indent The aim in the present paper is twofold. Firstly, we intend to fill the
above mentioned gap in the literature by showing that the use of cylindrical
models can benefit the analysis of circumplex data and cylindrical data in
psychology in general. More specifically we will show these benefits for
interpersonal teacher data from the field of educational psychology. Apart from
modelling the dependence between the linear and circular component of a
cylindrical variable we would also like to predict the two components from a set
of covariates in a regression model. Our second aim therefore is to adapt
several existing cylindrical models in such a way that they include a regression
of both the linear and circular component of a cylindrical variable onto a set
of covariates. These adapted cylindrical models are then used  to analyse the
teacher data.





\section{Modelling Framework}
Data that consist of a linear variable and a circular variable are called
cylindrical data. The circular variable is different from the linear variable in
the sense that it is measured on a different scale. Figure \ref{circline} shows
the difference between a circular scale (right) and a linear scale (left). The
most important difference is that on a circular scale the datapoints 0\(^\circ\)
and 360\(^\circ\) are connected and in fact represent the same number while on a
linear scale the two ends, \(-\infty\) and \(\infty\), are not connected and
consequently the values 0\(^\circ\) and 360\(^\circ\) are located on different
places on the scale. Both circular data and cylindrical data require special
analysis methods due to this periodicity in the scale of a circular variable
(see e.g. @fisher1995statistical for an introduction to circular data and
@mardia2000directional, @jammalamadaka2001topics and @ley2017modern for a more
elaborate overview).

\hfil \hspace{2cm} [Figure \ref{circline} about here] \hfil

\indent From now on we will refer to the circular random variable as $\Theta$,
the circular component, and the linear random variable as $Y$, the linear
component, with realizations $\theta_i$ and $y_i$ for each measurement $i=1,
\dots, n$ where $n$ is the sample size. The circular component is measured in
radians and consequently takes values in the interval $[0, 2\pi)$. Note that the
round brackets mean that $2\pi$ is not included in the interval since this
represents the same value as 0 as a result of periodicity. Radians can be
converted to degrees using the relation $1 \:\text{rad} =
1*\frac{180^\circ}{\pi}$. A circular variable is usually described using its
first two moments; the circular location or mean $\mu$ (first moment) and mean resultant length
$\rho$ (second moment)\footnote{In the von Mises distribution, a common distribution for a
circular outcome, we have a concentration parameter $\kappa$ that is related to
$\rho$ as $\rho = A_1(\kappa)$, where $A_1{\kappa} = I_1(\kappa)/I_0(\kappa)$
and $I_0()$ and $I_1()$ are modified Bessel functions of order 0 and 1,
respectively.}. The sample values of these two parameters are referred to as
$\bar{\theta}$ and $\hat{\rho}$. The mean resultant length lies between 0,
meaning the data is not concentrated at all \emph{i.e.} spread over the entire
circle, and 1, meaning all data is concentrated at a single point on the circle.
The linear component is measured on the real line and can take values on
$(-\infty, \infty)$ or we can constrain it to the positive real line $(0,
\infty)$. Depending on which distributional assumptions are used we describe the
distribution of the linear component using a mean $\mu$ and variance $\sigma^2$
or a scale $\nu$ and shape $\alpha$ parameter. When skewed distributions are
used for either the circular or linear component we may also include a skewness
parameter $\lambda$.\newline
\indent In a cylindrical framework we model the location $\mu_c$ of the circular
variable and the mean $\mu_l$ or scale $\nu$ of the linear variable. Note that
to prevent confusion we distinguish between the circular location and linear
mean using subscripts. In this paper we use covariates to predict the circular
and linear component using the following general type of prediction equations
for a cylindrical model with $q$ covariates for the linear component and $p$
covariates for the circular component:
\begin{equation}\label{circpredgen}
\hat{\theta_i} = g(\beta_0 + \beta_1z_1 + \dots + \beta_qz_p),
\end{equation}
\begin{equation}\label{linpredgen}
\hat{y_i} = h(\gamma_0 + \gamma_1x_1 + \dots + \gamma_qx_q),
\end{equation}
where $g()$ and $h()$ are link functions, $\boldsymbol{\beta} = (\beta_0, \dots,
\beta_p)$ is a vector of regression coefficients for the circular variable and
$\boldsymbol{\gamma} = (\gamma_0, \dots, \gamma_q)$ is a vector of regression
coefficients for the linear variable. The link functions that are chosen depend
on the type of distribution we choose for the linear and circular component. In
this paper we use either an identity link or an exponential function (when
modelling a scale parameter) for the linear component. For the circular
component the link functions are specific to circular data and include
arctangent functions. The specific type of link function and prediction
equations used will be introduced in the respective descriptions of the models
used in this paper.\newline
\indent In addition to modelling the circular and linear component of a
cylindrical variable separately using covariates we also model the relation
between them. In the literature, several methods have been put forward to do so.
Some of these are based on regressing the linear component onto the circular
component by including $\sin(\theta_i)$ and $\cos(\theta_i)$ into the prediction
equation in \eqref{linpredgen} [@mardia1978model;@johnson1978some;
@mastrantonio2015bayesian]. The linear component is thus modeled using the sine
and cosine of the circular component. Others model the relation in a different
way, e.g. by specifying a multivariate model for the linear and circular
variable and modelling their covariance matrix [@mastrantonio2018joint] or by
proposing a joint cylindrical distribution. For example, @abe2017tractable
introduce a cylindrical distribution based on a Weibull distribution for the
linear component and a sine-skewed von Mises distribution for the circular
component and link these through their respective shape and concentration
parameters.

\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{Plots/circline.pdf}
\caption{The difference between a linear scale (left) and a circular scale (right).}
\label{circline}
\end{figure}


\subsection{Four Cylindrical Regression Models}\label{Models}
One of the goals of this paper is to show the benefits of cylindrical methods
for the analysis of circumplex data and cylindrical data in psychology in
general. To do so we focus on four cylindrical models. The models were selected
for their relatively low complexity and the ease with which a regression
structure could be incorporated. But also because they show different ways of
modelling the linear and circular component and thereby illustrate a wider range
of cylindrical models available in the literature. As outlined in the previous
section, the cylindrical models contain a set of $q$ predictors $\boldsymbol{x}
= x_1, \dots, x_q$ and $p$ predictors $\boldsymbol{z} = z_1, \dots, z_p$ for the
linear and circular components, \(Y\) and \(\Theta\), respectively. The first
two models are based on a construction by @mastrantonio2015bayesian, while the
other models are extensions of the models from @abe2017tractable and
@mastrantonio2018joint. The four cylindrical  models are introduced separately
in the subsections below. However, to provide a more succinct overview and
comparison of the four models, Table \ref{TableModels} gives an overview of the
similarities and differences between the models.

\begin{sidewaystable}[h]
\centering
\caption{Comparison of the four cylindrical regression models} 
\begin{tabular}{lllll}
  \noalign{\smallskip}\hline\noalign{\smallskip}
\multicolumn{1}{l}{Aspect} & CL-PN & CL-GPN  & Abe-Ley  & GPN-SSN \\ \hline\noalign{\smallskip}
$\Theta$ & &&&\\
$\:\:$Distribution& PN & GPN & Sine-skewed vM & GPN\\
$\:\:$Domain & $[0, 2\pi)$ & $[0, 2\pi)$ & $[0, 2\pi)$ & $[0, 2\pi)$\\
$\:\:$Shape & symmetric, & asymmetric, & asymmetric, & asymmetric, \\
            & unimodal  & multimodal & unimodal   & multimodal \\\hline\noalign{\smallskip}
$Y$& &&&\\
$\:\:$Distribution & Normal & Normal & Weibull & skewed-Normal\\
$\:\:$Domain & $(-\infty, + \infty)$ & $(-\infty, + \infty)$ & $(0, + \infty)$ & $(-\infty, + \infty)$\\
$\:\:$Shape & symmetric, & symmetric, & asymmetric, & asymmetric, \\
            & unimodal  & unimodal  & unimodal   & unimodal\\\hline\noalign{\smallskip}
$\Theta$-$Y$ dependence &                                   &                                   & & \\
                        & $Y$ regressed on                  & $Y$ regressed on                  & circular concentration $\kappa$  & covariance \\
                        & $\sin(\Theta)$ and $\cos(\Theta)$ & $\sin(\Theta)$ and $\cos(\Theta)$ & and linear scale $\alpha$ & matrix\\\hline
\multicolumn{5}{l}{Note: PN and GPN refer to the projected normal and general projected normal distribution.}\\
\multicolumn{5}{l}{vM refers to the von-Mises distribution}\\

\end{tabular}
\label{TableModels}
\end{sidewaystable}

\hfil \hspace{2cm}[Table \ref{TableModels} about here] \hfil

\subsubsection{The modified Circular-Linear Projected Normal (CL-PN) and modified Circular-Linear General Projected Normal (CL-GPN) models}\label{CL-(G)PN}
Following @mastrantonio2015bayesian we consider two models where the prediction
equation for the linear component is specified by combining \eqref{linpredgen},
with an identity link function and including $\sin(\theta_i)$ and
$\cos(\theta_i)$ as follows:
\begin{equation}\label{linpredCLPNCLGPN}
\hat{y_i} = \gamma_0 + \gamma_{cos}*\cos(\theta_i)*r_i + \gamma_{sin}*\sin(\theta_i)*r_i + \gamma_1*x_1 + \dots + \gamma_q*x_q,
\end{equation}
\noindent where $r_i$ is a realization of the unobserved random variable
\(R\geq0\) that will be introduced below, \(\gamma_0, \gamma_{cos},
\gamma_{sin}, \gamma_1, \dots, \gamma_q\) are the intercept and regression
coefficients and \(x_1, \dots, x_q\) are the \(q\) covariates for the prediction
of the linear component. We assume a normal $N(\mu_l, \sigma^2)$ distribution
for the linear component.\newline
\indent For the circular component we assume either a projected normal (PN) or a
general projected normal (GPN) distribution. These distributions arise from a
projection of a distribution defined in bivariate space onto the circle. Figure
\ref{projection} represents this projection. In the left plot of Figure
\ref{projection} we see realizations $(s_i^{I}, s_i^{II})$ from the bivariate
normal variable \(\boldsymbol{S}\) that in the middle plot are projected to form
the circular component \(\Theta\) in the right plot. Mathematically the relation
between \(\boldsymbol{S}\) and \(\Theta\) is defined as follows:
\begin{equation}\label{projection_eq}
\boldsymbol{S} = \begin{bmatrix} S^{I} \\ S^{II} \end{bmatrix} = \begin{bmatrix} R \cos (\Theta) \\  R\sin (\Theta) \end{bmatrix},
\end{equation}
\noindent where \(R = \mid\mid \boldsymbol{S} \mid\mid\), the Euclidean norm of
\(\boldsymbol{S}\), that is represented by the lines connecting the bivariate
datapoints to the origin in the middle plot. We call $\boldsymbol{S}$ the
augmented representation of the circular component. It is a variable that in
contrast to \(\Theta\) is not observed and thus considered latent or auxiliary.
This then means that we do not model \(\Theta\) directly but indirectly through
$\boldsymbol{S}$.

\hfil \hspace{2cm} [Figure \ref{projection} about here] \hfil

\indent For both the PN and GPN distributions the circular location parameter
\(\mu_c\in [0, 2\pi)\) is modeled.\footnote{Note that for the CL-GPN
model the circular location parameter also depends on the variance-covariance
matrix and the circular predicted values should be computed using numerical
integration or Monte Carlo methods because a closed form expression for the mean
direction is not available.} The prediction equation for the circular component is specified by using a double arctangent link function in \eqref{circpredgen} as follows:
\begin{equation}\label{circpredCLPNCLGPN}
\hat{\theta_i} = \text{atan2}(\boldsymbol{\beta^{II}}\boldsymbol{z}_i, \boldsymbol{\beta^{I}}\boldsymbol{z}_i)
\end{equation}
\noindent where $\boldsymbol{\beta}^{I} = (\beta_0^{I},
\beta_1^{I}, \dots, \beta_p^{I})$ and
$\beta^{II} = (\beta_0^{II},
\beta_1^{II}, \dots, \beta_p^{II})$ are vectors with
intercepts and regression coefficients for the prediction of $S^{I}$ and
$S^{II}$ and $\boldsymbol{z}_i$ is a vector with predictor values for each
individual $i \in 1, \dots, n$ where $n$ is the sample size. Note that as a
result of the augmented representation of the circular component we have two sets
of regression coefficients and intercepts, in contrast to a single set in
\eqref{circpredgen}. This leads to problems when we want to interpret the effect
of a covariate on the circle. A circular regression line for simulated data is shown in Figure
\ref{circregline}, with covariate values on the x-axis and the predicted
circular component on the y-axis. As can be seen it is of a non-linear character
meaning that the effect of a covariate is different at different values of the
covariate. A circular regression line is usually described by the slope at the
inflection point, the point at which the slope of the regression line starts
flattening off (indicated with a square in Figure \ref{circregline}). By
default, the parameters from the PN and GPN models do not directly describe this
inflection point. For the PN distribution however, @CremersMulderKlugkist2017
solved this interpretation problem. They introduce a new parameter $b_c$ that describes the slope at
the inflection point of the regression line. For the GPN distribution the
interpretation problem however remains.

\hfil \hspace{2cm} [Figure \ref{circregline} about here] \hfil

\indent The main difference between the PN and GPN distribution lies in the
definition of their covariance matrix. For the PN distribution this is an
identity matrix, causing the distribution to be unimodal and symmetric, whereas
for the GPN distribution \(\boldsymbol{\Sigma} = \begin{bmatrix} \tau^2 + \xi^2
& \xi\\ \xi & 1 \end{bmatrix}\) where \(\xi,\tau \in (-\infty, +\infty)\),
allowing for multimodality and asymmetry/skewness as illustrated in Figure \ref{plotGPN}.

\hfil \hspace{2cm} [Figure \ref{plotGPN} about here] \hfil

\indent Both the CL-PN and CL-GPN models are estimated using Markov Chain Monte
Carlo (MCMC) methods based on @nunez2011bayesian, @wang2012directional and
@hernandez2016general for the regression of the circular component. A detailed
description of the Bayesian estimation and MCMC samplers can be found in the
Supplementary Material.

```{r plotprojecting, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}
library(MASS)
library(plotrix)

set.seed(10200)

Y <- mvrnorm(10, c(2,2), (diag(2)))

theta <- atan2(Y[,2], Y[,1])

tikz("Plots/plotprojecting.tex", standAlone =TRUE, height = 3, width = 9, pointsize = 12, engine = "pdftex")

par(mfrow = c(1,3), oma =c(0,0,0,0), mar = c(0,0,0,0))

plot(x = Y[,1], y = Y[,2], ylim = c(-2, 4), xlim = c(-2, 4),
     ylab = "", xlab ="", bty=  "n", xaxt = "n", yaxt ="n", asp = 1)

points(0,0, pch = 3)
draw.circle(0, 0, 1, nv = 100, border = NULL, col = NA, lty = 1, lwd = 2)


plot(x = Y[,1], y = Y[,2], ylim = c(-2, 4), xlim = c(-2, 4),
     ylab = "", xlab ="", bty=  "n", xaxt = "n", yaxt ="n", asp = 1)

points(0,0, pch = 3)
draw.circle(0, 0, 1, nv = 100, border = NULL, col = NA, lty = 1, lwd = 2)

points(x = cos(theta), y = sin(theta))
segments(x0 = cos(theta), x1 = Y[,1], y0 = sin(theta), y1 = Y[,2])
segments(x0 = cos(theta), x1 = 0, y0 = sin(theta), y1 = 0, lty = 2)


plot(x = cos(theta), y = sin(theta), ylim = c(-2, 4), xlim = c(-2, 4),
     ylab = "", xlab ="", bty=  "n", xaxt = "n", yaxt ="n", asp = 1)

points(0,0, pch = 3)
draw.circle(0, 0, 1, nv = 100, border = NULL, col = NA, lty = 1, lwd = 2)

dev.off() 

tools::texi2dvi("Plots/plotprojecting.tex",pdf=TRUE)


```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/plotprojecting.pdf}
\caption{Plot showing the projection of datapoints in bivariate space, $\boldsymbol{S}$, (left) to the circle (right). The lines connecting the bivariate datapoints to the circular datapoints represent the euclidean norms of the bivariate datapoints, realizations of the random variable $R$.}
\label{projection}
\end{figure}


```{r circregline, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

tikz("Plots/circregline.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

set.seed(101)

Xreal <- rnorm(100, -2, 1.5)

CompI  <- 1.1 + 2.5*Xreal + rnorm(100, 0.1,1)
CompII <- 2.1 + -0.4*Xreal + rnorm(100, 0.1,1)

Outreal <- atan2(CompII, CompI)

X <- seq(-5, 5, 0.1)

predCompI  <- 1.1 + 2.5*X
predCompII <- 2.1 + -0.4*X

PredCirc <- atan2(predCompII, predCompI)

axval <- ((1.1*2.5)+(2.1*-0.4))/(1.1^2 + 2.1^2)
acval <- atan2(2.1 + -0.4*axval, 1.1 + 2.5*axval)

plot(X, PredCirc*(180/pi), ylim = c(-200,200), type = "l", xlab = "Predictor",
     ylab = "Circular Outcome", bty = 'n', yaxt = "n", xaxt = "n")
axis(1, xaxp = c(-5, 0, 5), at = c(seq(-5, 5, by = 1)),
     labels = c(seq(-5, 5, by = 1)), las = 2)
axis(2, xaxp = c(-180, 0,180), at = c(seq(-180, 180, by = 60)),
     labels = c(seq(-180, 180, by = 60)), las = 2)
points(Xreal, Outreal*(180/pi), cex=0.5)
points(axval,
       acval*(180/pi),
       pch = 0)


dev.off()
tools::texi2dvi("Plots/circregline.tex",pdf=TRUE)

```


\begin{figure}[]
  \includegraphics[width = \textwidth]{Plots/circregline.pdf}
  \caption{Circular regression line for the relation between a covariate and a circular component with the simulated data the regression line was fit to. The square indicates the inflection point of the regression line.}
  \label{circregline}
\end{figure}


```{r plotGPN, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

require(MASS)
require(circular)

y1 = mvrnorm(1000, c(2,2), matrix(c(1^2 + 0^2, 0, 0, 1), nrow = 2, ncol = 2))
y2 = mvrnorm(1000, c(2,2), matrix(c(5^2 + 0.9^2, 0.9, 0.9, 1), nrow = 2, ncol = 2))
y3 = mvrnorm(1000, c(2,2), matrix(c(10^2 + -0.5^2, -0.5, -0.5, 1), nrow = 2, ncol = 2))


theta_1 = atan2(y1[,2], y1[,1])
theta_2 = atan2(y2[,2], y2[,1])
theta_3 = atan2(y3[,2], y3[,1])

tikz("Plots/plotGPN.tex", standAlone =TRUE, height = 3, width = 9, pointsize = 12, engine = "pdftex")

par(mfrow=c(1,3),
    oma=c(0,0,0,0),
    mar=c(0,0,0,0))

plot(as.circular(theta_1), stack = TRUE,
     main = " ", ylab = " ", xlab = " ", shrink = 1.3, axes = FALSE)
axis.circular(at = 0, labels = "0")
plot(as.circular(theta_2), stack = TRUE,
     main = " ", ylab = " ", xlab = " ", shrink = 1.3, axes = FALSE)
axis.circular(at = 0, labels = "0")
plot(as.circular(theta_3), stack = TRUE,
     main = " ", ylab = " ", xlab = " ", shrink = 1.3, axes = FALSE)
axis.circular(at = 0, labels = "0")

#Close the device
dev.off()

# Compile the tex file
tools::texi2dvi("Plots/plotGPN.tex",pdf=TRUE)


```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/plotGPN.pdf}
\caption{Plot showing 1000 samples from three GPN distributions with mean vector $(2,2)$ and $\tau = 1,5,10$ and $\xi = 0, 0.9, -0.5$ from left to right}
\label{plotGPN}
\end{figure}

\subsubsection{The modified Abe-Ley model}\label{WeiSSVM}
This model is an extension of the cylindrical model introduced in
@abe2017tractable to the regression context. The circular dependence between the
  linear and circular component, $Y$ and $\Theta$, is defined through a joint
  density function as follows:

\begin{equation}\label{WeiSSVMdensity}
f(\theta, y) = \frac{\alpha\nu^\alpha}{2\pi\cosh(\kappa)}
                 (1 +\lambda\sin(\theta - \mu_c))
                 y^{\alpha-1}
                 \exp[-(\nu y)^{\alpha}(1-\tanh(\kappa)\cos(\theta - \mu_c))],
\end{equation}

\noindent where \(\alpha > 0\) is a linear shape parameter, \(\kappa > 0\) and
\(\lambda \in [-1, 1]\) are circular concentration and skewness parameters
respectively. The Abe-Ley density thus concerns a combination of a Weibull
distribution, with scale parameter \(\nu>0\) and shape parameter \(\alpha\), for
the linear component and a sine-skewed von Mises distribution, with location
parameter \(\mu_c\in [0, 2\pi)\), concentration parameter \(\kappa>0\) and
skewness \(\lambda \in [-1,1]\), for the circular component. In contrast to the
CL-PN and CL-GPN models, the linear component \(Y\) is in this model defined
only on the positive real half-line \([0, + \infty)\) and thus can not be
negative. Our modification of the Abe-Ley model occurs at the level of the
linear scale parameter \(\nu>0\) and circular location parameter \(\mu_c\in [0,
2\pi)\), both of which we will model using covariates.\newline
\indent The prediction equation for the circular component equals:
\begin{equation}\label{circpredcAL}
\hat{\theta}_i = \beta_0 + 2\tan^{-1}(\boldsymbol{\beta}\boldsymbol{z}_i)
\end{equation}
Note that we have taken out the intercept $\beta_0$ from the link function $g() =
2\tan^{-1}$ in \eqref{circpredgen}. The vector $\boldsymbol{\beta}$ thus does
not contain an intercept. We do not directly predict the linear component. The
conditional distribution for the linear component is Weibull, meaning that we can
use methods from survival analsis to interpret the effect of a predictor. In
survival analysis a 'survival' function is used in which time is plotted against
the probability of survival of subjects suffering from a specific medical
condition. This probability is computed using the 'survival-function' defined as
\begin{equation}\label{survivalfunc}
\exp(-\alpha
y_i^{\hat{\nu}_i(1-\tanh(\kappa)\cos(\theta_i - \hat{\theta}_i))^{1/\alpha}}),
\end{equation}
\noindent with \(\hat{\nu}_i = \exp(\boldsymbol{\gamma}\boldsymbol{x}_i)\). Note that $\boldsymbol{x}_i$ also includes a 1 to be able to estimate the intercept $\gamma_0$. From
the survival function we also see that the circular concentration parameter
\(\kappa\) and the linear shape parameter \(\alpha\) regulate the
circular-linear dependence in the Abe-Ley model. In the exponent or power of the linear outcome, the prediction error on the circle, $\cos(\theta_i - \hat{\theta_i})$, is multiplied by $\tanh{\kappa}$, and raised to the power $1-\alpha$.\newline
\indent We can use numerical optimization (Nelder-Mead) to find solutions for
the maximum likelihood (ML) estimates for the parameters of the model.


\subsubsection{The modified Joint Projected and Skew Normal model (GPN-SSN)}\label{CL-GPN_multivariate}
This model is an extension of the cylindrical model introduced by
@mastrantonio2018joint to the regression context. Although the model may contain
several circular and linear components we will restrict ourselves to one
circular and one linear component in this paper. The circular component is
modelled by a GPN distribution, as in the CL-GPN model, while the linear component is modelled by a skew
normal distribution [@sahu2003new]. Because the GPN distribution is modelled
using a so-called augmented representation (see \eqref{projection_eq}) it is
convenient to use a similar tactic for modelling the skew normal distribution.
As in @mastrantonio2018joint, dependence between the linear and circular
component is created by modelling the augmented representation of \(\Theta\) and
\(Y\) together in a 3 dimensional normal distribution. The joint density of the model is then represented by:
\begin{equation}\label{YDThetarjoint} 
f(\theta, r, y, d) = 2\phi_{3}((\boldsymbol{s}^t,
y^t)^t \mid \boldsymbol{M} + (0,0,\lambda d)^t, \boldsymbol{\Sigma})
\phi(d \mid 0, 1)r, 
\end{equation}
\noindent where $\phi$ represents the normal probability density function, \(\boldsymbol{s} = (r(\cos(\theta), \sin(\theta)))^t\), $d$ is
the augmented representation of the linear component, $\boldsymbol{M} =
\boldsymbol{B}^t\boldsymbol{X}$, $\boldsymbol{B}_{(g + 1) \times 3}$ is
defined below, $\boldsymbol{X}$ is a design matrix and $\boldsymbol{\Sigma} =
\left ( \begin{matrix} \boldsymbol{\Sigma}_s & \boldsymbol{\Sigma}_{sy} \\
\boldsymbol{\Sigma}_{sy}^t & \boldsymbol{\Sigma}_y \\ \end{matrix} \right )$ a
variance-covariance matrix. The matrices \(\boldsymbol{\Sigma}_s\) and \(\boldsymbol{\Sigma}_y\) are the
covariance matrices for the variances of and covariances between the augmented
representations of the circular and linear component respectively. Note that because we only have one circular and one linear component in this paper, \(\boldsymbol{\Sigma}_s\) is a two by two matrix and \(\boldsymbol{\Sigma}_y\) is a scalar representing the variance of the augmented linear component. The matrix
\(\boldsymbol{\Sigma}_{sy}\) contains covariances between the augmented
representations of the circular and linear component. The matrix with
regression coefficients and intercepts, $\boldsymbol{B}$ is defined as follows:
\begin{equation}\label{regmatGPNSSN}
\boldsymbol{B} = \begin{bmatrix}
\beta_{0_{s^{I}}} & \beta_{0_{s^{II}}} & \beta_{0_{y}}\\
\beta_{1_{s^{I}}} & \beta_{1_{s^{II}}} & \beta_{1_{y}}\\
\vdots & \vdots & \vdots \\
\beta_{g_{s^{I}}} & \beta_{g_{s^{II}}} & \beta_{g_{y}} \end{bmatrix}.
\end{equation} 
\noindent Note that in contrast to the previous three models where we use $p$
and $q$ covariates for the circular and linear component respectively, we use $g
= p = q$ covariates for both components in the GPN-SSN model. Unlike the other
models, where $\mu_l$ and $\mu_c$ are modelled separately, the GPN-SSN thus has
a shared mean vector $\boldsymbol{M}$ and variance-covariance matrix
$\boldsymbol{\Sigma}$ for the linear and circular component, much like having
multiple outcomes in a MANOVA (multivariate analysis of variance) model. This
implies that the covariances in the variance-covariance matrix describe the
dependence between the circular and linear component.\newline
\indent The prediction equation for the circular component\footnote{Note that for the  GPN-SSN model the predicted circular component also depends on the variance-covariance matrix and the circular predicted values should be computed using numerical integration or Monte Carlo methods because a closed form expression for the mean direction is not available.} is similar to \eqref{circpredCLPNCLGPN} and equal to:
\begin{equation}\label{circpredGPNSSN}
\hat{\theta}_i = \text{atan2}(\boldsymbol{\beta^{II}}\boldsymbol{z}_i, \boldsymbol{\beta^{I}}\boldsymbol{z}_i),
\end{equation}
\noindent where $\boldsymbol{\beta^I} = (\beta_{0_{s^{I}}}, \beta_{1_{s^{I}}}, \dots, \beta_{g_{s^{I}}})$ and $\boldsymbol{\beta^{II}} = (\beta_{0_{s^{II}}}, \beta_{1_{s^{II}}}, \dots, \beta_{g_{s^{II}}})$.
For the linear component we subsititute $h()$ in \eqref{linpredgen} for an identity link and $\boldsymbol{\gamma}$ for $\boldsymbol{\beta}$:
\begin{equation}\label{linpredGPNSSN}
\hat{y}_i = \boldsymbol{\beta}\boldsymbol{x}_i,
\end{equation}
\noindent where $\boldsymbol{x}_i = \boldsymbol{z}_i$ and $\boldsymbol{\beta} = (\beta_{0_{y}}, \beta_{1_{y}}, \dots, \beta_{g_{y}})$.\newline
\indent We estimate the model using MCMC methods. A detailed description of these
methods is given in the Supplementary Material.


\subsection{Model Fit Criterion}\label{Modelfit}
For the four cylindrical models we focus on their out-of-sample predictive
performance to determine the fit of the model. To prevent possible problems
concerning overestimation, we use k-fold cross-validation and split our data
into 10 folds. Each of these folds (10 \(\%\) of the sample) is used once as a
holdout set and 9 times as part of a training set. The analysis will thus be
performed 10 times, each time on a different training set.\newline
\indent A proper criterion to compare out-of-sample predictive performance is
the Predictive Log Scoring Loss (PLSL) [@gneiting2007strictly]. The lower the
value of this criterion, the better the predictive performance of the model.
Because the joint density and thus also the likelihood for the modified GPN-SSN
model is not available in closed form [@mastrantonio2018joint] we compute the
PLSL for the circular and linear component separately for all models. Using ML
estimates this criterion can be computed as follows for the circular and linear
component:
\begin{equation}\label{PLSLMLcirc}
PLSL_c = -2 \sum_{i = 1}^{M}\log l(\theta_i \mid \hat{\boldsymbol{\vartheta}}),\nonumber
\end{equation}
\begin{equation}\label{PLSLMLlin}
PLSL_l = -2 \sum_{i = 1}^{M}\log l(y_i \mid \hat{\boldsymbol{\vartheta}}),\nonumber
\end{equation}
\noindent where \(l\) is the model likelihood, \(M\) is the sample size of the
holdout set, \(y_i\) and $\theta_i$ are the \(i^{th}\) datapoints from the
holdout set and \(\hat{\boldsymbol{\vartheta}}\) are the ML estimates of the
model parameters. Using posterior samples the criterion is similar to the log
pointwise predictive density (lppd) [@BDA, p. 169] and can be computed for the
circular and linear component as:
\begin{equation}\label{PLSLBayescirc}
PLSL_c = -2 \frac{1}{B} \sum_{j = 1}^{B}\sum_{i = 1}^{M} \log l(\theta_i \mid \boldsymbol{\vartheta}^{(j)}),\nonumber
\end{equation}
\begin{equation}\label{PLSLBayeslin}
PLSL_l = -2 \frac{1}{B} \sum_{j = 1}^{B}\sum_{i = 1}^{M} \log l(y_i \mid \boldsymbol{\vartheta}^{(j)}),\nonumber
\end{equation}
\noindent where \(B\) is the amount of posterior samples and
\(\boldsymbol{\vartheta}^{(j)}\) are the posterior estimates of the model
parameters for the \(j^{th}\) iteration. Note that although we fit the CL-PN,
CL-GPN and GPN-SSN models using Bayesian statistics, we do not take prior
information into account when assessing model fit with the PLSL. According to
@BDA this is not necessary since we are assessing the fit of a model to data,
the holdout set, only. They argue that the prior in such case is only of
interest for estimating the parameters of the model but not for determining the
predictive accuracy.\newline
\indent For each of the four cylindrical models and for each of the 10
cross-validation analyses we can then compute a PLSL for the circular and linear
component by using the conditional log-likelihoods of the respective component (see
Supplementary Material for a definition of the loglikelihoods). To evaluate the
predictive performance we average across the PLSL criteria of the
cross-validation analyses. We also assess the cross-validation variability by
means of the standard deviations of the PLSL criteria.





\section{Teacher Data}\label{Example}
\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{Plots/IPC-T2.png}
\caption{The interpersonal circle for teachers (IPC-T). The words presented in
the circumference of the circle are anchor words to describe the type of
behavior located in each part of the IPC.}
\label{QTI}
\end{figure}


The motivating example for this article comes from the field of educational
psychology and was collected for the studies on classroom climate of
@vanderWant2015role, @Claessens2016side and @pennings2017phd. An
indicator of the quality of the classroom climate is the students' perception of
their teachers' interpersonal behavior. These interpersonal perceptions, both in
educational psychology as well as in other areas of psychology, can be measured
using circumplex measurement instruments (see @horowitz2010handbook for an
overview of many such instruments).\newline
\indent The circumplex data used in this paper are measured using the
Questionnaire on Teacher Interaction (QTI) [@wubbels2006interpersonal] which is
one such circumplex measurement instrument. The QTI is designed to measure
student perceptions of their teachers' interpersonal behavior and contains items
that load on two interpersonal dimensions: Agency and Communion. The item scores
on the QTI are measured on a scale from 1 to 5 and are later converted to a
scale ranging from -1 to 1. Subsequently they are converted to scores on Agency
and Communion (see also footnote 2 of @wubbels2005two for more details on this
procedure). Agency refers to the degree of power or control a teacher exerts in
interaction with his/her students. Communion refers to the degree of
friendliness or affiliation a teacher conveys in interaction with his/her
students.  The loadings on the two dimensions of the QTI can be placed in a
two-dimensional space formed by Agency (vertical) and Communion (horizontal),
see Figure \ref{QTI}. This space is called the interpersonal circle/circumplex
(IPC) and different parts of the IPC are characterized by different teacher
behaviors, e.g. 'helpful' or 'uncertain'. The IPC is ``a continuous order with no
beginning or end'' [@gurtman2009exploring, p. 2]. We call such ordering a
circumplex ordering and the IPC is therefore often called the interpersonal
circumplex. The circumplex ordering also implies that scores on the IPC could be viewed as
a circular variable. This circular variable represents location on the IPC of
the interpersonal behavior that a teacher shows towards his/her students.

\hfil \hspace{2cm} [Figure \ref{QTI} about here] \hfil

\indent @Cremers2018Assessing  and @cremers2019circular explain the circular nature of the IPC data and analyze them as such using a circular regression and mixed-effects model respectively. The two dimension
scores, Agency and Communion, can be converted to a circular score using the
two-argument arctangent function in \eqref{PredVal}, where \(A\) represents a
score on the Agency dimension and \(C\) represents a score on the Communion
dimension\footnote{The selection of the origin in circumplex data depends on
the scaling of the Agency and Communion scores. Their respective 0 scores form the
origin. Although the scaling influences the average score and spread on the IPC and the intensity, the difference between the individual measurements will be retained (albeit them being different in size). With regards to the intensity, scaling has the same effect as in a standard linear regression. With regards to the location on the IPC (the circular component) rescaling to -1,1 affects the size of the circular (as well as bivariate linear) regression coefficients. This has a positive effect if the original Agency and Communion scores are far from the origin in bivariate space. In that case the locations on the IPC are very concentrated before scaling (small variance). Such data are hard to estimate using circular models as the circular regression coefficients may be very small (high risk of bias, see Cremers, Mainhard, Klugkist (2018a)). Scaling will thus improve estimation. Finally note that scaling is only considered an issue in those instances where cylindrical data is derived from measurements in bivariate space.}. Note that
when placing a unit circle on Figure \ref{QTI} we see that the Agency dimension
is related to the sine of the circular score and the Communion dimension is
related to the cosine of the circular score.
\begin{equation}\label{PredVal}
\theta          = \text{atan2}\left(A, \: C\right)  =
\left\{{\begin{array}{lcl}
                                                                       \arctan\left(\frac{A}{C}\right) & \text{if}  \quad&C > 0 \\
\arctan\left(\frac{A}{C}\right) + \pi & \text{if}  \quad& C  <  0  \:\: \&\:\: A \geq 0\\
 \arctan\left(\frac{A}{C}\right) - \pi & \text{if}  \quad&C  <  0 \:\:  \&\:\:A  < 0\\
 +\frac{\pi}{2} & \text{if}  \quad& C  =  0  \:\: \&\:\:A > 0\\
 -\frac{\pi}{2} & \text{if}  \quad& C =  0  \:\: \&\:\:A < 0\\
 \text{undefined} & \text{if} \quad& C =  0   \:\: \&\:\:A = 0.
 \end{array}}
\right.
\end{equation}
The resulting circular variable \(\theta\) can then be modelled and takes values
in $[0, 2\pi)$.\newline
\indent A circular analysis of circumplex data has several benefits: it is more
in line with its theoretical definition and it allows us to analyse the blend of
the two dimensions Agency and Communion instead of both dimensions separately.
This provides us with new insights compared to a separate analysis of the two
dimensions that is standard in the literature (see, e.g.,
@pennings2018interpersonal, @wright2009integrating or
@wubbels2006interpersonal). There is however one main drawback: when
two-dimensional data are converted to the location on the circle we lose some
information, namely the length of the two-dimensional vector \((A, C)^t\),
\emph{i.e.}, its Euclidean norm \(\mid\mid (A, C)^t \mid\mid\). This length
represents the intensity of the interpersonal behavior a teacher shows towards
his/her students. In a cylindrical model this intensity (the linear component)
can be modeled together with the location of interpersonal behavior of a teacher
on the IPC (the circular component). This leads to an improved analysis of
interpersonal circumplex data, over either analyzing the two dimensions
separately or using a circular model, because we take all information, circular
and linear, into account. In the next section we introduce several cylindrical
models that can be used to analyze the teacher data. First however we will
provide descriptives for the teacher data and conduct a 'standard' analysis for
these data that we can compare the results from the cylindrical models to.


\subsection{Data Description}\label{DataDescriptives}
```{r loaddata, return = FALSE, echo = FALSE, results = FALSE}
Dat <- read_spss("Data Heleen/mergeddata.sav")

#only select the first measurement occasion
Dat <- subset(Dat, Time == 0)

#sort on class size
Dat <- Dat[order(-Dat$nklas), ]

#Now remove duplicates 
#(because dataframe is sorted on class size the smallest classes are removed)

duplicates <- duplicated(Dat[,c("docnr", "Time")])
Dat <- Dat[!duplicates,]

nrow(Dat)

#center and remove missings for self-efficacy
Dat <- Dat %>% drop_na(Efficacy_CM)
Dat$SEc <- Dat$Efficacy_CM- mean(Dat$Efficacy_CM)
Dat$theta <- atan2(Dat$lAgency, Dat$lcommunion)
Dat$y <- sqrt(Dat$lAgency^2 + Dat$lcommunion^2)

set.seed(30)

#Create holdout and training samples for cross-validation

vec <-  seq(1:nrow(Dat))
flds <- createFolds(vec, k = 10, list = TRUE, returnTrain = FALSE)

for(i in 1:10) { 
  
 #Create name for holdout data 
 nam.h <- paste("hold.", i, sep = "")

 #Get holdout dataset for fold i
 assign(nam.h, Dat[flds[[i]],])

 #Create name for training data
 nam.tr <- paste("train.", i, sep = "")

 #Get training dataset for fold i
 assign(nam.tr, Dat[-flds[[i]],])

}

```

```{r dataplot, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

tikz("Plots/dataplot.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

SE_low = Dat[Dat$Efficacy_CM <= 4.04,]
SE_mid = Dat[Dat$Efficacy_CM > 4.04 & Dat$Efficacy_CM < 6.04,]
SE_high = Dat[Dat$Efficacy_CM >= 6.04,]

summary(SE_low$y)
summary(SE_mid$y)
summary(SE_high$y)

plot(SE_low$y, SE_low$theta*(180/pi), 
     ylab = "IPC", xlab = "IPC intensity",
     bty = "n", ylim = c(-180, 180))
points(SE_mid$y, SE_mid$theta*(180/pi), pch = 2)
points(SE_high$y, SE_high$theta*(180/pi), pch = 3)
legend(0.45, -75, legend = c("low SE", "average SE", "high SE"), pch = c(1,2,3), bty = "n")

dev.off()
tools::texi2dvi("Plots/dataplot.tex", pdf=TRUE)


```

```{r descripitves, echo = FALSE, results = FALSE}
#sample size
nrow(Dat)

#summary statistics
summary(Dat$Efficacy_CM)
summary(Dat$y)
summary(as.circular(Dat$theta%%(2*pi)))

sd(Dat$Efficacy_CM)
sd(Dat$y)

```

The teacher data was collected between 2010 and 2015 and contains several
repeated measures on the IPC of 161 teachers. Measurements were obtained using
the QTI and taken in different years and classes. For this paper we only
consider one measurement, the first occasion (2010) and largest class if data
for multiple classes were available. This results in a sample of 151 teachers.
The data includes the location of interpersonal behavior on the IPC (IPC), the
circular component, and the intensity of interpersonal behavior (IPC intensity),
the linear component. It also includes teachers' self-efficacy (\verb|SE|)
concerning classroom management as covariate that is used to model the IPC and
IPC intensity. This means that $\boldsymbol{x}_i = \boldsymbol{z}_i = (1,\:
\text{SE}_i)$ in all cylindrical models except for the Abe-Ley model where
$\boldsymbol{z}_i = \text{SE}_i$. In previous research, in psychology and
education it has been shown that higher self-efficacy is related to the quality
of interpersonal interactions [@locke2007selfefficacy; @want2018selfefficacy].
After listwise deletion of missings ($3$ in total, only for self-efficacy) we
have a sample of 148 teachers. We used listwise deletion of the missings in this
paper for simplicity. However, in general and especially when the number of
missings is larger, the influence of listwise deletion should be investigated
and necessary precaution (e.g. imputation of missing values) should be taken.
Table \ref{Tableteacherdescriptives} shows descriptives for the dataset. Note
that we also show the scores on the Agency and Communion component of the IPC
before transformation to a circular score. For the circular variable IPC we show
sample estimates for the circular mean $\bar{\theta}$ and mean resultant length
$\hat{\rho}$. For the linear variables (Agency, Communion, IPC intensity and the
covariate SE) we show sample estimates of the linear mean and standard deviation
(sd). Figure \ref{dataplot} is a scatterplot showing the relation between the
linear and circular component of the teacher data for teachers with low SE
(below 1 sd below the mean), average SE (between 1 sd below and 1 sd above the
mean) and high SE (above 1 sd above the mean).

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/dataplot.pdf}
\caption{Plot showing the relation between the linear and circular outcome component (in degrees) of the teacher data.}
\label{dataplot}
\end{figure}

\begin{table}[h]
\centering
\caption{Descriptives for the teacher dataset.} 
\begin{tabular}{lrrrl}
  \noalign{\smallskip}\hline\noalign{\smallskip}
Variable & mean/$\bar{\theta}$ & sd/$\hat{\rho}$ & Range & Type \\ \hline\noalign{\smallskip}
Agency & 0.19 & 0.16 & -0.33 - 0.49 & Linear\\
Communion & 0.30 & 0.24 & -0.58 - 0.77 & Linear\\
IPC &33.22$^\circ$& 0.76 & - & Circular\\
intensity IPC & 0.43 & 0.15 & 0.08 - 0.80 & Linear\\
SE & 5.04 & 1.00 & 1.5 - 7.0 & Linear\\
   \hline
\multicolumn{5}{l}{Note: For the circular variable IPC we show sample }\\
\multicolumn{5}{l}{estimates for the circular mean $\bar{\theta}$ and mean resultant length $\hat{\rho}$.}\\
\multicolumn{5}{l}{For the linear variable we show the sample mean,}\\
\multicolumn{5}{l}{standard deviation and range.}
\end{tabular}
\label{Tableteacherdescriptives}
\end{table}

\hfil \hspace{2cm} [Table \ref{Tableteacherdescriptives} about here] \hfil

\hfil \hspace{2cm} [Figure \ref{dataplot} about here] \hfil


\subsection{Standard Analysis}\label{StandardAnalysis}

To perform a standard analysis of the teacher data we fit two linear regression
models to the Agency and Communion scores of the teachers separately. In these
regression models we incorporate an intercept and the covariate self-efficacy.
Coefficients and standard errors from the models are shown in Table
\ref{Tableteacherstandardanalysis}. From the results we conclude that the effect
of self-efficacy is significant for both the Agency and Communion component. An
increase of 1 unit on self-efficacy leads to a 0.07 increase in Agency and a
0.09 increase in Communion. Note that in this setup we can only model the effect
of self-efficacy on the intensity, the size of the score, of the Agency and
Communion component. We can not quantify the effect on the location and
intensity on the IPC nor can we quantify their dependence.

\hfil \hspace{2cm} [Table \ref{Tableteacherstandardanalysis} about here] \hfil

```{r standardanalysis, cache = TRUE, echo = FALSE, results = FALSE}
A_res <- lm(lAgency ~ SEc, data = Dat)
C_res <- lm(lcommunion ~ SEc, data = Dat)

summary(A_res)
summary(C_res)

summary(Dat$lAgency)
summary(Dat$lcommunion)
sd(Dat$lAgency)
sd(Dat$lcommunion)
```

\begin{table}[h]
\centering
\caption{Regression coefficients and standard errors for the standard analysis of the teacher dataset.} 
\begin{tabular}{lrr}
  \noalign{\smallskip}\hline\noalign{\smallskip}
  & Agency & Communion \\\hline\noalign{\smallskip}
Intercept & 0.19 (0.01) & 0.30 (0.02) \\
SE & 0.07 (0.01) & 0.09 (0.02) \\ \hline
\multicolumn{3}{l}{Note: The intercepts indicate the average }\\
\multicolumn{3}{l}{Agency and Communion. The coefficient for }\\
\multicolumn{3}{l}{SE indicates the effect of self-efficacy }\\
\multicolumn{3}{l}{on Agency and Communion.}
\end{tabular}
\label{Tableteacherstandardanalysis}
\end{table}





\section{Results}\label{DataAnalysis}
In this section we analyze the teacher data with the help of the four
cylindrical models from the previous section. We will present the results,
posterior estimates and their interpretation for all four models.

```{r FitCLPN, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling CL-PN.R")

its <- 20000
set.seed(101)

res_CLPN <- list()

for(i in 1:10){

  train <- get(paste("train.", i, sep = ""))
  hold <- get(paste("hold.", i, sep = ""))

  ZI.PN.tr  <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))
  ZII.PN.tr <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))
  X.PN.tr   <- as.matrix(cbind(rep(1, length(train$theta)),
                                 cos(train$theta),
                                 sin(train$theta),
                                 train$SEc))

  ZI.PN.h  <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))
  ZII.PN.h <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))
  X.PN.h   <- as.matrix(cbind(rep(1, length(hold$theta)),
                               cos(hold$theta),
                               sin(hold$theta),
                               hold$SEc))

  res_CLPN[[i]] <- CLPN(train$theta, train$y, X.PN.tr, ZI.PN.tr, ZII.PN.tr, its,
                        hold$theta, hold$y, X.PN.h, ZI.PN.h, ZII.PN.h)

}

```


```{r FitCLGPN, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling CL-GPN.R")

its <- 20000
set.seed(101)

res_CLGPN <- list()

for(i in 1:10){

  train <- get(paste("train.", i, sep = ""))
  hold  <- get(paste("hold.", i, sep = ""))

  Z.GPN.tr <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))
  X.GPN.tr <- as.matrix(cbind(rep(1, length(train$theta)),
                              cos(train$theta),
                              sin(train$theta),
                              train$SEc))

  Z.GPN.h <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))
  X.GPN.h <- as.matrix(cbind(rep(1, length(hold$theta)),
                             cos(hold$theta),
                             sin(hold$theta),
                             hold$SEc))

  res_CLGPN[[i]] <- CLGPN(train$theta, train$y, X.GPN.tr, Z.GPN.tr, its, p = 2, 
                          hold$theta, hold$y, X.GPN.h, Z.GPN.h)
  
}

```

```{r FitCLGPNM, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Posterior Sampling Joint GPN-SSN.R")

its <- 20000
set.seed(101)

res_CLGPNM <- list()

for(i in 1:10){

  train <- get(paste("train.", i, sep = ""))
  hold  <- get(paste("hold.", i, sep = ""))

  X.MGPN.tr <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))

  X.MGPN.h <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))

  res_CLGPNM[[i]] <- JGPNSSN(train$theta, train$y, X.MGPN.tr, its, p = 1, q = 1, 
                             hold$theta, hold$y, X.MGPN.h)

}

```

```{r FitAbeLey, cache = TRUE, results = FALSE, echo = FALSE, eval = TRUE}

source("R-code/Abe-Ley optimization.R")

resAL <- list()

set.seed(101)

for(i in 1:10){

  train <- get(paste("train.", i, sep = ""))
  hold <- get(paste("hold.", i, sep = ""))

  Z.tr    <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))
  X.tr    <- as.matrix(cbind(rep(1, length(train$theta)), train$SEc))

  Z.h     <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))
  X.h     <- as.matrix(cbind(rep(1, length(hold$theta)), hold$SEc))
  
  theta.tr <- as.numeric(train$theta)
  y.tr     <- as.numeric(train$y)
  theta.h  <- as.numeric(hold$theta)
  y.h      <- as.numeric(hold$y)

  colnames(X.tr) <- c("ax", "bx")
  colnames(Z.tr) <- c("az", "bz")
  colnames(X.h) <- c("ax", "bx")
  colnames(Z.h) <- c("az", "bz")


  dat.t <- as.data.frame(cbind(theta.tr, y.tr, X.tr, Z.tr))
  dat.h <- as.data.frame(cbind(theta.h, y.h, X.h, Z.h))

  ui.reg = rbind(c(1, 0, 0, 0, 0, 0, 0),
                 c(-1, 0, 0, 0, 0, 0, 0),
                 c(0, 0, 0, 0, 1, 0, 0),
                 c(0, 0, 0, 0, 0, 1, 0),
                 c(0, 0, 0, 0, 0, 0, 1),
                 c(0, 0, 0, 0, 0, 0, -1))

  ci.reg = c(-pi, -pi, 0, 0, -1, -1)
  
  param <- c(1,1,1,1,1,1,0)*0.9

  ui.reg %*% param - ci.reg

  resAL[[i]] <- constrOptim(param, func.regII, ui = ui.reg, ci = ci.reg, method = "Nelder-Mead",
                            control = list(maxit = 1000000, fnscale = -1), data = dat.t)
  
  resAL[[i]]$ll.circ <- -2*func.reg.cond.circ(resAL[[i]]$par, dat.h)
  resAL[[i]]$ll.lin  <- -2*func.reg.cond.lin(resAL[[i]]$par, dat.h)
  
  
}

```

```{r geweke, results = FALSE, echo = FALSE, cache = TRUE}

for(i in 1:10){
  print(paste('fold', i))
  print(geweke.diag(mcmc(cbind(res_CLPN[[i]]$Gamma,res_CLPN[[i]]$BI,
                               res_CLPN[[i]]$BII,res_CLPN[[i]]$Sigma)), 0.25, 0.75))
  print(geweke.diag(mcmc(cbind(res_CLGPN[[i]]$Gamma,res_CLGPN[[i]]$BI,
                               res_CLGPN[[i]]$BII,res_CLGPN[[i]]$Sigma,res_CLGPN[[i]]$Sig)), 0.25, 0.75))
  print(geweke.diag(mcmc(cbind(res_CLGPNM[[i]]$lambda,res_CLGPNM[[i]]$Betacon[1,1,],
                               res_CLGPNM[[i]]$Betacon[1,2,], res_CLGPNM[[i]]$Betacon[1,3,],
                               res_CLGPNM[[i]]$Betacon[2,1,], res_CLGPNM[[i]]$Betacon[2,2,],
                               res_CLGPNM[[i]]$Betacon[2,3,], res_CLGPNM[[i]]$Sigmacon[1,1,],
                               res_CLGPNM[[i]]$Sigmacon[1,2,], res_CLGPNM[[i]]$Sigmacon[1,3,],
                               res_CLGPNM[[i]]$Sigmacon[2,1,], res_CLGPNM[[i]]$Sigmacon[2,2,],
                               res_CLGPNM[[i]]$Sigmacon[2,3,], res_CLGPNM[[i]]$Sigmacon[3,1,],
                               res_CLGPNM[[i]]$Sigmacon[3,2,], res_CLGPNM[[i]]$Sigmacon[3,3,])), 0.25, 0.75))
}



```


\subsection{Analysis}\label{DataResults}
In the Supplementary Material we have described the starting values for the MCMC
procedures for the CL-PN, CL-GPN and GPN-SSN models, hence it remains to specify
the starting  values for the maximum likelihood based Abe-Ley model: \(\eta_0 =
0.9, \eta_1 = 0.9, \nu_0 = 0.9, \nu_1 = 0.9, \kappa = 0.9, \alpha = 0.9, \lambda
= 0\). The initial number of iterations for the three MCMC samplers was set to
2000. After convergence checks via traceplots we concluded that some of the
parameters of the GPN-SSN model did not converge. Therefore we set the number of
iterations of the MCMC models to 20,000 and subtracted a burn-in of 5000 to
reach convergence (the Geweke diagnostics show absolute z-scores over 1.96 in
6\% of the estimated parameters). Note that we choose the same number of
iterations for all three models estimated using MCMC procedures to make
their comparison via the PLSL as fair as possible. Lastly, the predictor
\verb|SE| was centered before inclusion in the analysis as this allows the
intercepts to bear the classical meaning of average behavior.\newline
\indent Tables \ref{tab:estCLGPN}, \ref{tab:estAL} and \ref{tab:estCLGPNM} show
the results for the four cylindrical models that were fit to the teacher data.
For the models estimated using MCMC methods, CL-PN, CL-GPN and GPN-SSN, we show
descriptives of the posterior of the estimated parameters (posterior mode and
lower and upper bound of the 95\% highest posterior density (HPD) interval). For
the Abe-Ley model we show the maximum likelihood estimates of the parameters. To
compare the results of the four models we focus on the following aspects: the
estimated average scores (intercept) on the location and intensity on the IPC (1), the effect of self-efficacy on the location and intensity (2), the dependence between the location and intensity (3) and the model fit (4).

\hfil \hspace{2cm} [Table \ref{tab:estCLGPN}, \ref{tab:estAL} and \ref{tab:estCLGPNM} about here] \hfil

```{r estCLPN, cache = TRUE, echo = FALSE}

#Get 3 dimensional arrays (iteration, parameter, fold) for each parameter type
Gamma <- simplify2array(lapply(res_CLPN, "[[", "Gamma"))
BI    <- simplify2array(lapply(res_CLPN, "[[", "BI"))
BII   <- simplify2array(lapply(res_CLPN, "[[", "BII"))
Sigma <- simplify2array(lapply(res_CLPN, "[[", "Sigma"))

#Averge over the folds and compute the posterior modes and hpd intervals
CLPN.Gamma.m   <- rowMeans(apply(Gamma[5000:20000,,], 2:3, mode_est))
CLPN.Gamma.hpd <- apply(apply(Gamma[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLPN.Gamma.m.sd   <- apply(apply(Gamma[5000:20000,,], 2:3, mode_est),1, sd)
CLPN.Gamma.hpd.sd <- apply(apply(Gamma[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLPN.BI.m   <- rowMeans(apply(BI[5000:20000,,], 2:3, mode_est))
CLPN.BI.hpd <- apply(apply(BI[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLPN.BI.m.sd   <- apply(apply(BI[5000:20000,,], 2:3, mode_est), 1, sd)
CLPN.BI.hpd.sd <- apply(apply(BI[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLPN.BII.m   <- rowMeans(apply(BII[5000:20000,,], 2:3, mode_est))
CLPN.BII.hpd <- apply(apply(BII[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLPN.BII.m.sd   <- apply(apply(BII[5000:20000,,], 2:3, mode_est), 1, sd)
CLPN.BII.hpd.sd <- apply(apply(BII[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLPN.Sigma.m   <- mean(apply(Sigma[5000:20000,], 2, mode_est))
CLPN.Sigma.hpd <- rowMeans(apply(Sigma[5000:20000,], 2, hpd_est))
CLPN.Sigma.m.sd  <- sd(apply(Sigma[5000:20000,], 2, mode_est))
CLPN.Sigma.hpd.sd <- apply(apply(Sigma[5000:20000,], 2, hpd_est), 1, sd)

#Compute a cross-validation variance/standard deviation

#Put estimates in correct format for tables
modes.PN <- c(CLPN.BI.m, CLPN.BII.m, CLPN.Gamma.m, CLPN.Sigma.m, NA, NA, NA)
hpd.LB.PN <- c(CLPN.BI.hpd[1,], CLPN.BII.hpd[1,], CLPN.Gamma.hpd[1,], CLPN.Sigma.hpd[1], NA, NA, NA)
hpd.UB.PN <- c(CLPN.BI.hpd[2,], CLPN.BII.hpd[2,], CLPN.Gamma.hpd[2,], CLPN.Sigma.hpd[2], NA, NA, NA)

modes.PN.sd <- c(CLPN.BI.m.sd, CLPN.BII.m.sd, CLPN.Gamma.m.sd, CLPN.Sigma.m.sd, NA, NA, NA)
hpd.LB.PN.sd <- c(CLPN.BI.hpd.sd[1,], CLPN.BII.hpd.sd[1,], CLPN.Gamma.hpd.sd[1,], CLPN.Sigma.hpd.sd[1], NA, NA, NA)
hpd.UB.PN.sd <- c(CLPN.BI.hpd.sd[2,], CLPN.BII.hpd.sd[2,], CLPN.Gamma.hpd.sd[2,], CLPN.Sigma.hpd.sd[2], NA, NA, NA)

CLPNres.tab <- cbind(modes.PN, modes.PN.sd, hpd.LB.PN, hpd.LB.PN.sd, hpd.UB.PN, hpd.UB.PN.sd)
CLPNres.tab = CLPNres.tab

CLPNres.tab.new = cbind(apply(CLPNres.tab[,1:2], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                       sprintf("%.2f", round(x[2], 2)), ")")),
                        apply(CLPNres.tab[,3:4], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (",
                                                                       sprintf("%.2f", round(x[2], 2)), ")")),
                        apply(CLPNres.tab[,5:6], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (",
                                                                       sprintf("%.2f", round(x[2], 2)), ")")))


```


```{r estCLGPN, cache = TRUE, echo = FALSE}

#Get 3 or 4 dimensional arrays (iteration, parameter, fold)/(,,itertation, fold) for each parameter type
Gamma <- simplify2array(lapply(res_CLGPN, "[[", "Gamma"))
BI    <- simplify2array(lapply(res_CLGPN, "[[", "BI"))
BII   <- simplify2array(lapply(res_CLGPN, "[[", "BII"))
Sig   <- simplify2array(lapply(res_CLGPN, "[[", "Sig"))
Sigma <- simplify2array(lapply(res_CLGPN, "[[", "Sigma"))

#Averge over the folds and compute the posterior modes and hpd intervals
CLGPN.Gamma.m   <- rowMeans(apply(Gamma[5000:20000,,], 2:3, mode_est))
CLGPN.Gamma.m.sd   <- apply(apply(Gamma[5000:20000,,], 2:3, mode_est), 1, sd)
CLGPN.Gamma.hpd <- apply(apply(Gamma[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLGPN.Gamma.hpd.sd <- apply(apply(Gamma[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLGPN.BI.m   <- rowMeans(apply(BI[5000:20000,,], 2:3, mode_est))
CLGPN.BI.m.sd   <- apply(apply(BI[5000:20000,,], 2:3, mode_est), 1, sd)
CLGPN.BI.hpd <- apply(apply(BI[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLGPN.BI.hpd.sd <- apply(apply(BI[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLGPN.BII.m   <- rowMeans(apply(BII[5000:20000,,], 2:3, mode_est))
CLGPN.BII.m.sd   <- apply(apply(BII[5000:20000,,], 2:3, mode_est), 1, sd)
CLGPN.BII.hpd <- apply(apply(BII[5000:20000,,], 2:3, hpd_est), c(1,2), mean)
CLGPN.BII.hpd.sd <- apply(apply(BII[5000:20000,,], 2:3, hpd_est), c(1,2), sd)

CLGPN.Sig.m   <- mean(apply(Sig[5000:20000,], 2, mode_est))
CLGPN.Sig.m.sd   <- sd(apply(Sig[5000:20000,], 2, mode_est))
CLGPN.Sig.hpd <- rowMeans(apply(Sig[5000:20000,], 2, hpd_est))
CLGPN.Sig.hpd.sd <- apply(apply(Sig[5000:20000,], 2, hpd_est), 1, sd)

CLGPN.Sigma11.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,1]
CLGPN.Sigma11.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,1]
CLGPN.Sigma12.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,2]
CLGPN.Sigma12.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,1]
CLGPN.Sigma22.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,2]
CLGPN.Sigma22.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,2]

CLGPN.Sigma11.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,1]
CLGPN.Sigma11.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,1]
CLGPN.Sigma12.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,2]
CLGPN.Sigma12.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,1]
CLGPN.Sigma22.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,2]
CLGPN.Sigma22.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,2]

#Compute a cross-validation variance/standard deviation

#Put estimates in correct format for tables
modes.GPN  <- c(CLGPN.BI.m, CLGPN.BII.m,
                CLGPN.Gamma.m, CLGPN.Sig.m,
                CLGPN.Sigma11.m, CLGPN.Sigma12.m, CLGPN.Sigma22.m)
hpd.LB.GPN <- c(CLGPN.BI.hpd[1,], CLGPN.BII.hpd[1,],
               CLGPN.Gamma.hpd[1,], CLGPN.Sig.hpd[1],
               CLGPN.Sigma11.hpd[1], CLGPN.Sigma12.hpd[1], CLGPN.Sigma22.hpd[1])
hpd.UB.GPN <- c(CLGPN.BI.hpd[2,], CLGPN.BII.hpd[2,], 
               CLGPN.Gamma.hpd[2,], CLGPN.Sig.hpd[2],
               CLGPN.Sigma11.hpd[2], CLGPN.Sigma12.hpd[2], CLGPN.Sigma22.hpd[2])

modes.GPN.sd  <- c(CLGPN.BI.m.sd, CLGPN.BII.m.sd,
                   CLGPN.Gamma.m.sd, CLGPN.Sig.m.sd,
                   CLGPN.Sigma11.m.sd, CLGPN.Sigma12.m.sd, CLGPN.Sigma22.m.sd)
hpd.LB.GPN.sd <- c(CLGPN.BI.hpd.sd[1,], CLGPN.BII.hpd.sd[1,],
                   CLGPN.Gamma.hpd.sd[1,], CLGPN.Sig.hpd.sd[1],
                   CLGPN.Sigma11.hpd.sd[1], CLGPN.Sigma12.hpd.sd[1], CLGPN.Sigma22.hpd.sd[1])
hpd.UB.GPN.sd <- c(CLGPN.BI.hpd.sd[2,], CLGPN.BII.hpd.sd[2,], 
                   CLGPN.Gamma.hpd.sd[2,], CLGPN.Sig.hpd.sd[2],
                   CLGPN.Sigma11.hpd.sd[2], CLGPN.Sigma12.hpd.sd[2], CLGPN.Sigma22.hpd.sd[2])




CLGPNres.tab <- cbind(modes.GPN, modes.GPN.sd, hpd.LB.GPN, hpd.LB.GPN.sd, hpd.UB.GPN, hpd.UB.GPN.sd)

CLGPNres.tab.new = cbind(apply(CLGPNres.tab[,1:2], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                         sprintf("%.2f", round(x[2], 2)), ")")),
                         apply(CLGPNres.tab[,3:4], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                         sprintf("%.2f", round(x[2], 2)), ")")),
                         apply(CLGPNres.tab[,5:6], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                         sprintf("%.2f", round(x[2], 2)), ")")))

CLPNGPNres.tab <- cbind(CLPNres.tab.new, CLGPNres.tab.new)


rownames(CLPNGPNres.tab) <- c("$\\beta_0^{I}$", "$\\beta_1^{I}$",
                              "$\\beta_0^{II}$", "$\\beta_1^{II}$",
                              "$\\gamma_0$", "$\\gamma_{cos}$",
                              "$\\gamma_{sin}$", "$\\gamma_1$",
                              "$\\sigma$", "$\\sum_{1,1}$",
                              "$\\sum_{1,2}$", "$\\sum_{2,2}$")
colnames(CLPNGPNres.tab) <- rep(c('Mode', "HPD LB", "HPD UB"), 2)

#Create table
#kable(CLPNGPNres.tab, "latex", booktabs = TRUE, escape = FALSE, 
#      caption = "Results, cross-validation mean and standard deviation, for the modified CL-PN and CL-GPN model")%>%
#add_header_above(c("Parameter" = 1, "CL-PN" = 3, "CL-GPN" = 3))



```


\begin{table}

\caption{\label{tab:estCLGPN}Results, cross-validation mean and standard deviation, for the modified CL-PN and CL-GPN models}
\centering
\begin{tabular}[t]{lllllll}
\toprule
\multicolumn{1}{c}{Parameter} & \multicolumn{3}{c}{CL-PN} & \multicolumn{3}{c}{CL-GPN} \\
\cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-4} \cmidrule(l{2pt}r{2pt}){5-7}
  & Mode & HPD LB & HPD UB & Mode & HPD LB & HPD UB\\
\midrule
$\beta_0^{I}$ & 1.76 (0.09) & 1.50 (0.07) & 2.03 (0.09) & 2.43 (0.12) & 1.91 (0.10) & 3.05 (0.17)\\
$\beta_1^{I}$ & 0.65 (0.07) & 0.42 (0.06) & 0.90 (0.08) & 0.84 (0.11) & 0.45 (0.09) & 1.29 (0.15)\\
$\beta_0^{II}$ & 1.15 (0.05) & 0.92 (0.04) & 1.37 (0.04) & 1.47 (0.05) & 1.16 (0.04) & 1.78 (0.05)\\
$\beta_1^{II}$ & 0.58 (0.03) & 0.38 (0.04) & 0.79 (0.04) & 0.70 (0.06) & 0.47 (0.05) & 0.96 (0.08)\\
$\gamma_0$ & 0.38 (0.01) & 0.31 (0.01) & 0.44 (0.01) & 0.37 (0.01) & 0.31 (0.01) & 0.42 (0.01)\\
\addlinespace
$\gamma_{cos}$ & 0.04 (0.00) & 0.01 (0.00) & 0.06 (0.00) & 0.03 (0.00) & 0.01 (0.00) & 0.04 (0.00)\\
$\gamma_{sin}$ & -0.01 (0.00) & -0.04 (0.00) & 0.02 (0.00) & -0.00 (0.00) & -0.03 (0.00) & 0.03 (0.00)\\
$\gamma_1$ & 0.03 (0.01) & -0.00 (0.00) & 0.07 (0.01) & 0.03 (0.00) & -0.00 (0.00) & 0.06 (0.00)\\
$\sigma$ & 0.14 (0.00) & 0.12 (0.00) & 0.16 (0.00) & 0.14 (0.00) & 0.12 (0.00) & 0.16 (0.00)\\
$\sum_{1,1}$ & NA (NA) & NA (NA) & NA (NA) & 3.04 (0.29) & 1.85 (0.13) & 5.00 (0.41)\\
\addlinespace
$\sum_{1,2}$ & NA (NA) & NA (NA) & NA (NA) & 0.47 (0.12) & 0.12 (0.12) & 0.80 (0.10)\\
$\sum_{2,2}$ & NA (NA) & NA (NA) & NA (NA) & 1.00 (0.00) & 1.00 (0.00) & 1.00 (0.00)\\
\bottomrule
\multicolumn{7}{l}{Note: $\beta_0^{I}$, $\beta_0^{II}$ and $\gamma_0$ inform us about the location and intensity on the IPC}\\
\multicolumn{7}{l}{at the average self-efficacy. $\beta_1^{I}$, $\beta_1^{II}$ and $\gamma_1$ inform us about the effect of self-efficacy on the}\\
\multicolumn{7}{l}{location and intensity on the IPC. $\gamma_{cos}$ and $\gamma_{sin}$ inform us about the dependence}\\
\multicolumn{7}{l}{ between the location and intensity on the IPC. $\sum_{1,1}$, $\sum_{1,2}$ and $\sum_{2,2}$ are}\\
\multicolumn{7}{l}{elements of the variance-covariance matrix of the location on the IPC in the}\\
\multicolumn{7}{l}{CL-GPN model and $\sigma$ is the error standard deviation of the intensity of interpersonal behavior.}\\

\end{tabular}
\end{table}


```{r estAL, cache = TRUE, echo = FALSE}
#Get 2 dimensional arrays (parameter, fold)
resAL_arr <- simplify2array(lapply(resAL, "[[", "par"))

#Compute a cross-validation variance/standard deviation

#Put estimates in correct format for table
ALres.tab <- matrix(round(c(rowMeans(resAL_arr[1:4,]), mean(resAL_arr[6,]), 
                            mean(resAL_arr[5,]), mean(resAL_arr[7,]), 
                            apply(resAL_arr[1:4,], 1, sd), sd(resAL_arr[6,]), 
                            sd(resAL_arr[5,]), sd(resAL_arr[7,])), 2), 7, 2)

ALres.tab.new = cbind(apply(ALres.tab, 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (",
                                                             sprintf("%.2f", round(x[2], 2)), ")")))

rownames(ALres.tab.new) <- c("$\\beta_0$", "$\\beta_1$",
                             "$\\gamma_0$", "$\\gamma_1$",
                             "$\\alpha$", "$\\kappa$", "$\\lambda$")

#Create table
#kable(ALres.tab.new, "latex", booktabs = T, escape = FALSE, digits = 2,
#      caption = "Results, cross-validation mean and standard deviation, for the modified Abe-Ley model")%>%
#add_header_above(c("Parameter" = 1, "ML-estimate" = 1))

```

\begin{table}

\caption{\label{tab:estAL}Results, cross-validation mean and standard deviation (SD), for the modified Abe-Ley model}
\centering
\begin{tabular}[t]{llllllll}
\toprule
& $\beta_0$ & $\beta_1$ & $\gamma_0$  & $\gamma_1$ & $\alpha$ & $\kappa$ & $\lambda$\\
Mean & 0.36 & -0.03 & 1.17 & 0.04 & 3.66 & 1.51 & 0.70 \\
SD & 0.02 & 0.01 & 0.02 & 0.02 & 0.12 & 0.08 & 0.05\\
\bottomrule
\multicolumn{8}{l}{Note: $\beta_0$  and $\gamma_0$ inform us about the location and intensity }\\
\multicolumn{8}{l}{on the IPC at the average self-efficacy. $\beta_1$ }\\
\multicolumn{8}{l}{and $\gamma_1$ inform us about the effect of self-efficacy on the}\\
\multicolumn{8}{l}{location and intensity on the IPC. $\alpha$ is the } \\
\multicolumn{8}{l}{shape parameter of the distribution of the location }\\
\multicolumn{8}{l}{on the IPC. $\kappa$ and $\lambda$ respectively are the concentration}\\
\multicolumn{8}{l}{and skewness parameters for the distribution of the location}\\
\multicolumn{8}{l}{on the IPC.}\\
\end{tabular}
\end{table}


```{r estCLGPNM, cache = TRUE, echo = FALSE}
#Get 3 or 4 dimensional arrays (iteration, parameter, fold)/(,,itertation, fold) for each parameter type
lambda   <- simplify2array(lapply(res_CLGPNM, "[[", "lambda"))
Beta <- simplify2array(lapply(res_CLGPNM, "[[", "Beta"))
Betacon <- simplify2array(lapply(res_CLGPNM, "[[", "Betacon"))
Sigma <- simplify2array(lapply(res_CLGPNM, "[[", "Sigma"))
Sigmacon <- simplify2array(lapply(res_CLGPNM, "[[", "Sigmacon"))

#Averge over the folds and compute the posterior modes and hpd intervals
CLGPNM.Sigma11.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,1]
CLGPNM.Sigma11.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,1]
CLGPNM.Sigma12.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,2]
CLGPNM.Sigma12.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,1]
CLGPNM.Sigma13.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,3]
CLGPNM.Sigma13.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,1]
CLGPNM.Sigma22.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,2]
CLGPNM.Sigma22.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,2]
CLGPNM.Sigma23.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,3]
CLGPNM.Sigma23.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,2]
CLGPNM.Sigma33.m   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[3,3]
CLGPNM.Sigma33.hpd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,3]

CLGPNM.Sigma11.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,1]
CLGPNM.Sigma11.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,1]
CLGPNM.Sigma12.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,2]
CLGPNM.Sigma12.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,1]
CLGPNM.Sigma13.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,3]
CLGPNM.Sigma13.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,1]
CLGPNM.Sigma22.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,2]
CLGPNM.Sigma22.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,2]
CLGPNM.Sigma23.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,3]
CLGPNM.Sigma23.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,2]
CLGPNM.Sigma33.m.sd   <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[3,3]
CLGPNM.Sigma33.hpd.sd <- apply(apply(Sigma[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,3]

CLGPNM.Sigmacon11.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,1]
CLGPNM.Sigmacon11.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,1]
CLGPNM.Sigmacon12.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,2]
CLGPNM.Sigmacon12.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,1]
CLGPNM.Sigmacon13.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,3]
CLGPNM.Sigmacon13.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,1]
CLGPNM.Sigmacon22.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,2]
CLGPNM.Sigmacon22.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,2]
CLGPNM.Sigmacon23.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,3]
CLGPNM.Sigmacon23.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,2]
CLGPNM.Sigmacon33.m   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[3,3]
CLGPNM.Sigmacon33.hpd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,3,3]

CLGPNM.Sigmacon11.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,1]
CLGPNM.Sigmacon11.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,1]
CLGPNM.Sigmacon12.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,2]
CLGPNM.Sigmacon12.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,1]
CLGPNM.Sigmacon13.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,3]
CLGPNM.Sigmacon13.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,1]
CLGPNM.Sigmacon22.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,2]
CLGPNM.Sigmacon22.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,2]
CLGPNM.Sigmacon23.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,3]
CLGPNM.Sigmacon23.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,2]
CLGPNM.Sigmacon33.m.sd   <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[3,3]
CLGPNM.Sigmacon33.hpd.sd <- apply(apply(Sigmacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,3,3]

CLGPNM.Beta11.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,1]
CLGPNM.Beta11.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,1]
CLGPNM.Beta21.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,1]
CLGPNM.Beta21.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,1]
CLGPNM.Beta12.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,2]
CLGPNM.Beta12.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,2]
CLGPNM.Beta22.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,2]
CLGPNM.Beta22.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,2]
CLGPNM.Beta13.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,3]
CLGPNM.Beta13.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,3]
CLGPNM.Beta23.m   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,3]
CLGPNM.Beta23.hpd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,3]

CLGPNM.Beta11.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,1]
CLGPNM.Beta11.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,1]
CLGPNM.Beta21.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,1]
CLGPNM.Beta21.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,1]
CLGPNM.Beta12.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,2]
CLGPNM.Beta12.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,2]
CLGPNM.Beta22.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,2]
CLGPNM.Beta22.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,2]
CLGPNM.Beta13.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,3]
CLGPNM.Beta13.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,3]
CLGPNM.Beta23.m.sd   <- apply(apply(Beta[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,3]
CLGPNM.Beta23.hpd.sd <- apply(apply(Beta[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,3]

CLGPNM.Betacon11.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,1]
CLGPNM.Betacon11.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,1]
CLGPNM.Betacon21.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,1]
CLGPNM.Betacon21.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,1]
CLGPNM.Betacon12.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,2]
CLGPNM.Betacon12.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,2]
CLGPNM.Betacon22.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,2]
CLGPNM.Betacon22.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,2]
CLGPNM.Betacon13.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[1,3]
CLGPNM.Betacon13.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,1,3]
CLGPNM.Betacon23.m   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, mean)[2,3]
CLGPNM.Betacon23.hpd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, mean)[,2,3]

CLGPNM.Betacon11.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,1]
CLGPNM.Betacon11.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,1]
CLGPNM.Betacon21.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,1]
CLGPNM.Betacon21.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,1]
CLGPNM.Betacon12.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,2]
CLGPNM.Betacon12.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,2]
CLGPNM.Betacon22.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,2]
CLGPNM.Betacon22.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,2]
CLGPNM.Betacon13.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[1,3]
CLGPNM.Betacon13.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,1,3]
CLGPNM.Betacon23.m.sd   <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), mode_est), 1:2, sd)[2,3]
CLGPNM.Betacon23.hpd.sd <- apply(apply(Betacon[,,5000:20000,], c(1,2,4), hpd_est), 1:3, sd)[,2,3]

CLGPNM.lambda.m   <- mean(apply(lambda[5000:20000,,], 2, mode_est))
CLGPNM.lambda.m.sd   <- sd(apply(lambda[5000:20000,,], 2, mode_est))
CLGPNM.lambda.hpd <- rowMeans(apply(lambda[5000:20000,,], 2, hpd_est))
CLGPNM.lambda.hpd.sd <- apply(apply(lambda[5000:20000,,], 2, hpd_est), 1, sd)

#Compute a cross-validation variance/standard deviation

#Put estimates in correct format for tables
modes.GPN  <- c(CLGPNM.Beta11.m, CLGPNM.Beta12.m, CLGPNM.Beta13.m,
                CLGPNM.Beta21.m, CLGPNM.Beta22.m, CLGPNM.Beta23.m,
                CLGPNM.Sigma11.m, CLGPNM.Sigma22.m, CLGPNM.Sigma33.m,
                CLGPNM.Sigma12.m, CLGPNM.Sigma13.m, CLGPNM.Sigma23.m,
                CLGPNM.lambda.m)
hpd.LB.GPN <- c(CLGPNM.Beta11.hpd[1], CLGPNM.Beta12.hpd[1], CLGPNM.Beta13.hpd[1],
                CLGPNM.Beta21.hpd[1], CLGPNM.Beta22.hpd[1], CLGPNM.Beta23.hpd[1],
                CLGPNM.Sigma11.hpd[1], CLGPNM.Sigma22.hpd[1], CLGPNM.Sigma33.hpd[1],
                CLGPNM.Sigma12.hpd[1], CLGPNM.Sigma13.hpd[1], CLGPNM.Sigma23.hpd[1],
                CLGPNM.lambda.hpd[1])
hpd.UB.GPN <- c(CLGPNM.Beta11.hpd[2], CLGPNM.Beta12.hpd[2], CLGPNM.Beta13.hpd[2],
                CLGPNM.Beta21.hpd[2], CLGPNM.Beta22.hpd[2], CLGPNM.Beta23.hpd[2],
                CLGPNM.Sigma11.hpd[2], CLGPNM.Sigma22.hpd[2], CLGPNM.Sigma33.hpd[2],
                CLGPNM.Sigma12.hpd[2], CLGPNM.Sigma13.hpd[2], CLGPNM.Sigma23.hpd[2],
                CLGPNM.lambda.hpd[2])

modescon.GPN  <- c(CLGPNM.Betacon11.m, CLGPNM.Betacon12.m, CLGPNM.Betacon13.m,
                   CLGPNM.Betacon21.m, CLGPNM.Betacon22.m, CLGPNM.Betacon23.m,
                   CLGPNM.Sigmacon11.m, CLGPNM.Sigmacon22.m, CLGPNM.Sigmacon33.m,
                   CLGPNM.Sigmacon12.m, CLGPNM.Sigmacon13.m, CLGPNM.Sigmacon23.m,
                   CLGPNM.lambda.m)
hpdcon.LB.GPN <- c(CLGPNM.Betacon11.hpd[1], CLGPNM.Betacon12.hpd[1], CLGPNM.Betacon13.hpd[1],
                   CLGPNM.Betacon21.hpd[1], CLGPNM.Betacon22.hpd[1], CLGPNM.Betacon23.hpd[1],
                   CLGPNM.Sigmacon11.hpd[1], CLGPNM.Sigmacon22.hpd[1], CLGPNM.Sigmacon33.hpd[1],
                   CLGPNM.Sigmacon12.hpd[1], CLGPNM.Sigmacon13.hpd[1], CLGPNM.Sigmacon23.hpd[1],
                   CLGPNM.lambda.hpd[1])
hpdcon.UB.GPN <- c(CLGPNM.Betacon11.hpd[2], CLGPNM.Betacon12.hpd[2], CLGPNM.Betacon13.hpd[2],
                   CLGPNM.Betacon21.hpd[2], CLGPNM.Betacon22.hpd[2], CLGPNM.Betacon23.hpd[2],
                   CLGPNM.Sigmacon11.hpd[2], CLGPNM.Sigmacon22.hpd[2], CLGPNM.Sigmacon33.hpd[2],
                   CLGPNM.Sigmacon12.hpd[2], CLGPNM.Sigmacon13.hpd[2], CLGPNM.Sigmacon23.hpd[2],
                   CLGPNM.lambda.hpd[2])

modes.GPN.sd  <- c(CLGPNM.Beta11.m.sd, CLGPNM.Beta12.m.sd, CLGPNM.Beta13.m.sd,
                   CLGPNM.Beta21.m.sd, CLGPNM.Beta22.m.sd, CLGPNM.Beta23.m.sd,
                   CLGPNM.Sigma11.m.sd, CLGPNM.Sigma22.m.sd, CLGPNM.Sigma33.m.sd,
                   CLGPNM.Sigma12.m.sd, CLGPNM.Sigma13.m.sd, CLGPNM.Sigma23.m.sd,
                  CLGPNM.lambda.m.sd)
hpd.LB.GPN.sd <- c(CLGPNM.Beta11.hpd.sd[1], CLGPNM.Beta12.hpd.sd[1], CLGPNM.Beta13.hpd[1],
                   CLGPNM.Beta21.hpd.sd[1], CLGPNM.Beta22.hpd.sd[1], CLGPNM.Beta23.hpd[1],
                   CLGPNM.Sigma11.hpd.sd[1], CLGPNM.Sigma22.hpd.sd[1], CLGPNM.Sigma33.hpd[1],
                   CLGPNM.Sigma12.hpd.sd[1], CLGPNM.Sigma13.hpd.sd[1], CLGPNM.Sigma23.hpd[1],
                   CLGPNM.lambda.hpd.sd[1])
hpd.UB.GPN.sd <- c(CLGPNM.Beta11.hpd.sd[2], CLGPNM.Beta12.hpd.sd[2], CLGPNM.Beta13.hpd.sd[2],
                   CLGPNM.Beta21.hpd.sd[2], CLGPNM.Beta22.hpd.sd[2], CLGPNM.Beta23.hpd.sd[2],
                   CLGPNM.Sigma11.hpd.sd[2], CLGPNM.Sigma22.hpd.sd[2], CLGPNM.Sigma33.hpd.sd[2],
                   CLGPNM.Sigma12.hpd.sd[2], CLGPNM.Sigma13.hpd.sd[2], CLGPNM.Sigma23.hpd.sd[2],
                   CLGPNM.lambda.hpd.sd[2])

modescon.GPN.sd  <- c(CLGPNM.Betacon11.m.sd, CLGPNM.Betacon12.m.sd, CLGPNM.Betacon13.m.sd,
                      CLGPNM.Betacon21.m.sd, CLGPNM.Betacon22.m.sd, CLGPNM.Betacon23.m.sd,
                      CLGPNM.Sigmacon11.m.sd, CLGPNM.Sigmacon22.m.sd, CLGPNM.Sigmacon33.m.sd,
                      CLGPNM.Sigmacon12.m.sd, CLGPNM.Sigmacon13.m.sd, CLGPNM.Sigmacon23.m.sd,
                      CLGPNM.lambda.m.sd)
hpdcon.LB.GPN.sd <- c(CLGPNM.Betacon11.hpd.sd[1], CLGPNM.Betacon12.hpd.sd[1], CLGPNM.Betacon13.hpd.sd[1],
                      CLGPNM.Betacon21.hpd.sd[1], CLGPNM.Betacon22.hpd.sd[1], CLGPNM.Betacon23.hpd.sd[1],
                      CLGPNM.Sigmacon11.hpd.sd[1], CLGPNM.Sigmacon22.hpd.sd[1], CLGPNM.Sigmacon33.hpd.sd[1],
                      CLGPNM.Sigmacon12.hpd.sd[1], CLGPNM.Sigmacon13.hpd.sd[1], CLGPNM.Sigmacon23.hpd.sd[1],
                      CLGPNM.lambda.hpd.sd[1])
hpdcon.UB.GPN.sd <- c(CLGPNM.Betacon11.hpd.sd[2], CLGPNM.Betacon12.hpd.sd[2], CLGPNM.Betacon13.hpd.sd[2],
                      CLGPNM.Betacon21.hpd.sd[2], CLGPNM.Betacon22.hpd.sd[2], CLGPNM.Betacon23.hpd.sd[2],
                      CLGPNM.Sigmacon11.hpd.sd[2], CLGPNM.Sigmacon22.hpd.sd[2], CLGPNM.Sigmacon33.hpd.sd[2],
                      CLGPNM.Sigmacon12.hpd.sd[2], CLGPNM.Sigmacon13.hpd.sd[2], CLGPNM.Sigmacon23.hpd.sd[2],
                      CLGPNM.lambda.hpd.sd[2])

#Create table
CLGPNMres.tab <- cbind(modes.GPN, modes.GPN.sd, hpd.LB.GPN, hpd.LB.GPN.sd,
                       hpd.UB.GPN, hpd.UB.GPN.sd, modescon.GPN, modescon.GPN.sd, 
                       hpdcon.LB.GPN, hpdcon.LB.GPN.sd, hpdcon.UB.GPN, hpdcon.UB.GPN.sd)

CLGPNMres.tab.new = cbind(apply(CLGPNMres.tab[,1:2], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                           sprintf("%.2f", round(x[2], 2)), ")")),
                          apply(CLGPNMres.tab[,3:4], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                           sprintf("%.2f", round(x[2], 2)), ")")),
                          apply(CLGPNMres.tab[,5:6], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                           sprintf("%.2f", round(x[2], 2)), ")")),
                          apply(CLGPNMres.tab[,7:8], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                           sprintf("%.2f", round(x[2], 2)), ")")),
                          apply(CLGPNMres.tab[,9:10], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                            sprintf("%.2f", round(x[2], 2)), ")")),
                          apply(CLGPNMres.tab[,11:12], 1, function(x) paste0(sprintf("%.2f", round(x[1], 2)), " (", 
                                                                             sprintf("%.2f", round(x[2], 2)), ")")))


rownames(CLGPNMres.tab.new) <- c("$\\beta_{0_s^{I}}$", "$\\beta_{0_s^{II}}$",
                                 "$\\beta_{0_y}$", "$\\beta_{1_s^{I}}$",
                                 "$\\beta_{1_s^{II}}$", "$\\beta_{1_y}$",
                                 "$\\sum_{s_{1,1}}$", "$\\sum_{s_{2,2}}$",
                                 "$\\sum_{y_{3,3}}$", "$\\sum_{s_{1,2}}$",
                                 "$\\sum_{sy_{1,3}}$", "$\\sum_{sy_{2,3}}$",
                                 "$\\lambda$")
colnames(CLGPNMres.tab.new) <- c("Mode", "HPD LB", "HPD UB", "Mode", "HPD LB", "HPD UB")


#kable(CLGPNMres.tab.new, "latex", booktabs = TRUE, escape = FALSE, digits = 2,
#      caption = "Results, cross-validation mean and standard deviation, for the GPN-SSN model")%>%
#add_header_above(c("Parameter" = 1, "Unconstrained" = 3, "Constrained" = 3))

```


\begin{table}
\caption{\label{tab:estCLGPNM}Results, cross-validation mean and standard deviation, for the GPN-SSN model}
\centering
\begin{tabular}[t]{lllllll}
\toprule
\multicolumn{1}{c}{Parameter} & \multicolumn{3}{c}{Unconstrained} & \multicolumn{3}{c}{Constrained} \\
\cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-4} \cmidrule(l{2pt}r{2pt}){5-7}
  & Mode & HPD LB & HPD UB & Mode & HPD LB & HPD UB\\
\midrule
$\beta_{0_s^{I}}$ & 0.30 (0.01) & 0.26 (0.01) & 0.34 (0.01) & 2.11 (0.11) & 1.75 (0.09) & 2.50 (0.11)\\
$\beta_{0_s^{II}}$ & 0.19 (0.00) & 0.17 (0.01) & 0.21 (0.00) & 1.34 (0.06) & 1.10 (0.05) & 1.57 (0.06)\\
$\beta_{0_y}$ & 0.33 (0.01) & 0.30 (0.30) & 0.36 (0.01) & 0.33 (0.01) & 0.30 (0.01) & 0.36 (0.01)\\
\addlinespace
$\beta_{1_s^{I}}$ & 0.09 (0.01) & 0.05 (0.01) & 0.13 (0.01) & 0.60 (0.06) & 0.33 (0.05) & 0.90 (0.06)\\
$\beta_{1_s^{II}}$ & 0.07 (0.00) & 0.04 (0.00) & 0.09 (0.01) & 0.48 (0.03) & 0.30 (0.04) & 0.66 (0.04)\\
$\beta_{1_y}$ & 0.09 (0.01) & 0.06 (0.06) & 0.12 (0.01) & 0.09 (0.01) & 0.06 (0.01) & 0.12 (0.01)\\
\addlinespace
$\sum_{s_{1,1}}$ & 0.05 (0.00) & 0.04 (0.00) & 0.06 (0.00) & 2.44 (0.15) & 1.72 (0.07) & 3.46 (0.14)\\
$\sum_{s_{2,2}}$ & 0.02 (0.00) & 0.02 (0.00) & 0.03 (0.00) & 1.00 (0.00) & 1.00 (0.00) & 1.00 (0.00)\\
$\sum_{y_{3,3}}$ & 0.03 (0.00) & 0.02 (0.02) & 0.04 (0.00) & 0.03 (0.00) & 0.02 (0.00) & 0.04 (0.00)\\
$\sum_{s_{1,2}}$ & 0.00 (0.00) & -0.00 (0.00) & 0.01 (0.00) & 0.08 (0.06) & -0.20 (0.06) & 0.34 (0.06)\\
$\sum_{sy_{1,3}}$ & 0.03 (0.00) & 0.02 (0.00) & 0.04 (0.00) & 0.23 (0.01) & 0.17 (0.00) & 0.32 (0.01)\\
$\sum_{sy_{2,3}}$ & 0.01 (0.00) & 0.01 (0.01) & 0.02 (0.00) & 0.09 (0.01) & 0.06 (0.01) & 0.12 (0.01)\\
$\lambda$ & 0.16 (0.01) & 0.14 (0.01) & 0.18 (0.01) & 0.16 (0.01) & 0.14 (0.01) & 0.18 (0.01)\\
\bottomrule
\multicolumn{7}{l}{Note: $\beta_{0_s^{I}}$, $\beta_{0_s^{II}}$ and $\beta_{0_y}$ inform us about the location and intensity on the IPC}\\
\multicolumn{7}{l}{at the average self-efficacy. $\beta_{1_s^{I}}$, $\beta_{1_s^{II}}$ and $\beta_{1_y}$ inform us about the effect of self-efficacy }\\
\multicolumn{7}{l}{on the location and intensity on the IPC. $\sum_{s_{1,1}}$, $\sum_{s_{1,2}}$, $\sum_{s_{2,2}}$, $\sum_{y_{3,3}}$, $\sum_{sy_{1,3}}$,  and $\sum_{sy_{2,3}}$ }\\
\multicolumn{7}{l}{are elements of the variance-covariance matrix of which $\sum_{sy_{1,3}}$ and $\sum_{sy_{2,3}}$ inform us about}\\
\multicolumn{7}{l}{the dependence between the location and intensity on the IPC.}\\
\multicolumn{7}{l}{$\lambda$ is the skewness parameter of the distribution of the intensity of interpersonal behavior.}\\
\end{tabular}
\end{table}


\subsubsection{Average location and intensity on the IPC}
The parameters $\gamma_0$ in the CL-PN, CL-GPN and Abe-Ley model and the
parameter $\beta_{0_y}$ in the GPN-SSN model inform us about the intensity of
interpersonal behavior at the average self-efficacy. For the CL-PN, CL-GPN and
GPN-SSN models the parameters are estimated at 0.38, 0.37 and 0.33 respectively
and are a direct prediction of the intensity of interpersonal behavior at the
average self-efficacy. The estimate for the GPN-SSN model is notably lower and
likely to be caused by its skewed distribution for the intensity of
interpersonal behavior. In the Abe-Ley model, $\gamma_0$ influences the shape
parameter of the distribution of the intensity of interpersonal behavior and does
not directly estimate the average intensity. Instead we can use the survival
function to say something about the probability of having a certain intensity of
interpersonal behavior. Figure \ref{reglineweib} shows this function for several
values of self-efficacy. We look at the survival function at average values of
self-efficacy. Note that this function is the average of all survival functions
for observations that fall within 1 standard deviation of the mean. The survival
function indicates that the probability of having a low intensity of
interpersonal behavior is higher than having a high intensity. We however can not
make any direct statement about the estimated intensity using the Abe-Ley
model.

\hfil \hspace{2cm} [Figure \ref{reglineweib} about here] \hfil

\indent The parameters $\beta_0^{I}$, $\beta_0^{II}$, $\beta_0$,
$\beta_{0_{s^{I}}}$ and $\beta_{0_{s^{II}}}$ inform us about the location on the IPC at the average self-efficacy for the CL-PN, CL-GPN,
Abe-Ley and GPN-SSN model respectively. For the CL-PN, CL-GPN and GPN-SSN model
we need to combine the estimates for the underlying bivariate components $\{I,
II\}$ into one circular estimate using the double arctangent
function\footnote{\(\mbox{atan2}(\beta_0^{II}, \beta_0^{I})\) or
\(\mbox{atan2}(\beta_{0_{s^{II}}}, \beta_{0_{s^{I}}})\)}.
Table \ref{tab:means} shows that these circular estimates are similar for the
three models at 32.29$^\circ$, 33.70$^\circ$ and 35.53$^\circ$. In the Abe-Ley
model the location on the IPC at the average self-efficacy is
estimated at 0.36 radians or 20.63$^\circ$.

\hfil \hspace{2cm} [Table \ref{tab:means} about here] \hfil

```{r survivalplot, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}
summary(Dat$SEc)

y <- seq(0, 1, by = 0.05)
scross <- array(NA, dim = c(3, length(y), 10))


SE_low = Dat[Dat$Efficacy_CM <= 4.04,]
SE_mid = Dat[Dat$Efficacy_CM > 4.04 & Dat$Efficacy_CM < 6.04,]
SE_high = Dat[Dat$Efficacy_CM >= 6.04,]
  
for(i in 1:length(y)){
  mu_low <- mean(resAL_arr[1,]) + 2*atan(mean(resAL_arr[2,])*SE_low$SEc)
  beta_low <- exp(mean(resAL_arr[3,]) + mean(resAL_arr[4,])*SE_low$SEc)*(1 -     tanh(mean(resAL_arr[5,]))*cos(SE_low$theta - mu_low))^(1/mean(resAL_arr[6,]))
    
  mu_mid <- mean(resAL_arr[1,]) + 2*atan(mean(resAL_arr[2,])*SE_mid$SEc)
  beta_mid <- exp(mean(resAL_arr[3,]) + mean(resAL_arr[4,])*SE_mid$SEc)*(1 -     tanh(mean(resAL_arr[5,]))*cos(SE_mid$theta - mu_mid))^(1/mean(resAL_arr[6,]))
    
    
  mu_high <- mean(resAL_arr[1,]) + 2*atan(mean(resAL_arr[2,])*SE_high$SEc)
  beta_high <- exp(mean(resAL_arr[3,]) + mean(resAL_arr[4,])*SE_high$SEc)*(1 -     tanh(mean(resAL_arr[5,]))*cos(SE_high$theta - mu_high))^(1/mean(resAL_arr[6,]))

  if(i == 1){
    res_low = exp(-mean(resAL_arr[6,])*y[i]^beta_low)
    res_mid = exp(-mean(resAL_arr[6,])*y[i]^beta_mid)
    res_high = exp(-mean(resAL_arr[6,])*y[i]^beta_high)
  }else{
    res_low = cbind(res_low, exp(-mean(resAL_arr[6,])*y[i]^beta_low))
    res_mid = cbind(res_mid, exp(-mean(resAL_arr[6,])*y[i]^beta_mid))
    res_high = cbind(res_high, exp(-mean(resAL_arr[6,])*y[i]^beta_high))
  }
      
}


for(k in 1:10){
      
  train <- get(paste("train.", k, sep = ""))
      
  SE_low = train[train$Efficacy_CM <= 4.04,]
  SE_mid = train[train$Efficacy_CM > 4.04 & train$Efficacy_CM < 6.04,]
  SE_high = train[train$Efficacy_CM >= 6.04,]
  
  for(i in 1:length(y)){
    mu_low <- mean(resAL_arr[1,k]) + 2*atan(mean(resAL_arr[2,k])*SE_low$SEc)
    beta_low <- exp(mean(resAL_arr[3,k]) + mean(resAL_arr[4,k])*SE_low$SEc)*(1 -     tanh(mean(resAL_arr[5,k]))*cos(SE_low$theta - mu_low))^(1/mean(resAL_arr[6,k]))
    
    mu_mid <- mean(resAL_arr[1,k]) + 2*atan(mean(resAL_arr[2,k])*SE_mid$SEc)
    beta_mid <- exp(mean(resAL_arr[3,k]) + mean(resAL_arr[4,k])*SE_mid$SEc)*(1 -     tanh(mean(resAL_arr[5,k]))*cos(SE_mid$theta - mu_mid))^(1/mean(resAL_arr[6,k]))
    
    
    mu_high <- mean(resAL_arr[1,k]) + 2*atan(mean(resAL_arr[2,k])*SE_high$SEc)
    beta_high <- exp(mean(resAL_arr[3,k]) + mean(resAL_arr[4,k])*SE_high$SEc)*(1 -     tanh(mean(resAL_arr[5,k]))*cos(SE_high$theta - mu_high))^(1/mean(resAL_arr[6,k]))

    if(i == 1){
      res_low = exp(-mean(resAL_arr[6,k])*y[i]^beta_low)
      res_mid = exp(-mean(resAL_arr[6,k])*y[i]^beta_mid)
      res_high = exp(-mean(resAL_arr[6,k])*y[i]^beta_high)
    }else{
      res_low = cbind(res_low, exp(-mean(resAL_arr[6,k])*y[i]^beta_low))
      res_mid = cbind(res_mid, exp(-mean(resAL_arr[6,k])*y[i]^beta_mid))
      res_high = cbind(res_high, exp(-mean(resAL_arr[6,k])*y[i]^beta_high))
    }
      
  }
  
  
    
  scross[1,,k] <- apply(res_low, 2, mean)
  scross[2,,k] <- apply(res_mid, 2, mean)
  scross[3,,k] <- apply(res_high, 2, mean)
}


s = apply(scross, c(1,2), mean)


tikz("Plots/survivaldiffSE.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

plot(y, s[1,], type = "l", ylab = "P(IPC intensity)", xlab = "IPC intensity", bty = "n")
points(y, s[2,], type = "l", lty = 2)
points(y, s[3,], type = "l", lty = 3)
legend(0.7, 1, legend = c("low SE", "average SE", "high SE"), lty = c(1,2,3), bty = "n")

dev.off()
tools::texi2dvi("Plots/survivaldiffSE.tex", pdf=TRUE)


```

\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/survivaldiffSE.pdf}
\caption{Plot showing the probability of having a particular intensity of interpersonal behavior (survival plot) for the minimum, mean and maximum self-efficacy in the data.}
\label{reglineweib}
\end{figure}


```{r means, cache = TRUE, echo = FALSE}

theta_pred_PN   <- simplify2array(lapply(res_CLPN, "[[", "theta_pred"))
theta_pred_GPN  <- simplify2array(lapply(res_CLGPN, "[[", "theta_pred"))
theta_pred_GPNM <- simplify2array(lapply(res_CLGPNM, "[[", "theta_pred"))

theta_pred_PN <- array(unlist(theta_pred_PN),
                       dim = c(nrow(theta_pred_PN[[1]]), ncol(theta_pred_PN[[1]]), length(theta_pred_PN)))
theta_pred_GPN <- array(unlist(theta_pred_GPN),
                        dim = c(nrow(theta_pred_GPN[[1]]), ncol(theta_pred_GPN[[1]]), length(theta_pred_GPN)))
theta_pred_GPNM <- array(unlist(theta_pred_GPNM),
                         dim = c(nrow(theta_pred_GPNM[[1]]), ncol(theta_pred_GPNM[[1]]), length(theta_pred_GPNM)))

means <- round(rbind(mean_circ(apply(apply(theta_pred_PN[,5000:20000,], c(2,3), mean_circ), 2, mode_est_circ)),
                     mean_circ(apply(apply(theta_pred_GPN[,5000:20000,], c(2,3), mean_circ), 2, mode_est_circ)),
                     mean_circ(apply(apply(theta_pred_GPNM[,5000:20000,], c(2,3), mean_circ), 2, mode_est_circ)))*(180/pi), 2)
hpds  <- round(rbind(apply(apply(apply(theta_pred_PN[,5000:20000,], c(2,3), mean_circ), 2, hpd_est_circ), 1, mean_circ),
                     apply(apply(apply(theta_pred_GPN[,5000:20000,], c(2,3), mean_circ), 2, hpd_est_circ), 1, mean_circ),
                     apply(apply(apply(theta_pred_GPNM[,5000:20000,], c(2,3), mean_circ), 2, hpd_est_circ), 1, mean_circ))*(180/pi), 2)

means = cbind(means, hpds)


#rownames(means) <- c("Cl-PN", "CL-GPN", "GPN-SSN")
#colnames(means) <- c('Mode', "HPD LB", "HPD UB")
#kable(means, "latex", booktabs = T, escape = FALSE, digits = 2,
#      caption = "Posterior estimates for the circular mean in the CL-PN, CL-GPN and GPN-SSN models")%>%
#kable_styling() #%>%
#add_footnote("Note that these means are based on the posterior predictive distribution for the intercepts following (Wang & Gelfand, 2013).", #notation="alphabet")

```

\begin{table}
\caption{\label{tab:means}Posterior estimates (in degrees) for the circular mean (at SE = 0) in the CL-PN, CL-GPN and GPN-SSN models}
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & Mode & HPD LB & HPD UB\\
  \midrule
CL-PN & 32.29 & 24.81 & 39.71\\
CL-GPN & 33.70 & 26.72 & 41.15\\
GPN-SSN & 35.53 & 28.40 & 43.30\\
\bottomrule
\multicolumn{4}{l}{Note that these means are based on}\\
\multicolumn{4}{l}{their posterior predictive distribution }\\
\multicolumn{4}{l}{following (Wang and Gelfand, 2013)}\\
\end{tabular}
\end{table}


\subsubsection{The effect of self-efficacy}
The parameters $\gamma_1$ in the CL-PN, CL-GPN, Abe-Ley models and $\beta_{1_y}$
in the GPN-SSN model inform us about the effect of self-efficacy on the intensity
of interpersonal behavior. For the CL-PN, CL-GPN and GPN-SSN model the
parameters are estimated at 0.03, 0.03 and 0.09 respectively and are a direct
estimate of the effect of self-efficacy on the intensity of interpersonal
behavior, \emph{i.e.} an increase of 1 unit in self-efficacy leads to an
increase of 0.09 units in the intensity of interpersonal behavior according to
the GPN-SSN model. These estimates are however quite small and only different
from 0 (the HPD interval does not contain 0) in the GPN-SSN model. It is hard to
say which of the three models, CL-PN, CL-GPN or GPN-SSN, to use to base our
conclusions on. The models CL-GPN and CL-PN fit the linear component best according
to the model fit in Table \ref{tab:ModelFit}. In these models the linear component has a
symmetric distribution whereas in the GPN-SSN the distribution of the linear
component is skewed. However, the effect of self-efficacy is different from 0 only
in the GPN-SSN model which does not seem to match with its lower model fit.

\hfil \hspace{2cm} [Table \ref{tab:ModelFit} about here] \hfil

\indent In the Abe-Ley model, $\gamma_1$ influences the shape parameter of the distribution of
the intensity of interpersonal behavior and does not directly estimate the effect
of self-efficacy. Instead we can use the survival function to say something
about the probability of having a certain intensity of interpersonal behavior for
different values of self-efficacy. Figure \ref{reglineweib} shows this function
for low, average and high values of self-efficacy (as defined in Figure
\ref{dataplot}). This function indicates that the effect of self-efficacy on the
intensity of interpersonal behavior is not linear. The probability of having a
higher intensity of interpersonal behavior is highest for low self-efficacy and
lowest for average self-efficacy.\newline
\indent The parameters $\beta_1^{I}$, $\beta_1^{II}$, $\beta_1$,
$\beta_{1_{s^{I}}}$ and $\beta_{1_{s^{II}}}$ inform us about the effect of
self-efficacy on the location on the IPC in the CL-PN, CL-GPN,
Abe-Ley and GPN-SSN model respectively. For the CL-PN and Abe-Ley models we have
drawn the circular regression lines for this effect in Figure \ref{regline} (see
the description of the CL-PN and CL-GPN models for a detailed explanation of
circular regression lines). For the CL-PN model the inflection point is
indicated with a square in Figure \ref{regline}. The inflection point for the
Abe-Ley model falls outside the bounds of the plot and is therefore not
displayed. The slope at the inflection point, $b_c$, for the CL-PN model is
computed by using methods from @CremersMulderKlugkist2017 and is equal to 1.67
(-24.66, 29.33)\footnote{Note that this is a linear approximation to the
circular regression line representing the slope at a specific point. Therefore
it is possible for the HPD interval to be wider than $2\pi$. In this case the
interval is much wider and covers 0, indicating there is no evidence for an
effect.}. The parameter $\beta_1$ is the slope at the inflection point for the
Abe-Ley model and is equal to -0.03. Even though these slopes are
different\footnote{Note that the difference in regression lines (and thus inflection point) seems to be influenced by an outlier with a low self-efficacy and IPC value of approximately 160$^{\circ}$ = -200$^{\circ}$.}, the regression lines in Figure \ref{regline} are quite similar in the
data range. Both the regression line of the Abe-Ley model and the CL-PN model
show slopes that are not very steep in the range of the data indicating that the
effect of self-efficacy on the location on the IPC is not large.

\hfil \hspace{2cm} [Figure \ref{regline} about here] \hfil

\indent In the CL-GPN and GPN-SSN models we cannot compute circular regression
coefficients due to the fact that not only the mean vector of the GPN
distribution but also the covariance matrix influences the predicted value on
the circle. Instead, we will compute posterior predictive distributions for the
predicted circular component of individuals scoring the minimum, maximum and
median self-efficacy. The modes and 95\% HPD intervals of these posterior
predictive distributions are \(\hat{\theta}_{SE_{min}} = 215.74^\circ
(147.36^\circ, \: 44.49^\circ)\), \(\hat{\theta}_{SE_{median}} = 25.93^\circ
(337.02^\circ, \: 138.59^\circ)\), \(\hat{\theta}_{SE_{max}} = 30.86^\circ
(8.63^\circ, \: 72.19^\circ)\) for the CL-GPN model. Note that we display the
modes and HPD intervals for the posterior predictive distributions on the
interval $[0^\circ, 360^\circ)$ and that $44.49^\circ = 404.49^\circ$ due to the
periodicity of a circular variable. The posterior mode estimate of
$215.74^\circ$ thus lies within its HPD interval $(147.36^\circ, \:
44.49^\circ)$. For the GPN-SSN model the modes and 95\% HPD intervals of the
posterior predictive distributions are \(\hat{\theta}_{SE_{min}} = 206.87^\circ
(117.12^\circ, \: 72.02^\circ)\), \(\hat{\theta}_{SE_{median}} = 24.68^\circ
(334.73^\circ, \: 128.27^\circ)\), \(\hat{\theta}_{SE_{max}} = 29.81^\circ
(0.74^\circ, \: 80.61^\circ)\). For both the CL-GPN and GPN-SSN model the HPD
intervals of the mode of the posterior predictive intervals of individuals
scoring the minimum, median and maximum self-efficacy overlap. This indicates
that the effect of self-efficacy, if there is any, on the location on the IPC shows is not expected to be strong. Had the HPD intervals not
overlapped we could have concluded that compared to teachers with a lower self-efficacy increases, the
score on the IPC moves counterclockwise.

```{r ppreg, echo = FALSE, results = FALSE, eval = FALSE}
theta_pred_GPN.min <- simplify2array(lapply(res_CLGPN, "[[", "theta_pred.min"))
theta_pred_GPN.median <- simplify2array(lapply(res_CLGPN, "[[", "theta_pred.meadian"))
theta_pred_GPN.max <- simplify2array(lapply(res_CLGPN, "[[", "theta_pred.max"))

mean_circ(apply(theta_pred_GPN.min[5000:20000,], 2, mode_est_circ))*(180/pi)
mean_circ(apply(theta_pred_GPN.median[5000:20000,], 2, mode_est_circ))*(180/pi)
mean_circ(apply(theta_pred_GPN.max[5000:20000,], 2, mode_est_circ))*(180/pi)

apply(apply(theta_pred_GPN.min[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)
apply(apply(theta_pred_GPN.median[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)
apply(apply(theta_pred_GPN.max[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)


```

```{r ppreggpnm, echo = FALSE, results = FALSE, eval = FALSE}
theta_pred_GPNM.min <- simplify2array(lapply(res_CLGPNM, "[[", "theta_pred.min"))
theta_pred_GPNM.median <- simplify2array(lapply(res_CLGPNM, "[[", "theta_pred.median"))
theta_pred_GPNM.max <- simplify2array(lapply(res_CLGPNM, "[[", "theta_pred.max"))

mean_circ(apply(theta_pred_GPNM.min[5000:20000,], 2, mode_est_circ))*(180/pi)
mean_circ(apply(theta_pred_GPNM.median[5000:20000,], 2, mode_est_circ))*(180/pi)
mean_circ(apply(theta_pred_GPNM.max[5000:20000,], 2, mode_est_circ))*(180/pi)

dim(theta_pred_GPNM.median)

apply(apply(theta_pred_GPNM.min[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)
apply(apply(theta_pred_GPNM.median[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)
apply(apply(theta_pred_GPNM.max[5000:20000,], 2, hpd_est_circ), 1, mean_circ)*(180/pi)

```


```{r circreg, cache = TRUE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

tikz("Plots/reglinediffSE.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

a1 <- CLPN.BI.m[1]
b1 <- CLPN.BI.m[2]
a2 <- CLPN.BII.m[1]
b2 <- CLPN.BII.m[2]
ax <- -(a1*b1 + a2*b2)/(b1^2 + b2^2)
ac <- atan2(a2 + b2*ax, a1 + b1*ax)

x <- seq(-5, 5, by = 0.1)

pred.AL <- 0.37 + 2*atan(-0.01*x)
pred.PN <- atan2(a2 + b2*x, a1 + b1*x)

plot(Dat$SEc, Dat$theta*(180/pi), bty = "n", yaxt = "n",
     xlim = c(-5, 2), ylab = "$\\theta$", xlab = "self-efficacy")
axis(side = 2,  at = c(-160, -80, 0, 80, 160))
points(x, pred.AL*(180/pi), type = "l", lty = 1)
points(x, pred.PN*(180/pi), type = "l", lty = 2)
points(ax, ac*(180/pi), pch = 15)

dev.off()
tools::texi2dvi("Plots/reglinediffSE.tex", pdf=TRUE)

```


```{r reg.coef, cache = TRUE, echo = FALSE, results = FALSE}


BI    <- simplify2array(lapply(res_CLPN, "[[", "BI"))
BII   <- simplify2array(lapply(res_CLPN, "[[", "BII"))

bc <- matrix(NA, 15001, 10)
SAM <- matrix(NA, 15001, 10)

for(i in 1:10){
  
  a1 <- BI[5000:20000,1,i]
  b1 <- BI[5000:20000,2,i]
  a2 <- BII[5000:20000,1,i]
  b2 <- BII[5000:20000,2,i]
  
  ax <- -(a1*b1 + a2*b2)/(b1^2 + b2^2)
  ac <- atan2(a2 + b2*ax, a1 + b1*ax)
  bc[,i] <- -tan(atan2(a2, a1)-ac)/ax
  SAM[,i] <- bc[i]/(1 + (bc[i]*(-ax))^2)
  
}

mean(apply(bc, 2, mode_est))
rowMeans(apply(bc, 2, hpd_est))

mean(apply(SAM, 2, mode_est))
rowMeans(apply(SAM, 2, hpd_est))


```


\begin{figure}
\centering
\includegraphics[width = \textwidth]{Plots/reglinediffSE.pdf}
\caption{Plot showing circular regression lines for the effect of self-efficacy
as predicted by the Abe-Ley model (solid line) and CL-PN model (dashed line). The
black square indicates the inflection point of the circular regression line for
the CL-PN model.}
\label{regline}
\end{figure}


\subsubsection{Dependence between location and intensity on the IPC}
The relation between the location and intensity on the IPC in the
CL-PN and CL-GPN models is described by the parameters $\gamma_{\cos}$ and
$\gamma_{\sin}$. The HPD interval of \(\gamma_{\cos}\) does not include 0 for
both the CL-PN and CL-GPN models, meaning that the cosine component of the location has an effect on the intensity.\newline
\indent In the teacher data the sine and cosine components have a substantive
meaning. This is illustrated in Figure \ref{QTI}. In a unit circle the
horizontal axis (Communion) represents the cosine and the vertical axis
(Agency) represents the sine of an angle. For the teacher data this means that
the Communion (cosine) dimension of the IPC positively effects the intensity of a
teachers' interpersonal behavior, in plain words: teachers exhibiting
interpersonal behavior types with higher communion scores (e.g., 'helpful' and
'understanding' in Figure 2) are stronger in their interpersonal behavior.\newline 
\indent In the GPN-SSN model the dependence between the location and intensity on the IPC is modelled through the covariances between the linear
component and the sine and cosine of the circular component \(\sum_{sy_{2,3}}\) and
\(\sum_{sy_{1,3}}\). Both covariances, \(\sum_{sy_{2,3}} = 0.09\) and
\(\sum_{sy_{1,3}} = 0.23\), are different from zero, but the one of the cosine
component, and thus the correlation with the Communion dimension, is larger.
This means that teachers scoring both high on Communion and Agency show stronger
behavior. Together with the results from the CL-PN and CL-GPN models in the
previous paragraph this translates to the conclusion that teachers with higher
intensity scores have a location on the IPC that lies between
0$^\circ$ and 90$^\circ$. To get these scores on the circle both the Agency and
the Communion score of a Teacher have to be positive (see \eqref{PredVal}). This
corresponds to the pattern observed in the teacher data in Figure \ref{QTI}. At
an intensity of 0.4 and up we see that the scores on the circle range on average
between 0$^\circ$ and 100$^\circ$.

```{r correlation, echo = FALSE, eval = FALSE, results = FALSE}

require(pracma)

alpha <- mean(resAL_arr[6,])
kappa <- mean(resAL_arr[5,])
lambda <- mean(resAL_arr[7,])

#R cannot compute legendre functions of non-integer degree
#leg.0 <- legendre(1/alpha, cosh(kappa))[1,] 
#leg.1 <- legendre(1/alpha, cosh(kappa))[2,]
#leg.2 <- legendre(1/alpha, cosh(kappa))[3,]

#so we use http://functions.wolfram.com/webMathematica/FunctionEvaluation.jsp?name=LegendreP2General

1/3.82
cosh(1.58)

```


\subsubsection{Model fit}
Table \ref{tab:ModelFit} shows the values of the PLSL criterion for the linear
and circular components of the four cylindrical models fit to the teacher
data.\newline \indent The CL-PN and CL-GPN models have the best out-of-sample
predictive performance for the linear component. They show roughly the same
performance because they model the linear component in the same way. We should
note that even though the predictive performance of the Abe-Ley model for the
linear component is worst on average, the standard deviation of the
cross-validation estimates is rather large. This means that in some samples, the
Abe-Ley model shows a lower PLSL value than the average of 25.49.\newline
\indent The Abe-Ley model has the best out-of-sample predictive performance for
the circular component. This would suggest that for the circular variable a
slightly skewed distribution fits best. However, both the GPN-SSN  and the
CL-GPN models fit much worse even though the distribution for the circular
component in these models can also take a skewed shape. It should be noted that
the  standard deviation of the cross-validation estimates is rather large for
the Abe-Ley and the CL-GPN model. It is possible that these large standard
deviations for the PLSL are caused by the fact that they are computed for a
relatively small sample size, but this does not explain why the PLSL has a large
standard deviation for only a few cylindrical models and not for all.\newline
\indent In this situation, where one model fits the linear component best and
another one fits the circular component best, it is hard to determine which model
we should choose. In this case the results for the CL-PN /CL-GPN and Abe-Ley
model are quite different regarding the effect of self-efficacy on the linear
component (intensity of interpersonal behavior). Because the Abe-Ley fit for the
linear part is worst we would choose to trust the results for the CL-PN and
CL-GPN models here. For the circular part however the results of the CL-PN/CL-GPN
models do not differ as much from the Abe-Ley model and we reach the same
conclusion for both models, namely that the effect of self-efficacy on location on the IPC is not very strong. Therefore we would prefer the
CL-PN/CL-GPN models in this case because where it matters in terms of
interpretation (the linear part) they show better fit.

```{r ModelFit, cache = TRUE, echo = FALSE}

CLGPNM.ll.circ <- simplify2array(lapply(res_CLGPNM, "[[", "ll.circ"))
CLGPNM.ll.lin  <- simplify2array(lapply(res_CLGPNM, "[[", "ll.lin"))
CLGPN.ll.circ  <- simplify2array(lapply(res_CLGPN, "[[", "ll.circ"))
CLGPN.ll.lin   <- simplify2array(lapply(res_CLGPN, "[[", "ll.lin"))
CLPN.ll.circ   <- simplify2array(lapply(res_CLPN, "[[", "ll.circ"))
CLPN.ll.lin    <- simplify2array(lapply(res_CLPN, "[[", "ll.lin"))

CLGPNM.ll.circ.m <- mean(-2*apply(CLGPNM.ll.circ[5:20,], 2, mean))
CLGPNM.ll.lin.m  <- mean(-2*apply(CLGPNM.ll.lin[5:20,], 2, mean))
CLGPN.ll.circ.m  <- mean(-2*apply(CLGPN.ll.circ[5:20,], 2, mean))
CLGPN.ll.lin.m   <- mean(-2*apply(CLGPN.ll.lin[5:20,], 2, mean))
CLPN.ll.circ.m   <- mean(-2*apply(CLPN.ll.circ[5:20,], 2, mean))
CLPN.ll.lin.m    <- mean(-2*apply(CLPN.ll.lin[5:20,], 2, mean))

CLGPNM.ll.circ.sd <- sd(-2*apply(CLGPNM.ll.circ[5:20,], 2, mean))
CLGPNM.ll.lin.sd  <- sd(-2*apply(CLGPNM.ll.lin[5:20,], 2, mean))
CLGPN.ll.circ.sd  <- sd(-2*apply(CLGPN.ll.circ[5:20,], 2, mean))
CLGPN.ll.lin.sd   <- sd(-2*apply(CLGPN.ll.lin[5:20,], 2, mean))
CLPN.ll.circ.sd   <- sd(-2*apply(CLPN.ll.circ[5:20,], 2, mean))
CLPN.ll.lin.sd    <- sd(-2*apply(CLPN.ll.lin[5:20,], 2, mean))

AL.ll.circ <- simplify2array(lapply(resAL, "[[", "ll.circ"))
AL.ll.lin <- simplify2array(lapply(resAL, "[[", "ll.lin"))

AL.ll.circ.m <- mean(AL.ll.circ)
AL.ll.lin.m  <- mean(AL.ll.lin)

AL.ll.circ.sd <- sd(AL.ll.circ)
AL.ll.lin.sd  <- sd(AL.ll.lin)

PLSL.m <- round(rbind(c(CLPN.ll.circ.m, CLGPN.ll.circ.m, AL.ll.circ.m, CLGPNM.ll.circ.m),
                      c(CLPN.ll.lin.m, CLGPN.ll.lin.m, AL.ll.lin.m, CLGPNM.ll.lin.m)), 2)
PLSL.sd <- round(rbind(c(CLPN.ll.circ.sd, CLGPN.ll.circ.sd, AL.ll.circ.sd, CLGPNM.ll.circ.sd),
                      c(CLPN.ll.lin.sd, CLGPN.ll.lin.sd, AL.ll.lin.sd, CLGPNM.ll.lin.sd)), 2)
PLSL.sd <- apply(PLSL.sd, 1:2, function(x) paste0("(", sprintf("%.2f", round(x, 2)), ")"))


PLSL <- data.frame(PLSL.m[1,], PLSL.sd[1,], PLSL.m[2,], PLSL.sd[2,])

rownames(PLSL) <- c("CL-PN", "CL-GPN", "Abe-Ley", "GPN-SSN")
colnames(PLSL) <- c("mean", "sd", "mean", "sd")

#kable(PLSL, "latex", booktabs = T, escape = FALSE,
#      caption = "PLSL criteria, cross-validation mean and standard deviation, for the circular and linear outcome in #the four cylindrical models")%>%
#add_header_above(c("Model" = 1, "Circular" = 2, "Linear" = 2))

```


\begin{table}

\caption{\label{tab:ModelFit}PLSL criteria, cross-validation mean and standard deviation, for the circular and linear component in the four cylindrical models}
\centering
\begin{tabular}[t]{lrlrl}
\toprule
\multicolumn{1}{c}{Model} & \multicolumn{2}{c}{Circular} & \multicolumn{2}{c}{Linear} \\
\cmidrule(l{2pt}r{2pt}){1-1} \cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-5}
  & mean & sd & mean & sd\\
\midrule
CL-PN & 82.96 & (9.47) & -17.65 & (3.70)\\
CL-GPN & 78.21 & (14.53) & -18.30 & (3.00)\\
Abe-Ley & 31.97 & (22.07) & 25.49 & (17.46)\\
GPN-SSN & 107.10 & (10.52) & -2.37 & (7.01)\\
\bottomrule
\end{tabular}
\end{table}





\section{Discussion}\label{Discussion}
In this paper we modified four models for cylindrical data in such a way that
they include a regression of both the linear and circular component onto a set of
covariates. Subsequently we have shown how these four methods can be used to
analyze a dataset on the interpersonal behavior of teachers. In this final
section we will first comment on what researchers can gain by using cylindrical
models for the teacher data. Subsequently we will comment on the differences
between the cylindrical models that were introduced in this paper.\newline
\indent Concerning the teacher data, the advantage of cylindrical data analysis
is that we were able to analyze the information about the location and intensity
on the IPC simultaneously. In previous research, the two components of the
interpersonal circumplex (\emph{i.e.}, Agency and Communion) were analyzed
separately. Such an approach only provides information about the intensity of a
teachers' score on Agency and Communion separately and the information about the
combination of Agency and Communion, which describes the location on the IPC,
gets lost as was observed in the Standard Analysis section. A first solution to
include both dimensions as a circular variable in data analysis was described by
@Cremers2018Assessing. A downside of that analysis was that information about
the intensity could not be retained. In the present study, we have shown how
using cylindrical models can simultaneously model the information about the
location and intensity on the IPC and how these are influenced by teachers'
self-efficacy in classroom management. Although we do not find any strong
effects of self-efficacy on either the location and intensity\footnote{similar
to the standard analysis}, the four cylindrical models do provide a way of
analyzing and interpreting this effect. This is beneficial for future research
in which we may want to investigate the effect of further covariates on data
from the circumplex. \newline
\indent Furthermore, in addition to being able to assess the influence of
covariates, the cylindrical models also provide information about the dependence
between the location and intensity on the IPC. This is a second advantage over
the standard analysis in which it is impossible to quantify the relation between
intensity and location. We found that stronger behavior is associated with
higher scores on the Communion and in some models also the Agency dimension.
This implies that teachers whose interpersonal behavior ranges between
0$^\circ$ and 90$^\circ$ on the IPC, the 'helpful' and 'directing' subtypes, are
stronger in their behavior than teachers of the other subtypes.\newline
\indent As mentioned in the introduction, data from the interpersonal circumplex
is not the only type of cylindrical data that occurs in psychology. The methods
presented in this paper are also of use for research on human navigation and
eye-tracking research. For example, in a study by @pannasch2008duration, the relation between
saccade amplitudes and fixation duration is investigated in different
experimental settings. The saccade amplitude is a circular outcome that measures
the turning angle of the eye between fixations on different focal points, e.g.,
an individual first looks at one object in a picture and then moves focus to
another object. The outcome variables of interest are thus cylindrical in nature
(saccade amplitude is a circular outcome and fixation duration a linear one).
More importantly however, the researchers are interested in both the relation
between the circular and linear outcome and how covariates (different
experimental settings) affect the outcomes. These research questions can be
answered using the cylindrical models introduced in this paper. Furthermore,
even though cylindrical models are already used in fields outside of psychology,
the addition of a regression structure to the models is of use in these fields
as well. \newline
\indent Out of the four cylindrical models investigated in this paper, the
results from CL-PN and Abe-Ley models are most straightforward to interpret. In
the CL-GPN and GPN-SSN models the interpretation of the parameters of the
circular outcome component is more complex, if at all possible. This is caused
by the fact that in addition to the mean vector the covariance matrix of the GPN
distribution affects the location of the circular data, making it difficult to
compute regression coefficients on the circle. @wang2012directional state that
Monte Carlo integration can be used to compute a circular mean and variance for
the GPN distribution. In future research, this solution might be applied to the
methods of @CremersMulderKlugkist2017 in order to compute circular coefficients
for GPN models.\newline
\indent The GPN-SSN model is more flexible compared to the other three
cylindrical models that were investigated in this paper. Multiple linear and
circular components can be included and we can thus apply the model to
multivariate cylindrical data. In addition the GPN-SSN, the CL-GPN and CL-PN
models are extendable to a mixed-effects structure and can thus also be fit to
longitudinal data (see @nunez2014bayesian and @hernandez2016general for
hierarchical/mixed-effects models for the PN and GPN distributions
respectively). For the Abe-Ley model this may also be possible but has not been
done in previous research for the conditional distribution of its circular
component (sine-skewed von Mises). Concerning asymmetry, both the GPN-SSN as
well as the Abe-Ley model allow for non-symmetrical shapes of the distributions
of both the linear and circular component, while the CL-GPN model permits an
asymmetric  circular component.\newline
\indent The four cylindrical models that were modified to the regression
context in this paper are not the only cylindrical distributions
available from the literature. Other interesting cylindrical distributions have been introduced by
@fernandez2007models, @kato2008dependent and @sugasawa2015flexible (for more references we
refer to Chapter 2 of @ley2017modern). In the present study we
have decided not to include these distributions for reasons of space,
complexity of the models and ease of implementing a regression
structure. In future research however it would be interesting to
investigate other types of cylindrical distributions as well in order to
compare the interpretability, flexibility and model fit to the models
developed in the present study.

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

\newpage
\section*{{\normalfont References}}
<div id="refs"></div>

